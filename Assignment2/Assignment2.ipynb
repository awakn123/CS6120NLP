{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1\n",
    "Repository Link: [Github](https://github.com/awakn123/CS6120NLP/tree/main)\n",
    "\n",
    "Members: Yun Cao, Yue Liu, Nan Chen, Muyang Cheng\n",
    "# Part 1: Data Preprocessing:\n",
    "1.1 Load the dataset and perform initial exploration to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           headline   category  \\\n",
      "0           0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1           1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2           2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3           3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4           4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "         date  headline_length  short_description_length  \n",
      "0  2022-09-23               76                       154  \n",
      "1  2022-09-23               89                       159  \n",
      "2  2022-09-23               69                        64  \n",
      "3  2022-09-23               56                       159  \n",
      "4  2022-09-22               77                       156  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('News_Category_Dataset_v3.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Clean the text data, including removing special characters, stopwords, applying lowercasing, correcting spelling, standardizing, handling contractions, and lemtization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pkg_resources\n",
    "import inflect\n",
    "import contractions\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p = inflect.engine()\n",
    "\n",
    "def standardize_numbers(text):\n",
    "    return ' '.join([p.number_to_words(word) if word.isdigit() else word for word in text.split()])\n",
    "\n",
    "def handle_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # standardize\n",
    "    text = standardize_numbers(text)\n",
    "    # handle contractions\n",
    "    text = handle_contractions(text)\n",
    "    # correct typos\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)\n",
    "        corrected_words.append(suggestions[0].term if suggestions else word)\n",
    "    text = ' '.join(corrected_words)\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # remove stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # rejoin words\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# clean data\n",
    "df['cleaned_headline'] = df['headline'].apply(clean_text)\n",
    "df['cleaned_description'] = df['short_description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Perform text tokenization and vectorization using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_headline = tfidf_vectorizer.fit_transform(df['cleaned_headline'])\n",
    "# df_headline_tfidf = pd.DataFrame(tfidf_headline.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# df_headline_tfidf = df_headline_tfidf.add_prefix('headline_')\n",
    "\n",
    "tfidf_description = tfidf_vectorizer.fit_transform(df['cleaned_description'])\n",
    "# df_description_tfidf = pd.DataFrame(tfidf_description.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# df_description_tfidf = df_description_tfidf.add_prefix('description_')\n",
    "\n",
    "# df = pd.concat([df, df_headline_tfidf, df_description_tfidf], axis=1)\n",
    "tfidf= hstack([tfidf_headline,tfidf_description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Extract and analyze different features from the text that might be useful for classification, such as word count,\n",
    "sentence length, n-grams, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 35  5 ... 28 28 28]\n",
      "        ARTS  ARTS & CULTURE  BLACK VOICES  BUSINESS  COLLEGE  COMEDY  CRIME  \\\n",
      "0          0               0             0         0        0       0      0   \n",
      "1          0               0             0         0        0       0      0   \n",
      "2          0               0             0         0        0       1      0   \n",
      "3          0               0             0         0        0       0      0   \n",
      "4          0               0             0         0        0       0      0   \n",
      "...      ...             ...           ...       ...      ...     ...    ...   \n",
      "209522     0               0             0         0        0       0      0   \n",
      "209523     0               0             0         0        0       0      0   \n",
      "209524     0               0             0         0        0       0      0   \n",
      "209525     0               0             0         0        0       0      0   \n",
      "209526     0               0             0         0        0       0      0   \n",
      "\n",
      "        CULTURE & ARTS  DIVORCE  EDUCATION  ...  TECH  THE WORLDPOST  TRAVEL  \\\n",
      "0                    0        0          0  ...     0              0       0   \n",
      "1                    0        0          0  ...     0              0       0   \n",
      "2                    0        0          0  ...     0              0       0   \n",
      "3                    0        0          0  ...     0              0       0   \n",
      "4                    0        0          0  ...     0              0       0   \n",
      "...                ...      ...        ...  ...   ...            ...     ...   \n",
      "209522               0        0          0  ...     1              0       0   \n",
      "209523               0        0          0  ...     0              0       0   \n",
      "209524               0        0          0  ...     0              0       0   \n",
      "209525               0        0          0  ...     0              0       0   \n",
      "209526               0        0          0  ...     0              0       0   \n",
      "\n",
      "        U.S. NEWS  WEDDINGS  WEIRD NEWS  WELLNESS  WOMEN  WORLD NEWS  \\\n",
      "0               1         0           0         0      0           0   \n",
      "1               1         0           0         0      0           0   \n",
      "2               0         0           0         0      0           0   \n",
      "3               0         0           0         0      0           0   \n",
      "4               1         0           0         0      0           0   \n",
      "...           ...       ...         ...       ...    ...         ...   \n",
      "209522          0         0           0         0      0           0   \n",
      "209523          0         0           0         0      0           0   \n",
      "209524          0         0           0         0      0           0   \n",
      "209525          0         0           0         0      0           0   \n",
      "209526          0         0           0         0      0           0   \n",
      "\n",
      "        WORLDPOST  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "...           ...  \n",
      "209522          0  \n",
      "209523          0  \n",
      "209524          0  \n",
      "209525          0  \n",
      "209526          0  \n",
      "\n",
      "[209527 rows x 42 columns]\n",
      "  (0, 0)\t2022.0\n",
      "  (0, 1)\t9.0\n",
      "  (0, 2)\t23.0\n",
      "  (0, 3)\t76.0\n",
      "  (0, 4)\t154.0\n",
      "  (0, 5)\t11.0\n",
      "  (0, 6)\t29.0\n",
      "  (0, 21)\t1.0\n",
      "  (0, 4813)\t0.4014786693070016\n",
      "  (0, 8663)\t0.29929734165964034\n",
      "  (0, 36456)\t0.35457348185482146\n",
      "  (0, 25921)\t0.4387121214999312\n",
      "  (0, 33874)\t0.4272960037329111\n",
      "  (0, 31468)\t0.3108011569310383\n",
      "  (0, 1743)\t0.20030711712887878\n",
      "  (0, 23619)\t0.24267544754358217\n",
      "  (0, 14431)\t0.2298950887091365\n",
      "  (0, 58600)\t0.21400704810038232\n",
      "  (0, 75485)\t0.28045662422535655\n",
      "  (0, 47728)\t0.35317215989031575\n",
      "  (0, 74091)\t0.13753342119029918\n",
      "  (0, 55843)\t0.332513472938695\n",
      "  (0, 72122)\t0.19500690559077039\n",
      "  (0, 84363)\t0.24227370797257375\n",
      "  (0, 64467)\t0.18029182290051832\n",
      "  :\t:\n",
      "  (209526, 16)\t1.0\n",
      "  (209526, 17)\t1.0\n",
      "  (209526, 21)\t1.0\n",
      "  (209526, 36566)\t0.40594519679276264\n",
      "  (209526, 11600)\t0.4447067286327728\n",
      "  (209526, 17597)\t0.464908393519722\n",
      "  (209526, 17711)\t0.3632839196341211\n",
      "  (209526, 21798)\t0.28835477226059664\n",
      "  (209526, 22162)\t0.3257039586832296\n",
      "  (209526, 31264)\t0.3163824563600451\n",
      "  (209526, 91083)\t0.3846719712124488\n",
      "  (209526, 51456)\t0.27574102190935923\n",
      "  (209526, 89098)\t0.330253478734662\n",
      "  (209526, 82670)\t0.28609037880888466\n",
      "  (209526, 75563)\t0.2978890088340583\n",
      "  (209526, 69784)\t0.25945065465480016\n",
      "  (209526, 90219)\t0.35357159900719864\n",
      "  (209526, 49825)\t0.22630141779833543\n",
      "  (209526, 74256)\t0.1952530005594735\n",
      "  (209526, 60325)\t0.22080397740533095\n",
      "  (209526, 87009)\t0.19932619914443978\n",
      "  (209526, 59384)\t0.186234776288964\n",
      "  (209526, 91146)\t0.19737183479567053\n",
      "  (209526, 89902)\t0.13861674049531952\n",
      "  (209526, 89712)\t0.18258940024862164\n"
     ]
    }
   ],
   "source": [
    "#pip install category_encoders\n",
    "\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# change date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "# word count\n",
    "df['headline_word_count'] = df['headline'].apply(lambda x: len(str(x).split()))\n",
    "df['description_word_count'] = df['short_description'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# encode authors using Binary encoding\n",
    "encoder = BinaryEncoder(cols=['authors'], return_df=True)\n",
    "df_encoded = encoder.fit_transform(df['authors'])\n",
    "df_encoded_sparse = csr_matrix(df_encoded.values)\n",
    "\n",
    "# drop extra columns\n",
    "selected_columns = ['year', 'month', 'day', 'headline_length', 'short_description_length', 'headline_word_count', 'description_word_count' ]\n",
    "new_df = df[selected_columns].copy()\n",
    "# combine\n",
    "original_data = hstack([csr_matrix(new_df), df_encoded_sparse,tfidf])\n",
    "\n",
    "# encode category using label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['category']) # use for Logistic Regression, Random Forest, and XGBoost\n",
    "\n",
    "y_categorical = pd.get_dummies(df['category']) # use for Artificial Neural Network and Convolutional Neural Network \n",
    "\n",
    "print(y)\n",
    "print(y_categorical)\n",
    "print(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative explained variance ratio with 100 components: 0.9998\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality Reduction using TruncatedSVD (which is better for sparse matrix)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "n_components = 100\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "reduced_data = svd.fit_transform(original_data)\n",
    "\n",
    "# Calculate cumulative explained variance ratio\n",
    "cumulative_explained_variance_ratio = sum(svd.explained_variance_ratio_)\n",
    "print(f\"Cumulative explained variance ratio with {n_components} components: {cumulative_explained_variance_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "# Create an instance of the UMAP model\n",
    "reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1,  n_components=2, metric='euclidean')\n",
    "\n",
    "# Fit the model to your TF-IDF data and transform the data\n",
    "reduced_data_umap = reducer.fit_transform(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we did dimensionality reduction useing TruncatedSVD. We chose 100 as n_components and get 99.98% cumulative explained variance ratio which effectively reduces the dimensionality of the data while retaining almost all of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Implementation and Evaluation\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "According to [XGBoost: Introduction to XGBoost Algorithm in Machine Learning](https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost), it is a machine learning algorithm, utilizes the gradient boosting frameowrk, and would combine multiple individual models, often decision trees, to build an ensemble learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "def handle_xgboost(X, y):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\tX_train_csr = csr_matrix(X_train)\n",
    "\tX_test_csr = csr_matrix(X_test)\n",
    "\n",
    "\tdtrain = xgb.DMatrix(X_train_csr, label=y_train)\n",
    "\tdtest = xgb.DMatrix(X_test_csr, label=y_test)\n",
    "\n",
    "\t# Define your model parameters\n",
    "\tparams = {\n",
    "\t    'objective': 'multi:softmax',  # Use 'binary:logistic' if you have two classes\n",
    "\t    'num_class': len(np.unique(y)),  # Needed for multi-class classification\n",
    "\t\t\t'tree_method': 'hist',  # Faster histogram optimized algorithm\n",
    "\t}\n",
    "\tevals = [(dtest, 'eval')]\n",
    "\tnum_boost_round = 1000  # Set higher for early stopping\n",
    "\tearly_stopping_rounds = 10\n",
    "\n",
    "\t# Train the model\n",
    "\tbst = xgb.train(params, dtrain, num_boost_round, evals=evals, early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "\t# Make predictions\n",
    "\tpredictions = bst.predict(dtest)\n",
    "\treturn y_test, predictions\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def print_precision(y_test, p):\n",
    "\t# Accuracy\n",
    "\taccuracy = accuracy_score(y_test, p)\n",
    "\tprint(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\t# Precision\n",
    "\tprecision = precision_score(y_test, p, average='macro')\n",
    "\tprint(f\"Precision: {precision}\")\n",
    "\n",
    "\t# Recall\n",
    "\trecall = recall_score(y_test, p, average='macro')\n",
    "\tprint(f\"Recall: {recall}\")\n",
    "\n",
    "\t# F1 Score\n",
    "\tf1 = f1_score(y_test, p, average='macro')\n",
    "\tprint(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "During my processing period, the key difference is its effectiveness. It is too slow to run on the whole dataset, but also the parameter tuning. With default parameter and a tfidf dataset, the XGBoost model was running 74 minutes in my computer, but still not end. \n",
    "\n",
    "Think of previous assignment, dimension reduction maybe a good way. So we tried the PCA analysis. After running the model with a 100 features PCA result, we get the precision: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pca, p_pca = handle_xgboost(reduced_data, y)\n",
    "print_precision(y_pca, p_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems not so well but can be updated, and it takes 3.5 minutes. But I hope to reduce more and make it within 1 minutes.\n",
    "\n",
    "So I tried to reduce the train model size. With a 0.98 test size, which means only use 0.02, nearly 4,000 rows to train the model, the time is only 14s. However, the result is \n",
    "\n",
    "```\n",
    "Accuracy: 0.4427404705435455\n",
    "Precision: 0.41335755375629696\n",
    "Recall: 0.25124844089816784\n",
    "F1 Score: 0.2880839218375771\n",
    "```\n",
    "\n",
    "It is not so good. I think the amount is too less and make the whole model underfit. \n",
    "\n",
    "\n",
    "Then we tried the UMAP to reduce the whole dataset to 2d. The efficiency improves greatly, we only need 17s to run the whole dataset. However, the precision is not so good: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_umap, p_umap = handle_xgboost(reduced_data_umap, y)\n",
    "print_precision(y_umap, p_umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worse than only use 2% data. Tthe UMAP ignores too many features.\n",
    "\n",
    "After studying, I found that the stratify sample may be a good way to find a good parameter. And for avoiding the computing burdern of all parameter combination, we decided only consider this 2 kinds of parameters: `max_depth` and `reg_lambda`.\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'reg_lambda': [1, 10]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # Assuming X, y represent your full dataset\n",
    "# # Select a random subset\n",
    "# X_subset, X_val, y_subset, y_val = train_test_split(reduced_data, y, test_size=0.9, stratify=y, random_state=42)\n",
    "\n",
    "# # Define your model and parameter grid\n",
    "# model = XGBClassifier()\n",
    "# param_grid = {\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'reg_lambda': [1, 10],\n",
    "# }\n",
    "\n",
    "# # Perform grid search on the subset\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "# grid_search.fit(X_subset, y_subset)\n",
    "\n",
    "# # Sorting the results by the mean test score in descending order\n",
    "# results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "# sorted_results_df = results_df.sort_values(by='rank_test_score')\n",
    "\n",
    "# print(sorted_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1 hour running, We found max_depth =7 and reg_lambda = 10 is the best, but the time is so long that I cannot run other parameters. We would run XGBoost model with this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reduced_data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_csr = csr_matrix(X_train)\n",
    "X_test_csr = csr_matrix(X_test)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_csr, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_csr, label=y_test)\n",
    "\n",
    "# Define your model parameters\n",
    "params = {\n",
    "    'objective': 'multi:softprob',  # Use 'binary:logistic' if you have two classes\n",
    "    'num_class': len(np.unique(y)),  # Needed for multi-class classification\n",
    "\t\t'tree_method': 'hist',  # Faster histogram optimized algorithm\n",
    "\t\t'max_depth': 7,\n",
    "\t\t'reg_lambda': 10\n",
    "}\n",
    "evals = [(dtest, 'eval')]\n",
    "num_boost_round = 1000  # Set higher for early stopping\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_boost_round, evals=evals, early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "# Make predictions\n",
    "predictions = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With above XGBoost model, we get the result, confusion matrix and ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert probability predictions to predicted class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print_precision(y_test, predicted_labels)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test contains your true labels\n",
    "true_labels = y_test  # Replace y_test with your actual true labels\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)  # Use predictions_binary or the equivalent for your case\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Class1', 'Class2'], yticklabels=['Class1', 'Class2'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(conf_matrix)\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Assuming you have n_classes classes\n",
    "n_classes = len(np.unique(true_labels))  # Make sure true_labels is your array of true labels\n",
    "\n",
    "true_labels_binarized = label_binarize(true_labels, classes=range(n_classes))\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(true_labels_binarized[:, i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "plt.figure(figsize=(7, 7))\n",
    "colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems work well in ROC curve and confusion matrix, but the accuracy, precision, recall, and f1score are not so good. What I get is the model works well in some classes, but not so good in the precision part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
