{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing:\n",
    "1.1 Load the dataset and perform initial exploration to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           headline   category  \\\n",
      "0           0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1           1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2           2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3           3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4           4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "         date  headline_length  short_description_length  \n",
      "0  2022-09-23               76                       154  \n",
      "1  2022-09-23               89                       159  \n",
      "2  2022-09-23               69                        64  \n",
      "3  2022-09-23               56                       159  \n",
      "4  2022-09-22               77                       156  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('News_Category_Dataset_v3.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Clean the text data, including removing special characters, stopwords, applying lowercasing, correcting spelling, standardizing, handling contractions, and lemtization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/muyangcheng329/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/muyangcheng329/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pkg_resources\n",
    "import inflect\n",
    "import contractions\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p = inflect.engine()\n",
    "\n",
    "def standardize_numbers(text):\n",
    "    return ' '.join([p.number_to_words(word) if word.isdigit() else word for word in text.split()])\n",
    "\n",
    "def handle_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # standardize\n",
    "    text = standardize_numbers(text)\n",
    "    # handle contractions\n",
    "    text = handle_contractions(text)\n",
    "    # correct typos\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)\n",
    "        corrected_words.append(suggestions[0].term if suggestions else word)\n",
    "    text = ' '.join(corrected_words)\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # remove stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # rejoin words\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# clean data\n",
    "df['cleaned_headline'] = df['headline'].apply(clean_text)\n",
    "df['cleaned_description'] = df['short_description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Perform text tokenization and vectorization using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_headline = tfidf_vectorizer.fit_transform(df['cleaned_headline'])\n",
    "# df_headline_tfidf = pd.DataFrame(tfidf_headline.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# df_headline_tfidf = df_headline_tfidf.add_prefix('headline_')\n",
    "\n",
    "tfidf_description = tfidf_vectorizer.fit_transform(df['cleaned_description'])\n",
    "# df_description_tfidf = pd.DataFrame(tfidf_description.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# df_description_tfidf = df_description_tfidf.add_prefix('description_')\n",
    "\n",
    "# df = pd.concat([df, df_headline_tfidf, df_description_tfidf], axis=1)\n",
    "tfidf= hstack([tfidf_headline,tfidf_description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Extract and analyze different features from the text that might be useful for classification, such as word count,\n",
    "sentence length, n-grams, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4715/4715 [==============================] - 38s 8ms/step - loss: 2.3925 - accuracy: 0.3846 - val_loss: 2.1582 - val_accuracy: 0.4366\n",
      "Epoch 2/5\n",
      "4715/4715 [==============================] - 38s 8ms/step - loss: 1.9319 - accuracy: 0.4770 - val_loss: 2.1329 - val_accuracy: 0.4393\n",
      "Epoch 3/5\n",
      "4715/4715 [==============================] - 38s 8ms/step - loss: 1.6420 - accuracy: 0.5417 - val_loss: 2.2317 - val_accuracy: 0.4256\n",
      "Epoch 4/5\n",
      "4715/4715 [==============================] - 39s 8ms/step - loss: 1.3447 - accuracy: 0.6228 - val_loss: 2.4428 - val_accuracy: 0.4051\n",
      "Epoch 5/5\n",
      "4715/4715 [==============================] - 41s 9ms/step - loss: 1.0661 - accuracy: 0.7032 - val_loss: 2.7807 - val_accuracy: 0.3909\n",
      "1310/1310 [==============================] - 2s 2ms/step - loss: 2.7777 - accuracy: 0.3915\n",
      "Test score: 2.7776715755462646\n",
      "Test accuracy: 0.3914952576160431\n"
     ]
    }
   ],
   "source": [
    "#pip install category_encoders\n",
    "\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# change date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "# word count\n",
    "df['headline_word_count'] = df['headline'].apply(lambda x: len(str(x).split()))\n",
    "df['description_word_count'] = df['short_description'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# encode authors using Binary encoding\n",
    "encoder = BinaryEncoder(cols=['authors'], return_df=True)\n",
    "df_encoded = encoder.fit_transform(df['authors'])\n",
    "df_encoded_sparse = csr_matrix(df_encoded.values)\n",
    "\n",
    "# drop extra columns\n",
    "selected_columns = ['year', 'month', 'day', 'headline_length', 'short_description_length', 'headline_word_count', 'description_word_count' ]\n",
    "new_df = df[selected_columns].copy()\n",
    "# combine\n",
    "original_data = hstack([csr_matrix(new_df), df_encoded_sparse,tfidf])\n",
    "\n",
    "# encode category using label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['category']) # use for Logistic Regression, Random Forest, and XGBoost\n",
    "\n",
    "y_categorical = pd.get_dummies(df['category']) # use for Artificial Neural Network and Convolutional Neural Network \n",
    "\n",
    "# print(y)\n",
    "# print(y_categorical)\n",
    "# print(original_data)\n",
    "#Convolutional Neural Network (CNN): Implement a CNN to capture local dependencies in text data. Discuss the choice of the architecture in the context of text data.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode category labels with LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['category_encoded'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# We will use 'cleaned_description' as the input feature\n",
    "texts = df['cleaned_description'].astype(str)\n",
    "\n",
    "# Tokenize and encode the texts\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(f'sequences:{sequences}')\n",
    "\n",
    "\n",
    "# Use pad_sequences to ensure uniform input sequence length\n",
    "data_padded = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Prepare labels\n",
    "labels = np.array(df['category_encoded'])\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Define the CNN model structure\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=100, input_length=100))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model performance on the test set\n",
    "score, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test score: {score}')\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "# Epoch 1/5\n",
    "# 4715/4715 [==============================] - 36s 8ms/step - loss: 2.3940 - accuracy: 0.3846 - val_loss: 2.1523 - val_accuracy: 0.4363\n",
    "# Epoch 2/5\n",
    "# 4715/4715 [==============================] - 36s 8ms/step - loss: 1.9341 - accuracy: 0.4775 - val_loss: 2.1369 - val_accuracy: 0.4406\n",
    "# Epoch 3/5\n",
    "# 4715/4715 [==============================] - 36s 8ms/step - loss: 1.6529 - accuracy: 0.5400 - val_loss: 2.2381 - val_accuracy: 0.4259\n",
    "# Epoch 4/5\n",
    "# 4715/4715 [==============================] - 36s 8ms/step - loss: 1.3577 - accuracy: 0.6185 - val_loss: 2.4587 - val_accuracy: 0.4098\n",
    "# Epoch 5/5\n",
    "# 4715/4715 [==============================] - 36s 8ms/step - loss: 1.0805 - accuracy: 0.6988 - val_loss: 2.7785 - val_accuracy: 0.3966\n",
    "# 1310/1310 [==============================] - 2s 2ms/step - loss: 2.7542 - accuracy: 0.3964\n",
    "# Test score: 2.754225492477417\n",
    "# Test accuracy: 0.39638715982437134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction using TruncatedSVD (which is better for sparse matrix)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "n_components = 100\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "reduced_data = svd.fit_transform(original_data)\n",
    "\n",
    "# Calculate cumulative explained variance ratio\n",
    "cumulative_explained_variance_ratio = sum(svd.explained_variance_ratio_)\n",
    "print(f\"Cumulative explained variance ratio with {n_components} components: {cumulative_explained_variance_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we did dimensionality reduction useing TruncatedSVD. We chose 100 as n_components and get 99.98% cumulative explained variance ratio which effectively reduces the dimensionality of the data while retaining almost all of the information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
