{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing:\n",
    "1.1 Load the dataset and perform initial exploration to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           headline   category  \\\n",
      "0           0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1           1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2           2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3           3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4           4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "         date  headline_length  short_description_length  \n",
      "0  2022-09-23               76                       154  \n",
      "1  2022-09-23               89                       159  \n",
      "2  2022-09-23               69                        64  \n",
      "3  2022-09-23               56                       159  \n",
      "4  2022-09-22               77                       156  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('News_Category_Dataset_v3.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Clean the text data, including removing special characters, stopwords, applying lowercasing, correcting spelling, standardizing, handling contractions, and lemtization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/muyangcheng329/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/muyangcheng329/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pkg_resources\n",
    "import inflect\n",
    "import contractions\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p = inflect.engine()\n",
    "\n",
    "def standardize_numbers(text):\n",
    "    return ' '.join([p.number_to_words(word) if word.isdigit() else word for word in text.split()])\n",
    "\n",
    "def handle_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # standardize\n",
    "    text = standardize_numbers(text)\n",
    "    # handle contractions\n",
    "    text = handle_contractions(text)\n",
    "    # correct typos\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)\n",
    "        corrected_words.append(suggestions[0].term if suggestions else word)\n",
    "    text = ' '.join(corrected_words)\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # remove stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # rejoin words\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# clean data\n",
    "df['cleaned_headline'] = df['headline'].apply(clean_text)\n",
    "df['cleaned_description'] = df['short_description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Perform text tokenization and vectorization using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_headline = tfidf_vectorizer.fit_transform(df['cleaned_headline'])\n",
    "# df_headline_tfidf = pd.DataFrame(tfidf_headline.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# df_headline_tfidf = df_headline_tfidf.add_prefix('headline_')\n",
    "\n",
    "tfidf_description = tfidf_vectorizer.fit_transform(df['cleaned_description'])\n",
    "# df_description_tfidf = pd.DataFrame(tfidf_description.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# df_description_tfidf = df_description_tfidf.add_prefix('description_')\n",
    "\n",
    "# df = pd.concat([df, df_headline_tfidf, df_description_tfidf], axis=1)\n",
    "tfidf= hstack([tfidf_headline,tfidf_description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Extract and analyze different features from the text that might be useful for classification, such as word count,\n",
    "sentence length, n-grams, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install category_encoders\n",
    "\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "# change date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "# word count\n",
    "df['headline_word_count'] = df['headline'].apply(lambda x: len(str(x).split()))\n",
    "df['description_word_count'] = df['short_description'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# encode authors using Binary encoding\n",
    "encoder = BinaryEncoder(cols=['authors'], return_df=True)\n",
    "df_encoded = encoder.fit_transform(df['authors'])\n",
    "df_encoded_sparse = csr_matrix(df_encoded.values)\n",
    "\n",
    "# drop extra columns\n",
    "selected_columns = ['year', 'month', 'day', 'headline_length', 'short_description_length', 'headline_word_count', 'description_word_count' ]\n",
    "new_df = df[selected_columns].copy()\n",
    "# combine\n",
    "data = hstack([csr_matrix(new_df), df_encoded_sparse,tfidf])\n",
    "\n",
    "# print(data)\n",
    "\n",
    "#Logistic Regression: Use as a baseline model to understand the linear separability of text categories.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = data  \n",
    "y = df['category']  \n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the logistic regression model\n",
    "model = LogisticRegression(max_iter=500)  # Increase iteration count to ensure convergence\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)  # zero_division=0\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Accuracy: 0.20483940247219967\n",
    "# Precision: 0.06672682431636665\n",
    "# Recall: 0.20483940247219967\n",
    "# F1 Score: 0.08638762416940857\n",
    "\n",
    "\n",
    "# Accuracy: The model correctly predicts 20.48% of the samples. This is a relatively low value, indicating that the model is not very good at correctly classifying data points in the test set overall.\n",
    "\n",
    "# Precision: The weighted average precision is 6.67%. Precision refers to the proportion of samples predicted as positive that are actually positive. This very low value indicates that only a small fraction of the samples predicted by the model as positive are correct.\n",
    "\n",
    "# Recall: The weighted average recall is the same as accuracy, at 20.48%. Recall calculates the proportion of samples that were predicted as positive and are actually positive out of all actual positive samples. This metric indicates the proportion of true positive samples the model is able to identify.\n",
    "\n",
    "# F1 Score: The weighted average F1 score is 8.64%. The F1 score is the harmonic mean of precision and recall, trying to provide a balance between the two. A low F1 score indicates a lack of balance between precision and recall, with poor performance in both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
