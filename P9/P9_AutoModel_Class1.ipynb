{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModel class for more flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities for \"The best movie I've ever watched!\": tensor([0.4189, 0.5811], grad_fn=<UnbindBackward0>)\n",
      "Predicted class name for \"The best movie I've ever watched!\": LABEL_1\n",
      "Predicted probabilities for \"What an awful movie. I regret watching it.\": tensor([0.8513, 0.1487], grad_fn=<UnbindBackward0>)\n",
      "Predicted class name for \"What an awful movie. I regret watching it.\": LABEL_0\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Load the tokenizer and pre-trained model\n",
    "# ------------------------------\n",
    "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# ------------------------------\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "# ------------------------------\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "# print the predicted probabilities and class names\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "for i, prob in enumerate(probs):\n",
    "    print(f\"Predicted probabilities for \\\"{text[i]}\\\": {prob}\")\n",
    "    print(f\"Predicted class name for \\\"{text[i]}\\\": {model.config.id2label[predicted_classes[i]]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the tokenizer class do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1996, 2190, 3185,  102],\n",
      "        [ 101, 2054, 2019, 9643,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n",
      "['the', 'best', 'movie', 'i', \"'\", 've', 'ever', 'watched', '!']\n",
      "[1996, 2190, 3185, 1045, 1005, 2310, 2412, 3427, 999]\n",
      "the best movie i've ever watched!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "print(tokenizer(text, return_tensors=\"pt\", padding=True)  ) # padding=True adds padding tokens to the input\n",
    "# either max_length or padding can be used to specify the maximum length of the input, but not both\n",
    "\n",
    "\n",
    "tokens = tokenizer.tokenize(text[0])\n",
    "print(tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(decoded)\n",
    "\n",
    "# attention mask = 0 means ignore the token, 1 means consider the token\n",
    "# padding mask = 0 means the token is a padding token, 1 means it is not\n",
    "\n",
    "# decoding removes the capitalization and adds spaces between tokens\n",
    "# notice two tokens are added at the beginning and end of the sentence\n",
    "# [CLS] and [SEP] tokens are added to the beginning and end of the sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline vs automodel\n",
    "Sentiment analysis with and without pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment from task_pipeline: NEGATIVE, Confidence from task_pipeline: 0.9996139407157898\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Import pipeline\n",
    "# --------------------\n",
    "from transformers import pipeline\n",
    "\n",
    "# --------------------\n",
    "# Create the task pipeline\n",
    "# --------------------\n",
    "model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "task_pipeline = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", tokenizer=tokenizer)\n",
    "\n",
    "text = \"this a non sentence. I am not sure what to do with it.\"\n",
    "\n",
    "# --------------------\n",
    "# Predict the sentiment\n",
    "# --------------------\n",
    "task_output = task_pipeline(text)\n",
    "\n",
    "print(f\"Sentiment from task_pipeline: {task_output[0]['label']}, Confidence from task_pipeline: {task_output[0]['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'don', \"'\", 't', 'know', 'how', 'i', 'feel', 'about', 'this', 'movie', '.']\n",
      "[1045, 2123, 1005, 1056, 2113, 2129, 1045, 2514, 2055, 2023, 3185, 1012]\n",
      "i don't know how i feel about this movie.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# Now using the tokenizer to tokenize the text\n",
    "# ----------------------------------------\n",
    "\n",
    "X_train = [\"I love this movie because it is meant for me!\", \n",
    "\"I hate this movie.  It did not have a sequence or a plot.\", \n",
    "\"I don't know how I feel about this movie.\"]\n",
    "\n",
    "tokens = tokenizer.tokenize(X_train[2])\n",
    "print(tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998345375061035}, {'label': 'NEGATIVE', 'score': 0.9997679591178894}, {'label': 'NEGATIVE', 'score': 0.9971460700035095}]\n",
      "{'input_ids': tensor([[ 101, 1045, 2293, 2023, 3185, 2138, 2009, 2003, 3214, 2005, 2033,  999,\n",
      "          102,    0,    0,    0,    0],\n",
      "        [ 101, 1045, 5223, 2023, 3185, 1012, 2009, 2106, 2025, 2031, 1037, 5537,\n",
      "         2030, 1037, 5436, 1012,  102],\n",
      "        [ 101, 1045, 2123, 1005, 1056, 2113, 2129, 1045, 2514, 2055, 2023, 3185,\n",
      "         1012,  102,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = [\"I love this movie because it is meant for me!\", \n",
    "\"I hate this movie.  It did not have a sequence or a plot.\", \n",
    "\"I don't know how I feel about this movie.\"]\n",
    "\n",
    "print(task_pipeline(X_train))\n",
    "\n",
    "# play with the parameters of the tokenizer\n",
    "batch = tokenizer(X_train, padding=True, truncation=True,max_length = 20, return_tensors=\"pt\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2293, 2023, 3185, 2138, 2009, 2003, 3214, 2005, 2033,  999,\n",
      "          102,    0,    0,    0,    0],\n",
      "        [ 101, 1045, 5223, 2023, 3185, 1012, 2009, 2106, 2025, 2031, 1037, 5537,\n",
      "         2030, 1037, 5436, 1012,  102],\n",
      "        [ 101, 1045, 2123, 1005, 1056, 2113, 2129, 1045, 2514, 2055, 2023, 3185,\n",
      "         1012,  102,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.1754,  4.5310],\n",
      "        [ 4.6287, -3.7397],\n",
      "        [ 3.2484, -2.6078]]), hidden_states=None, attentions=None)\n",
      "tensor([[1.6549e-04, 9.9983e-01],\n",
      "        [9.9977e-01, 2.3204e-04],\n",
      "        [9.9715e-01, 2.8539e-03]])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# Using AutoModelForSequenceClassification to load the model and pass the batch to the model for inference\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True,max_length = 20, return_tensors=\"pt\")\n",
    "print(batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    print(probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison of AutoModels and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the pipeline class:\n",
      "Sentence: POSITIVE, Sentiment: 1.00\n",
      "Sentence: NEGATIVE, Sentiment: 1.00\n",
      "Sentence: NEGATIVE, Sentiment: 1.00\n",
      "\n",
      "Using the AutoModel class:\n",
      "Sentence: I love this movie! It's fantastic., Sentiment: Positive\n",
      "Sentence: The book was okay, but not great., Sentiment: Negative\n",
      "Sentence: I had a terrible experience at the restaurant., Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Using the pipeline class\n",
    "def use_pipeline():\n",
    "    # Create a sentiment analysis pipeline\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    # Define some example sentences\n",
    "    sentences = [\n",
    "        \"I love this movie! It's fantastic.\",\n",
    "        \"The book was okay, but not great.\",\n",
    "        \"I had a terrible experience at the restaurant.\"\n",
    "    ]\n",
    "\n",
    "    # Use the pipeline to analyze sentiment\n",
    "    results = sentiment_pipeline(sentences)\n",
    "\n",
    "    # Print the results\n",
    "    for result in results:\n",
    "        print(f\"Sentence: {result['label']}, Sentiment: {result['score']:.2f}\")\n",
    "\n",
    "# Using the AutoModel class\n",
    "def use_automodel():\n",
    "    # Load the pre-trained model and tokenizer\n",
    "    model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    tokenizer1 = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # Define some example sentences\n",
    "    sentences = [\n",
    "        \"I love this movie! It's fantastic.\",\n",
    "        \"The book was okay, but not great.\",\n",
    "        \"I had a terrible experience at the restaurant.\"\n",
    "    ]\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    encoded_inputs = tokenizer1(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Make predictions\n",
    "    outputs = model(**encoded_inputs)\n",
    "    predicted_labels = outputs.logits.argmax(dim=1)\n",
    "\n",
    "    # Print the results\n",
    "    for sentence, label in zip(sentences, predicted_labels):\n",
    "        sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
    "        print(f\"Sentence: {sentence}, Sentiment: {sentiment}\")\n",
    "\n",
    "# Run the examples\n",
    "print(\"Using the pipeline class:\")\n",
    "use_pipeline()\n",
    "\n",
    "print(\"\\nUsing the AutoModel class:\")\n",
    "use_automodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2293,  2023,  3185,   999,  2009,  1005,  1055, 10392,\n",
       "          1012,   102],\n",
       "        [  101,  1996,  2338,  2001,  3100,  1010,  2021,  2025,  2307,  1012,\n",
       "           102,     0],\n",
       "        [  101,  1045,  2018,  1037,  6659,  3325,  2012,  1996,  4825,  1012,\n",
       "           102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# dir(tokenizer)[-40:]\n",
    "sentences = [\n",
    "        \"I love this movie! It's fantastic.\",\n",
    "        \"The book was okay, but not great.\",\n",
    "        \"I had a terrible experience at the restaurant.\"\n",
    "    ]\n",
    "# Tokenize the sentences\n",
    "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\",   max_length = 15, )\n",
    "encoded_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AutoTokenizer in module transformers.models.auto.tokenization_auto:\n",
      "\n",
      "class AutoTokenizer(builtins.object)\n",
      " |  This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when\n",
      " |  created with the [`AutoTokenizer.from_pretrained`] class method.\n",
      " |  \n",
      " |  This class cannot be instantiated directly using `__init__()` (throws an error).\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False)\n",
      " |      Register a new tokenizer in this mapping.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          config_class ([`PretrainedConfig`]):\n",
      " |              The configuration corresponding to the model to register.\n",
      " |          slow_tokenizer_class ([`PretrainedTokenizer`], *optional*):\n",
      " |              The slow tokenizer to register.\n",
      " |          fast_tokenizer_class ([`PretrainedTokenizerFast`], *optional*):\n",
      " |              The fast tokenizer to register.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs) from builtins.type\n",
      " |      Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.\n",
      " |      \n",
      " |      The tokenizer class to instantiate is selected based on the `model_type` property of the config object (either\n",
      " |      passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
      " |      falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
      " |      \n",
      " |          - **albert** -- [`AlbertTokenizer`] or [`AlbertTokenizerFast`] (ALBERT model)\n",
      " |          - **align** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ALIGN model)\n",
      " |          - **bark** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Bark model)\n",
      " |          - **bart** -- [`BartTokenizer`] or [`BartTokenizerFast`] (BART model)\n",
      " |          - **barthez** -- [`BarthezTokenizer`] or [`BarthezTokenizerFast`] (BARThez model)\n",
      " |          - **bartpho** -- [`BartphoTokenizer`] (BARTpho model)\n",
      " |          - **bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BERT model)\n",
      " |          - **bert-generation** -- [`BertGenerationTokenizer`] (Bert Generation model)\n",
      " |          - **bert-japanese** -- [`BertJapaneseTokenizer`] (BertJapanese model)\n",
      " |          - **bertweet** -- [`BertweetTokenizer`] (BERTweet model)\n",
      " |          - **big_bird** -- [`BigBirdTokenizer`] or [`BigBirdTokenizerFast`] (BigBird model)\n",
      " |          - **bigbird_pegasus** -- [`PegasusTokenizer`] or [`PegasusTokenizerFast`] (BigBird-Pegasus model)\n",
      " |          - **biogpt** -- [`BioGptTokenizer`] (BioGpt model)\n",
      " |          - **blenderbot** -- [`BlenderbotTokenizer`] or [`BlenderbotTokenizerFast`] (Blenderbot model)\n",
      " |          - **blenderbot-small** -- [`BlenderbotSmallTokenizer`] (BlenderbotSmall model)\n",
      " |          - **blip** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BLIP model)\n",
      " |          - **blip-2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (BLIP-2 model)\n",
      " |          - **bloom** -- [`BloomTokenizerFast`] (BLOOM model)\n",
      " |          - **bridgetower** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (BridgeTower model)\n",
      " |          - **bros** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BROS model)\n",
      " |          - **byt5** -- [`ByT5Tokenizer`] (ByT5 model)\n",
      " |          - **camembert** -- [`CamembertTokenizer`] or [`CamembertTokenizerFast`] (CamemBERT model)\n",
      " |          - **canine** -- [`CanineTokenizer`] (CANINE model)\n",
      " |          - **chinese_clip** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Chinese-CLIP model)\n",
      " |          - **clap** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (CLAP model)\n",
      " |          - **clip** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (CLIP model)\n",
      " |          - **clipseg** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (CLIPSeg model)\n",
      " |          - **clvp** -- [`ClvpTokenizer`] (CLVP model)\n",
      " |          - **code_llama** -- [`CodeLlamaTokenizer`] or [`CodeLlamaTokenizerFast`] (CodeLlama model)\n",
      " |          - **codegen** -- [`CodeGenTokenizer`] or [`CodeGenTokenizerFast`] (CodeGen model)\n",
      " |          - **convbert** -- [`ConvBertTokenizer`] or [`ConvBertTokenizerFast`] (ConvBERT model)\n",
      " |          - **cpm** -- [`CpmTokenizer`] or [`CpmTokenizerFast`] (CPM model)\n",
      " |          - **cpmant** -- [`CpmAntTokenizer`] (CPM-Ant model)\n",
      " |          - **ctrl** -- [`CTRLTokenizer`] (CTRL model)\n",
      " |          - **data2vec-audio** -- [`Wav2Vec2CTCTokenizer`] (Data2VecAudio model)\n",
      " |          - **data2vec-text** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (Data2VecText model)\n",
      " |          - **deberta** -- [`DebertaTokenizer`] or [`DebertaTokenizerFast`] (DeBERTa model)\n",
      " |          - **deberta-v2** -- [`DebertaV2Tokenizer`] or [`DebertaV2TokenizerFast`] (DeBERTa-v2 model)\n",
      " |          - **distilbert** -- [`DistilBertTokenizer`] or [`DistilBertTokenizerFast`] (DistilBERT model)\n",
      " |          - **dpr** -- [`DPRQuestionEncoderTokenizer`] or [`DPRQuestionEncoderTokenizerFast`] (DPR model)\n",
      " |          - **electra** -- [`ElectraTokenizer`] or [`ElectraTokenizerFast`] (ELECTRA model)\n",
      " |          - **ernie** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ERNIE model)\n",
      " |          - **ernie_m** -- [`ErnieMTokenizer`] (ErnieM model)\n",
      " |          - **esm** -- [`EsmTokenizer`] (ESM model)\n",
      " |          - **falcon** -- [`PreTrainedTokenizerFast`] (Falcon model)\n",
      " |          - **fastspeech2_conformer** --  (FastSpeech2Conformer model)\n",
      " |          - **flaubert** -- [`FlaubertTokenizer`] (FlauBERT model)\n",
      " |          - **fnet** -- [`FNetTokenizer`] or [`FNetTokenizerFast`] (FNet model)\n",
      " |          - **fsmt** -- [`FSMTTokenizer`] (FairSeq Machine-Translation model)\n",
      " |          - **funnel** -- [`FunnelTokenizer`] or [`FunnelTokenizerFast`] (Funnel Transformer model)\n",
      " |          - **gemma** -- [`GemmaTokenizer`] or [`GemmaTokenizerFast`] (Gemma model)\n",
      " |          - **git** -- [`BertTokenizer`] or [`BertTokenizerFast`] (GIT model)\n",
      " |          - **gpt-sw3** -- [`GPTSw3Tokenizer`] (GPT-Sw3 model)\n",
      " |          - **gpt2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (OpenAI GPT-2 model)\n",
      " |          - **gpt_bigcode** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPTBigCode model)\n",
      " |          - **gpt_neo** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPT Neo model)\n",
      " |          - **gpt_neox** -- [`GPTNeoXTokenizerFast`] (GPT NeoX model)\n",
      " |          - **gpt_neox_japanese** -- [`GPTNeoXJapaneseTokenizer`] (GPT NeoX Japanese model)\n",
      " |          - **gptj** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPT-J model)\n",
      " |          - **gptsan-japanese** -- [`GPTSanJapaneseTokenizer`] (GPTSAN-japanese model)\n",
      " |          - **groupvit** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (GroupViT model)\n",
      " |          - **herbert** -- [`HerbertTokenizer`] or [`HerbertTokenizerFast`] (HerBERT model)\n",
      " |          - **hubert** -- [`Wav2Vec2CTCTokenizer`] (Hubert model)\n",
      " |          - **ibert** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (I-BERT model)\n",
      " |          - **idefics** -- [`LlamaTokenizerFast`] (IDEFICS model)\n",
      " |          - **instructblip** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (InstructBLIP model)\n",
      " |          - **jukebox** -- [`JukeboxTokenizer`] (Jukebox model)\n",
      " |          - **kosmos-2** -- [`XLMRobertaTokenizer`] or [`XLMRobertaTokenizerFast`] (KOSMOS-2 model)\n",
      " |          - **layoutlm** -- [`LayoutLMTokenizer`] or [`LayoutLMTokenizerFast`] (LayoutLM model)\n",
      " |          - **layoutlmv2** -- [`LayoutLMv2Tokenizer`] or [`LayoutLMv2TokenizerFast`] (LayoutLMv2 model)\n",
      " |          - **layoutlmv3** -- [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`] (LayoutLMv3 model)\n",
      " |          - **layoutxlm** -- [`LayoutXLMTokenizer`] or [`LayoutXLMTokenizerFast`] (LayoutXLM model)\n",
      " |          - **led** -- [`LEDTokenizer`] or [`LEDTokenizerFast`] (LED model)\n",
      " |          - **lilt** -- [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`] (LiLT model)\n",
      " |          - **llama** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaMA model)\n",
      " |          - **llava** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaVa model)\n",
      " |          - **longformer** -- [`LongformerTokenizer`] or [`LongformerTokenizerFast`] (Longformer model)\n",
      " |          - **longt5** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (LongT5 model)\n",
      " |          - **luke** -- [`LukeTokenizer`] (LUKE model)\n",
      " |          - **lxmert** -- [`LxmertTokenizer`] or [`LxmertTokenizerFast`] (LXMERT model)\n",
      " |          - **m2m_100** -- [`M2M100Tokenizer`] (M2M100 model)\n",
      " |          - **marian** -- [`MarianTokenizer`] (Marian model)\n",
      " |          - **mbart** -- [`MBartTokenizer`] or [`MBartTokenizerFast`] (mBART model)\n",
      " |          - **mbart50** -- [`MBart50Tokenizer`] or [`MBart50TokenizerFast`] (mBART-50 model)\n",
      " |          - **mega** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (MEGA model)\n",
      " |          - **megatron-bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Megatron-BERT model)\n",
      " |          - **mgp-str** -- [`MgpstrTokenizer`] (MGP-STR model)\n",
      " |          - **mistral** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Mistral model)\n",
      " |          - **mixtral** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Mixtral model)\n",
      " |          - **mluke** -- [`MLukeTokenizer`] (mLUKE model)\n",
      " |          - **mobilebert** -- [`MobileBertTokenizer`] or [`MobileBertTokenizerFast`] (MobileBERT model)\n",
      " |          - **mpnet** -- [`MPNetTokenizer`] or [`MPNetTokenizerFast`] (MPNet model)\n",
      " |          - **mpt** -- [`GPTNeoXTokenizerFast`] (MPT model)\n",
      " |          - **mra** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (MRA model)\n",
      " |          - **mt5** -- [`MT5Tokenizer`] or [`MT5TokenizerFast`] (MT5 model)\n",
      " |          - **musicgen** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (MusicGen model)\n",
      " |          - **mvp** -- [`MvpTokenizer`] or [`MvpTokenizerFast`] (MVP model)\n",
      " |          - **nezha** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Nezha model)\n",
      " |          - **nllb** -- [`NllbTokenizer`] or [`NllbTokenizerFast`] (NLLB model)\n",
      " |          - **nllb-moe** -- [`NllbTokenizer`] or [`NllbTokenizerFast`] (NLLB-MOE model)\n",
      " |          - **nystromformer** -- [`AlbertTokenizer`] or [`AlbertTokenizerFast`] (NystrÃ¶mformer model)\n",
      " |          - **oneformer** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OneFormer model)\n",
      " |          - **openai-gpt** -- [`OpenAIGPTTokenizer`] or [`OpenAIGPTTokenizerFast`] (OpenAI GPT model)\n",
      " |          - **opt** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (OPT model)\n",
      " |          - **owlv2** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OWLv2 model)\n",
      " |          - **owlvit** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OWL-ViT model)\n",
      " |          - **pegasus** -- [`PegasusTokenizer`] or [`PegasusTokenizerFast`] (Pegasus model)\n",
      " |          - **pegasus_x** -- [`PegasusTokenizer`] or [`PegasusTokenizerFast`] (PEGASUS-X model)\n",
      " |          - **perceiver** -- [`PerceiverTokenizer`] (Perceiver model)\n",
      " |          - **persimmon** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Persimmon model)\n",
      " |          - **phi** -- [`CodeGenTokenizer`] or [`CodeGenTokenizerFast`] (Phi model)\n",
      " |          - **phobert** -- [`PhobertTokenizer`] (PhoBERT model)\n",
      " |          - **pix2struct** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (Pix2Struct model)\n",
      " |          - **plbart** -- [`PLBartTokenizer`] (PLBart model)\n",
      " |          - **prophetnet** -- [`ProphetNetTokenizer`] (ProphetNet model)\n",
      " |          - **qdqbert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (QDQBert model)\n",
      " |          - **qwen2** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2 model)\n",
      " |          - **rag** -- [`RagTokenizer`] (RAG model)\n",
      " |          - **realm** -- [`RealmTokenizer`] or [`RealmTokenizerFast`] (REALM model)\n",
      " |          - **reformer** -- [`ReformerTokenizer`] or [`ReformerTokenizerFast`] (Reformer model)\n",
      " |          - **rembert** -- [`RemBertTokenizer`] or [`RemBertTokenizerFast`] (RemBERT model)\n",
      " |          - **retribert** -- [`RetriBertTokenizer`] or [`RetriBertTokenizerFast`] (RetriBERT model)\n",
      " |          - **roberta** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (RoBERTa model)\n",
      " |          - **roberta-prelayernorm** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (RoBERTa-PreLayerNorm model)\n",
      " |          - **roc_bert** -- [`RoCBertTokenizer`] (RoCBert model)\n",
      " |          - **roformer** -- [`RoFormerTokenizer`] or [`RoFormerTokenizerFast`] (RoFormer model)\n",
      " |          - **rwkv** -- [`GPTNeoXTokenizerFast`] (RWKV model)\n",
      " |          - **seamless_m4t** -- [`SeamlessM4TTokenizer`] or [`SeamlessM4TTokenizerFast`] (SeamlessM4T model)\n",
      " |          - **seamless_m4t_v2** -- [`SeamlessM4TTokenizer`] or [`SeamlessM4TTokenizerFast`] (SeamlessM4Tv2 model)\n",
      " |          - **siglip** -- [`SiglipTokenizer`] (SigLIP model)\n",
      " |          - **speech_to_text** -- [`Speech2TextTokenizer`] (Speech2Text model)\n",
      " |          - **speech_to_text_2** -- [`Speech2Text2Tokenizer`] (Speech2Text2 model)\n",
      " |          - **speecht5** -- [`SpeechT5Tokenizer`] (SpeechT5 model)\n",
      " |          - **splinter** -- [`SplinterTokenizer`] or [`SplinterTokenizerFast`] (Splinter model)\n",
      " |          - **squeezebert** -- [`SqueezeBertTokenizer`] or [`SqueezeBertTokenizerFast`] (SqueezeBERT model)\n",
      " |          - **stablelm** -- [`GPTNeoXTokenizerFast`] (StableLm model)\n",
      " |          - **switch_transformers** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (SwitchTransformers model)\n",
      " |          - **t5** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (T5 model)\n",
      " |          - **tapas** -- [`TapasTokenizer`] (TAPAS model)\n",
      " |          - **tapex** -- [`TapexTokenizer`] (TAPEX model)\n",
      " |          - **transfo-xl** -- [`TransfoXLTokenizer`] (Transformer-XL model)\n",
      " |          - **tvp** -- [`BertTokenizer`] or [`BertTokenizerFast`] (TVP model)\n",
      " |          - **umt5** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (UMT5 model)\n",
      " |          - **vilt** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ViLT model)\n",
      " |          - **vipllava** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (VipLlava model)\n",
      " |          - **visual_bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (VisualBERT model)\n",
      " |          - **vits** -- [`VitsTokenizer`] (VITS model)\n",
      " |          - **wav2vec2** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2 model)\n",
      " |          - **wav2vec2-bert** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2-BERT model)\n",
      " |          - **wav2vec2-conformer** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2-Conformer model)\n",
      " |          - **wav2vec2_phoneme** -- [`Wav2Vec2PhonemeCTCTokenizer`] (Wav2Vec2Phoneme model)\n",
      " |          - **whisper** -- [`WhisperTokenizer`] or [`WhisperTokenizerFast`] (Whisper model)\n",
      " |          - **xclip** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (X-CLIP model)\n",
      " |          - **xglm** -- [`XGLMTokenizer`] or [`XGLMTokenizerFast`] (XGLM model)\n",
      " |          - **xlm** -- [`XLMTokenizer`] (XLM model)\n",
      " |          - **xlm-prophetnet** -- [`XLMProphetNetTokenizer`] (XLM-ProphetNet model)\n",
      " |          - **xlm-roberta** -- [`XLMRobertaTokenizer`] or [`XLMRobertaTokenizerFast`] (XLM-RoBERTa model)\n",
      " |          - **xlm-roberta-xl** -- [`XLMRobertaTokenizer`] or [`XLMRobertaTokenizerFast`] (XLM-RoBERTa-XL model)\n",
      " |          - **xlnet** -- [`XLNetTokenizer`] or [`XLNetTokenizerFast`] (XLNet model)\n",
      " |          - **xmod** -- [`XLMRobertaTokenizer`] or [`XLMRobertaTokenizerFast`] (X-MOD model)\n",
      " |          - **yoso** -- [`AlbertTokenizer`] or [`AlbertTokenizerFast`] (YOSO model)\n",
      " |      \n",
      " |      Params:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                  - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                    using the [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n",
      " |                  - A path or url to a single saved vocabulary file if and only if the tokenizer only requires a\n",
      " |                    single vocabulary file (like Bert or XLNet), e.g.: `./my_model_directory/vocab.txt`. (Not\n",
      " |                    applicable to all derived classes)\n",
      " |          inputs (additional positional arguments, *optional*):\n",
      " |              Will be passed along to the Tokenizer `__init__()` method.\n",
      " |          config ([`PretrainedConfig`], *optional*)\n",
      " |              The configuration object used to determine the tokenizer class to instantiate.\n",
      " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download the model weights and configuration files and override the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      " |              file exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          use_fast (`bool`, *optional*, defaults to `True`):\n",
      " |              Use a [fast Rust-based tokenizer](https://huggingface.co/docs/tokenizers/index) if it is supported for\n",
      " |              a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer\n",
      " |              is returned instead.\n",
      " |          tokenizer_type (`str`, *optional*):\n",
      " |              Tokenizer type to be loaded.\n",
      " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
      " |              should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      " |              execute code present on the Hub on your local machine.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the Tokenizer `__init__()` method. Can be used to set special tokens like\n",
      " |              `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      " |              `additional_special_tokens`. See parameters in the `__init__()` for more details.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      \n",
      " |      >>> # Download vocabulary from huggingface.co and cache.\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      " |      \n",
      " |      >>> # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      " |      \n",
      " |      >>> # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      " |      >>> # tokenizer = AutoTokenizer.from_pretrained(\"./test/bert_saved_model/\")\n",
      " |      \n",
      " |      >>> # Download vocabulary from huggingface.co and define model-specific arguments\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "help(AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
