{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 30, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Your max_length is set to 30, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 30, but your input_length is only 6. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 30, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Your max_length is set to 30, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Your max_length is set to 30, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Your max_length is set to 30, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "Your max_length is set to 30, but your input_length is only 17. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Your max_length is set to 30, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Your max_length is set to 30, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Your max_length is set to 30, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Your max_length is set to 30, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Your max_length is set to 30, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    dr. goldberg offers everything i look for in a...   \n",
      "1    Unfortunately, the frustration of being Dr. Go...   \n",
      "2    Been going to Dr. Goldberg for over 10 years. ...   \n",
      "3    Got a letter in the mail last week that said D...   \n",
      "4    I don't know what Dr. Goldberg was like before...   \n",
      "..                                                 ...   \n",
      "145  Even when we didn't have a car Filene's Baseme...   \n",
      "146  Love this store!  Don't always have much luck ...   \n",
      "147  Another store which has gone the way of the Do...   \n",
      "148  $9.75 for a red bull and vodka? I'm sorry, I t...   \n",
      "149  Really enjoyed this a lot more than I thought ...   \n",
      "\n",
      "                                               summary  stars  \n",
      "0     dr. goldberg offers everything i look for in ...      4  \n",
      "1    The frustration of being Dr. Goldberg's patien...      1  \n",
      "2    I've been going to Dr. Goldberg for over 10 ye...      3  \n",
      "3    Dr. Goldberg is moving to Arizona to take a ne...      3  \n",
      "4    Dr. Goldberg is only interested in the co-pay ...      0  \n",
      "..                                                 ...    ...  \n",
      "145  Filene's Basement was worth the bus trip to th...      3  \n",
      "146  \"Don't always have much luck with the sales, b...      4  \n",
      "147  There was nothing basement like about it. What...      1  \n",
      "148  $9.75 for a red bull and vodka? I'm sorry, I t...      1  \n",
      "149  The drinks were on the pricey side, but I didn...      2  \n",
      "\n",
      "[150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('yelp_review_full')\n",
    "df = dataset['train'].to_pandas()  \n",
    "\n",
    "# Create a text summarization pipeline using the BART model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Initialize an empty list to store summaries\n",
    "summaries = []\n",
    "\n",
    "# Dynamically adjust max_length and summarize the text\n",
    "for text in df['text'].iloc[:150].tolist():\n",
    "    input_length = len(text.split())  # Estimate the number of words in the input\n",
    "    max_length = min(130, max(30, int(input_length * 0.5)))  # Dynamically adjust max_length\n",
    "    summary = summarizer(text, max_length=max_length, min_length=30, truncation=True)[0]['summary_text']\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Create a new dataframe to store the original texts and their summaries\n",
    "# Make sure to also copy the star rating information, assuming in the original dataset the star rating is stored in the 'label' column\n",
    "df_summary = df.iloc[:150].copy()  # Copy the first 150 rows\n",
    "df_summary['summary'] = summaries  # Add the summaries to the new dataframe\n",
    "df_summary['stars'] = df['label'].iloc[:150]  # Copy the corresponding star ratings\n",
    "\n",
    "print(df_summary[['text', 'summary', 'stars']])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels - 1  # Convert labels from 1-5 to 0-4\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare the data\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "X = df_summary['summary'].tolist()  # Summary texts\n",
    "y = df_summary['stars'].to_numpy()  # Star ratings\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2021)\n",
    "\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = YelpReviewDataset(train_encodings, y_train)\n",
    "val_dataset = YelpReviewDataset(val_encodings, y_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition, Training and Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9a4abf650449cdbf7e0c6935476584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2102, 'grad_norm': 5.436811923980713, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.67}\n",
      "{'loss': 1.2112, 'grad_norm': 5.261388301849365, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.33}\n",
      "{'loss': 1.1035, 'grad_norm': 5.03201150894165, 'learning_rate': 3e-06, 'epoch': 2.0}\n",
      "{'loss': 1.143, 'grad_norm': 5.402960777282715, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.67}\n",
      "{'train_runtime': 23.2161, 'train_samples_per_second': 15.506, 'train_steps_per_second': 1.938, 'train_loss': 1.1458648787604437, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46b1085af4246c6a90e1cf76401282e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2915648221969604,\n",
       " 'eval_runtime': 0.5079,\n",
       " 'eval_samples_per_second': 59.072,\n",
       " 'eval_steps_per_second': 7.876,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Result List\n",
    "5   'train_loss': 1.1762760480244954 'eval_loss': 1.5997843742370605\n",
    "\n",
    "15  'train_loss': 1.3541520436604817 'eval_loss': 0.7361515164375305\n",
    "\n",
    "30  'train_loss': 1.2990621990627713 'eval_loss': 1.175505518913269\n",
    "\n",
    "100 'train_loss': 1.2386985460917155 'eval_loss': 1.1782324314117432\n",
    "\n",
    "150 'train_loss': 1.1339557965596516 'eval_loss': 1.2087987661361694\n",
    "\n",
    "The second result in the list shows an intriguing case where the evaluation loss is significantly lower than the training loss. This is an unusual scenario in machine learning models because typically, the training loss is lower due to the model being directly trained on that data. Here, an eval_loss of 0.736 compared to a train_loss of 1.354 suggests a few possible scenarios:\n",
    "\n",
    "Strong Generalization: The model may generalize exceptionally well to the validation data, possibly because the validation set characteristics closely match the model's training data, or the model has learned robust features that work well across varied data samples.\n",
    "\n",
    "Data Distribution: The distribution of data in the training set might be more challenging or diverse compared to the evaluation set, which can lead to higher training loss.\n",
    "\n",
    "Evaluation Set Size: If the evaluation set is significantly smaller or not as diverse, it might not represent the difficulty of the overall dataset, leading to a deceptively low evaluation loss.\n",
    "\n",
    "Overfitting Avoidance: It's also possible that certain regularization techniques or early stopping mechanisms were effective in preventing the model from overfitting, hence the model performs better on unseen data.\n",
    "\n",
    "Random Variation: Especially with smaller datasets, random variation can sometimes lead to situations where evaluation loss is lower than training loss.\n",
    "\n",
    "# The number 300 is used to specify the number of text entries from the Yelp review dataset that we want to process.\n",
    "\n",
    "1.Data Selection: df['text'].iloc[:300].tolist() – This line selects the first 300 entries from the 'text' column of the dataframe df. The .iloc[:300] is an indexing operation used in pandas to select data by position. Here, it slices the first 300 rows of the dataframe.\n",
    "\n",
    "2.Summarization Process: The loop for text in df['text'].iloc[:300].tolist() iterates over each of the first 300 text entries. For each text, the script estimates the input length, adjusts the maximum summary length accordingly, and then generates a summary using the BART model.\n",
    "\n",
    "# Ideal Range\n",
    "\n",
    "Close to 0 but not 0: Ideally, the loss value should be close to 0, which indicates that the model's predictions are very close to the actual labels. However, a loss value of 0 might suggest that the model is overfitting on the training set, perfectly memorizing the training data, which could lead to poor performance on unseen data.\n",
    "\n",
    "Eval loss close to train loss: If the evaluation loss (eval_loss) and training loss (train_loss) are very close, it is usually a good sign, indicating that the model performs consistently on both the training and validation sets, demonstrating good generalization ability.\n",
    "\n",
    "# Specific Range\n",
    "\n",
    "Loss value: The specific range of loss values greatly depends on the characteristics of the dataset and the complexity of the model. For some tasks, such as simple classification problems, the loss value might be between 0.01 and 0.5. For more complex tasks or more challenging datasets, the loss value might range from 1 to 10, or even higher.\n",
    "\n",
    "Difference: The difference between the training loss and evaluation loss should be as small as possible. A good rule of thumb is that the difference should not exceed 10% to 20% of the training loss. For example, if the train_loss is 0.5, the eval_loss should ideally be between 0.5 and 0.6.\n",
    "\n",
    "# Loss Values in Our Model\n",
    "\n",
    "Training Loss (train_loss) of 1.1339557965596516: This indicates the average degree of error the model has when recognizing patterns in the training data. For a complex NLP task, this value might be considered reasonable in the initial stages, especially considering the use of a large pre-trained model and a complex dataset.\n",
    "\n",
    "Evaluation Loss (eval_loss) of 1.2087987661361694: This is slightly higher than the training loss. Such a scenario is common in machine learning models, as the evaluation loss represents the model's performance on unseen data, which is usually slightly higher than the training loss.\n",
    "\n",
    "# Judging Whether Loss Values are \"High\" or \"Low\"\n",
    "\n",
    "Generally, for text classification tasks, if both training and evaluation losses are below 1.0, this is usually considered good performance. However, even if loss values are between 1.0 and 2.0, this may still be acceptable performance depending on the task's complexity and the characteristics of the dataset.\n",
    "\n",
    "It's important to monitor how loss values change with training progress. Ideally, you would want to see both training and evaluation losses gradually decrease as the number of epochs increases, and the gap between them should not significantly widen over time, as this may indicate overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
