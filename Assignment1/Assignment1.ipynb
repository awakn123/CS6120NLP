{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Read data from patient notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install textblob\n",
    "import pandas as pd\n",
    "import re\n",
    "#from textblob import TextBlob\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('patient_notes.csv')\n",
    "\n",
    "# Case Conversion\n",
    "df['pn_history'] = df['pn_history'].str.lower()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Case Conversion and iii.\tCorrecting Typos and Spelling\n",
    "Removing Puncutation and Special Characters seems would influence standardizing formatsand handling contractions parts, so it is moved to later.\n",
    "\n",
    "We tried the TextBlob. But after taking 17 minutes that the first 100 rows have not been handled, we plan to try some other ways.\n",
    "\n",
    "Then I tried the symspellpy. However, it cannot be installed by conda, only can be dealt with by pip. So I move to SpaCy.\n",
    "\n",
    "Then I found the pyspellchecker is something I must use with SpaCy and also not able to installed by conda. So I return to symspellpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install symspellpy\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "# Initialize and load SymSpell\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# Function to correct spelling in a sentence\n",
    "def correct_spelling(text):\n",
    "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else text\n",
    "\n",
    "# Function to apply correct_spelling to a Series\n",
    "def apply_correct_spelling(series):\n",
    "    return series.apply(correct_spelling)\n",
    "\n",
    "# Function to parallelize\n",
    "def parallelize_dataframe(df, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df_combined = pd.concat(pool.map(apply_correct_spelling, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df_combined\n",
    "\n",
    "# Applying the function in parallel\n",
    "# df_head = df.head(1000)\n",
    "if __name__ == 'main':\n",
    "\tdf['pn_history'] = parallelize_dataframe(df['pn_history'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th Standardizing Formats 5th Handling Contractions\n",
    "Standardizing Formats for numbers: \"17\", “17-year-old”, “3-4”, or “17yo”\n",
    "\n",
    "Handling Contractions: Expanding contractions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install inflect\n",
    "# pip install contractions\n",
    "import inflect\n",
    "import contractions\n",
    "\n",
    "# Create an engine for inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "# Standardizing Formats \"17\", “17-year-old”, “3-4”, or “17yo”\n",
    "def convert_numbers_in_string(s):\n",
    "    # Find all numbers in the string\n",
    "    numbers_in_words = re.findall(r'\\b\\d+\\b', s)\n",
    "    numbers_in_parts = re.findall(r'\\d+', s)\n",
    "\n",
    "    # For each number\n",
    "    for number in numbers_in_words:\n",
    "        # Convert the number to words\n",
    "        word = p.number_to_words(number)\n",
    "        # Replace the number with the word in the string\n",
    "        s = re.sub(r'\\b' + number + r'\\b', word, s)\n",
    "\n",
    "    for number in numbers_in_parts:\n",
    "        # Convert the number to words\n",
    "        word = p.number_to_words(number)\n",
    "        # Replace the number with the word (followed by a space) in the string\n",
    "        s = re.sub(number, word + \" \", s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# Handling Contractions: Expanding contractions\" \n",
    "def expand_contractions(text):\n",
    "    # Define a dictionary of contractions and their expanded forms\n",
    "    custom_contractions = {\n",
    "        \"yo\": \"year old\",\n",
    "        \"y.o.\": \"year old\",\n",
    "        \"y/o\": \"year old\",\n",
    "        \"y\": \"year\",\n",
    "        \"yr\": \"year\",\n",
    "        \"f\": \"female\",\n",
    "        \"m\": \"male\",\n",
    "        \"mo\": \"month\",\n",
    "        \"yr\": \"year\",\n",
    "        \"c/o\": \"complains of\",\n",
    "        \"c/of\": \"complains of\",\n",
    "        \"c/m\": \"complains of\",\n",
    "        \"cc\": \"chief complaint\",\n",
    "        \"h/o\": \"history of\",\n",
    "        \"pt\": \"patient\",\n",
    "        \"w\": \"week\",\n",
    "        \"wk\": \"week\",\n",
    "        \"hrs\": \"hours\",\n",
    "        \"hx\": \"history\",\n",
    "        \"pmh\": \"past medical history\",\n",
    "        \"pmhx\": \"past medical history\",\n",
    "        \"psh\": \"past surgical history\",\n",
    "        \"psurghx\": \"past surgical history\",\n",
    "        \"pshh\": \"past surgical history\",\n",
    "        \"meds\": \"medications\",\n",
    "        \"hosp\": \"hosipital\",\n",
    "        \"fh\": \"Family history\",\n",
    "        \"fhx\": \"Family history\",\n",
    "        \"fmh\": \"Family history\",\n",
    "        \"sh\":\"social history\",\n",
    "        \"soc\": \"social history\",\n",
    "        \"Rx\": \"prescription\",\n",
    "        \"ros\":\"review of systems\",\n",
    "        \"hpi\": \"history of present illness\"\n",
    "    }\n",
    "\n",
    "    # First, use the contractions library to expand common English contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Then, handle the custom contractions\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # For each word in the text\n",
    "    for i in range(len(words)):\n",
    "        # If the word is a contraction\n",
    "        if words[i] in custom_contractions:\n",
    "            # Replace the contraction with its expanded form in the text\n",
    "            words[i] = custom_contractions[words[i]]\n",
    "    # Join the words back into a text string\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    # Fill NaN values with a default value\n",
    "    df.fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Convert columns to string type\n",
    "    df['pn_history'] = df['pn_history'].astype(str)\n",
    "\n",
    "    df['pn_history'] = df['pn_history'].apply(convert_numbers_in_string)\n",
    "\n",
    "    # Apply the function to the 'pn_history' column\n",
    "    df['pn_history'] = df['pn_history'].apply(expand_contractions)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = process_data(df)\n",
    "\n",
    "# Write the data back to the CSV file\n",
    "# df.to_csv('data.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd. Removing Puncutation and Special Characters parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation and Special Characters\n",
    "df['pn_history'] = df['pn_history'].str.replace(r'[^a-zA-Z0-9 ]', ' ', regex=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th. Stemming\n",
    "This part would apply the stemming. I choose LancasterStemmer because it seems to be most efficient to find the root and would reduce the number of tokens, which make the last cluster easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Create an instance of LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Apply LancasterStemmer to the pn_history column\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([stemmer.stem(word) for word in re.findall(r'\\w+', x)]))\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th. Lemmatizer\n",
    "Apply lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in re.findall(r'\\w+', x)]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7th. Stop list\n",
    "Apply a stop word list to filter out unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Apply stop word filtering to the pn_history column\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([word for word in re.findall(r'\\w+', x) if word not in stop_words]))\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "### a.CountVectorizer\n",
    "Use CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge scikit-learn\n",
    "#pip install -U scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the \"pn_history\" column\n",
    "dtm = vectorizer.fit_transform(df['pn_history'])\n",
    "\n",
    "# Convert the DTM to a DataFrame\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DTM DataFrame\n",
    "print(dtm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = dtm.shape[0] * dtm.shape[1]\n",
    "nonzero_elements = dtm.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "dtm_df.info()\n",
    "print('Memory Usage:', dtm.data.nbytes/ (1024**3), 'GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DTM has 42,146 documents(patient's notes) and 29,081 unique terms. With a sparsity level of approximately 99.72%, it shows that the majority of the matrix elements are zero, meaning each document contains only a small subset of the total terms. Each element represents the frequency of a term in this document and is stored as an int64 type, and the total memory usage of the matrix is around 9.1 GB (If it is stored in csr_matrix, only occupy about 0.026 GB). This highlights the DTM as a highly sparse and memory-intensive structure, which is typical in text analysis involving large text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.TfidfVectorizer\n",
    "Use TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "# Fit and transform the \"pn_history\" column\n",
    "dtm_tfidf = vectorizer.fit_transform(df['pn_history'])\n",
    "\n",
    "# Convert the DTM to a DataFrame\n",
    "dtm_tfidf_df = pd.DataFrame(dtm_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF DTM DataFrame\n",
    "print(dtm_tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = dtm_tfidf.shape[0] * dtm_tfidf.shape[1]\n",
    "nonzero_elements = dtm_tfidf.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "dtm_tfidf_df.info()\n",
    "print('Memory Usage:', dtm_tfidf.data.nbytes/ (1024**3), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has 42,146 rows and 29,081 columns, corresponding to the documents(patient's notes) and terms respectively. The data type of all cells is float64. Each cell represents the TF-IDF score(which is in default setting) of a term in a specific document. The memory usage is substantial at 9.1 GB (If it is stored in csr_matrix, only occupy about 0.026 GB), indicating that while the matrix is sparse, it still occupies a significant amount of memory when converted to a dense format. The sparsity of the DTM is about 99.72%, which means the vast majority of the matrix elements are zeros. This reflects the common case in text data where most terms appear in only a small number of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.n-gram\n",
    "After studying the n-gram, I think we could try to find gram from bigger to smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Load your CSV file\n",
    "# Assuming the CSV has a column 'patient_notes' with the text data\n",
    "patient_notes = df['pn_history'].astype(str)\n",
    "\n",
    "# Optional: Download NLTK stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 4), stop_words=stopwords.words('english'))\n",
    "\n",
    "# Fit and transform the patient notes\n",
    "X = vectorizer.fit_transform(patient_notes)\n",
    "\n",
    "# Convert to DataFrame for better readability (optional)\n",
    "#dtm = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Now dtm is your document-term matrix with unigrams, bigrams, trigrams, and 4-grams\n",
    "#print(dtm.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = X.shape[0] * X.shape[1]\n",
    "nonzero_elements = X.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "print('DTM Shape:', X.shape)\n",
    "print('Data Types:', X.dtype)\n",
    "print('Memory Usage:', X.data.nbytes/ (1024**3), 'GB')  # Convert bytes to GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DTM, created using CountVectorizer with an n-gram range of 1 to 4 and English stop words, comprises 42,146 documents(patient's notes) and spans 3,928,802 unique n-grams. Each cell is int64 type and a cell's value indicates how many times the corresponding n-gram appears in that document. It exhibits an extremely high sparsity level of approximately 99.9904%, indicating that the vast majority of the matrix entries are zeros. Despite its vast dimensionality, the actual memory footprint is notably low at around 0.118 GB, thanks to its sparse matrix format, making it efficient for storing and processing such a large-scale text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "### a.Normalization and Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Compute the mean TF-IDF value for each document\n",
    "doc_mean_tfidf = np.array(dtm_tfidf.mean(axis=1)).flatten()\n",
    "\n",
    "# Calculate the overall mean and standard deviation of these mean values\n",
    "mean = np.mean(doc_mean_tfidf)\n",
    "std = np.std(doc_mean_tfidf)\n",
    "\n",
    "# Define the threshold for outliers (e.g., mean + 2 * standard deviation)\n",
    "threshold = mean + 2 * std\n",
    "\n",
    "# Identify outliers as those documents whose mean TF-IDF value exceeds the threshold\n",
    "outlier_indices = np.where(doc_mean_tfidf > threshold)[0]\n",
    "\n",
    "# Culculate percentage of outliers\n",
    "outlier_percentage = len(outlier_indices) / len(doc_mean_tfidf)\n",
    "print(outlier_percentage)\n",
    "\n",
    "# Generate outlier picture\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(doc_mean_tfidf)), doc_mean_tfidf, label='Documents')\n",
    "plt.scatter(outlier_indices, doc_mean_tfidf[outlier_indices], color='red', label='Outliers')\n",
    "plt.axhline(y=threshold, color='green', linestyle='--', label='Outlier Threshold')\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Mean TF-IDF Value')\n",
    "plt.title('Mean TF-IDF Values with Outliers')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Filter the DTM to only include non-outlier documents\n",
    "non_outlier_mask = doc_mean_tfidf <= threshold\n",
    "filtered_dtm = dtm_tfidf[non_outlier_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the mean TF-IDF value of each document to do normalization for the data. We identify outliers as those documents with mean TF-IDF values significantly exceeding the average, determined as being beyond two standard deviations from the mean. These outliers, constituting 1.5% of our dataset, are presumed to contain a high frequency of unusual terms, possibly typographical errors or incorrect terminology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.Dimension Reduction\n",
    " We use UMAP (Uniform Manifold Approximation and Projection) as the choice for dimensionality reduction in this context. The reason being, it can effectively visualize high-dimensional datasets in lower-dimensional space, thereby revealing the natural clusters in the dataset.\n",
    "\n",
    "We’ve chosen the parameters for UMAP as follows:random_state: given the stochastic nature of UMAP, we’ve used the random_state parameter to ensure reproducibility of results. n_neighbors: we’ve carefully selected this parameter which is 15 to balance between preserving the local and global structure of the data. min_dist: we use the default value 0.1 to control how tightly UMAP is allowed to pack points together in the lower-dimensional representation. A smaller value results in a “clumpier” visualization where points are packed closely together.n_components: it determined the dimensionality of the reduced space. In this case, we choose 2 and 3. metric: we’ve chosen the Euclidean metric for computing distance in the original high-dimensional space. This choice reflects the nature of the data, where features are all measured on compatible scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "# Create an instance of the UMAP model\n",
    "reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1,  n_components=2, metric='euclidean')\n",
    "\n",
    "# Fit the model to your TF-IDF data and transform the data\n",
    "umap_embedding = reducer.fit_transform(filtered_dtm)\n",
    "\n",
    "print(umap_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(umap_embedding[:, 0], umap_embedding[:, 1])\n",
    "plt.title('UMAP Results', fontsize=20)\n",
    "plt.xlabel('UMAP_1')\n",
    "plt.ylabel('UMAP_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Measures\n",
    "mean_distance = np.mean(np.linalg.norm(umap_embedding, axis=1))\n",
    "median_distance = np.median(np.linalg.norm(umap_embedding, axis=1))\n",
    "\n",
    "print(f'Mean distance from origin: {mean_distance}')\n",
    "print(f'Median distance from origin: {median_distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and median distances from the origin are approximately 10.90 and 10.84, respectively. This suggests that the physician’s notes are spread out from the origin in the reduced dimensional space, indicating a diversity in the content. The closeness of the mean and median suggests a symmetrical distribution around the origin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the UMAP model\n",
    "reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1, n_components=3, metric='euclidean')\n",
    "\n",
    "# Fit the model to your TF-IDF data and transform the data\n",
    "umap_embedding = reducer.fit_transform(filtered_dtm)\n",
    "\n",
    "print(umap_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(umap_embedding[:, 0], umap_embedding[:, 1], umap_embedding[:, 2], s=100)\n",
    "plt.title('UMAP Results', fontsize=20)\n",
    "plt.xlabel('UMAP_1')\n",
    "plt.ylabel('UMAP_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Measures\n",
    "mean_distance = np.mean(np.linalg.norm(umap_embedding, axis=1))\n",
    "median_distance = np.median(np.linalg.norm(umap_embedding, axis=1))\n",
    "\n",
    "print(f'Mean distance from origin: {mean_distance}')\n",
    "print(f'Median distance from origin: {median_distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and median distances from the origin are approximately 11.53 and 12.88, respectively. This suggests that the physician’s notes are spread out from the origin in the reduced dimensional space, indicating a diversity in the content. The closeness of the mean and median suggests a symmetrical distribution around the origin. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS6120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
