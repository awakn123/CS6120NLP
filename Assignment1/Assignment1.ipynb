{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Read data from patient notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install textblob\n",
    "import pandas as pd\n",
    "import re\n",
    "#from textblob import TextBlob\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('patient_notes.csv')\n",
    "\n",
    "# Case Conversion\n",
    "df['pn_history'] = df['pn_history'].str.lower()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Case Conversion and iii.\tCorrecting Typos and Spelling\n",
    "Removing Puncutation and Special Characters seems would influence standardizing formatsand handling contractions parts, so it is moved to later.\n",
    "\n",
    "We tried the TextBlob. But after taking 17 minutes that the first 100 rows have not been handled, we plan to try some other ways.\n",
    "\n",
    "Then I tried the symspellpy. However, it cannot be installed by conda, only can be dealt with by pip. So I move to SpaCy.\n",
    "\n",
    "Then I found the pyspellchecker is something I must use with SpaCy and also not able to installed by conda. So I return to symspellpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install symspellpy\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "# Initialize and load SymSpell\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# Function to correct spelling in a sentence\n",
    "def correct_spelling(text):\n",
    "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else text\n",
    "\n",
    "# Function to apply correct_spelling to a Series\n",
    "def apply_correct_spelling(series):\n",
    "    return series.apply(correct_spelling)\n",
    "\n",
    "# Function to parallelize\n",
    "def parallelize_dataframe(df, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df_combined = pd.concat(pool.map(apply_correct_spelling, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df_combined\n",
    "\n",
    "# Applying the function in parallel\n",
    "# df_head = df.head(1000)\n",
    "if __name__ == 'main':\n",
    "\tdf['pn_history'] = parallelize_dataframe(df['pn_history'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th Standardizing Formats 5th Handling Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install inflect\n",
    "# pip install contractions\n",
    "import inflect\n",
    "import contractions\n",
    "\n",
    "# Create an engine for inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "# Standardizing Formats \"17\", “17-year-old”, “3-4”, or “17yo”\n",
    "def convert_numbers_in_string(s):\n",
    "    # Find all numbers in the string\n",
    "    numbers_in_words = re.findall(r'\\b\\d+\\b', s)\n",
    "    numbers_in_parts = re.findall(r'\\d+', s)\n",
    "\n",
    "    # For each number\n",
    "    for number in numbers_in_words:\n",
    "        # Convert the number to words\n",
    "        word = p.number_to_words(number)\n",
    "        # Replace the number with the word in the string\n",
    "        s = re.sub(r'\\b' + number + r'\\b', word, s)\n",
    "\n",
    "    for number in numbers_in_parts:\n",
    "        # Convert the number to words\n",
    "        word = p.number_to_words(number)\n",
    "        # Replace the number with the word (followed by a space) in the string\n",
    "        s = re.sub(number, word + \" \", s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# Handling Contractions: Expanding contractions\" \n",
    "# \"yo\", \"y.o.\" \"y/o\" to \"year old\",  \n",
    "# \"y\" to \"year\"\n",
    "# \"f\" to \"female\", \n",
    "# \"m\" to \"male\", \n",
    "# \"mo\" to \"month\", \n",
    "# \"yr\" to \"year\"\n",
    "# \"c/o\" \"c/of\" \"c/m\" to \"complains of\",\n",
    "# \"cc\" to \"chief complaint\"\n",
    "# \"h/o\" to \"history of\"\n",
    "# \"pt\" to \"patient\"\n",
    "# \"w\", \"wk\" to \"week\", \"hrs\" to \"hours\"\"\n",
    "# \"x\" to times?\n",
    "def expand_contractions(text):\n",
    "    # Define a dictionary of contractions and their expanded forms\n",
    "    custom_contractions = {\n",
    "        \"yo\": \"year old\",\n",
    "        \"y.o.\": \"year old\",\n",
    "        \"y/o\": \"year old\",\n",
    "        \"y\": \"year\",\n",
    "        \"yr\": \"year\",\n",
    "        \"f\": \"female\",\n",
    "        \"m\": \"male\",\n",
    "        \"mo\": \"month\",\n",
    "        \"yr\": \"year\",\n",
    "        \"c/o\": \"complains of\",\n",
    "        \"c/of\": \"complains of\",\n",
    "        \"c/m\": \"complains of\",\n",
    "        \"cc\": \"chief complaint\",\n",
    "        \"h/o\": \"history of\",\n",
    "        \"pt\": \"patient\",\n",
    "        \"w\": \"week\",\n",
    "        \"wk\": \"week\",\n",
    "        \"hrs\": \"hours\",\n",
    "        \"hx\": \"history\",\n",
    "        \"pmh\": \"past medical history\",\n",
    "        \"pmhx\": \"past medical history\",\n",
    "        \"psh\": \"past surgical history\",\n",
    "        \"psurghx\": \"past surgical history\",\n",
    "        \"pshh\": \"past surgical history\",\n",
    "        \"meds\": \"medications\",\n",
    "        \"hosp\": \"hosipital\",\n",
    "        \"fh\": \"Family history\",\n",
    "        \"fhx\": \"Family history\",\n",
    "        \"fmh\": \"Family history\",\n",
    "        \"sh\":\"social history\",\n",
    "        \"soc\": \"social history\",\n",
    "        \"Rx\": \"prescription\",\n",
    "        \"ros\":\"review of systems\",\n",
    "        \"hpi\": \"history of present illness\"\n",
    "    }\n",
    "\n",
    "    # First, use the contractions library to expand common English contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Then, handle the custom contractions\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # For each word in the text\n",
    "    for i in range(len(words)):\n",
    "        # If the word is a contraction\n",
    "        if words[i] in custom_contractions:\n",
    "            # Replace the contraction with its expanded form in the text\n",
    "            words[i] = custom_contractions[words[i]]\n",
    "    # Join the words back into a text string\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    # Fill NaN values with a default value\n",
    "    df.fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Convert columns to string type\n",
    "    df['pn_history'] = df['pn_history'].astype(str)\n",
    "\n",
    "    df['pn_history'] = df['pn_history'].apply(convert_numbers_in_string)\n",
    "\n",
    "    # Apply the function to the 'pn_history' column\n",
    "    df['pn_history'] = df['pn_history'].apply(expand_contractions)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = process_data(df)\n",
    "\n",
    "# Write the data back to the CSV file\n",
    "# df.to_csv('data.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd. Removing Puncutation and Special Characters parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation and Special Characters\n",
    "df['pn_history'] = df['pn_history'].str.replace(r'[^a-zA-Z0-9 ]', ' ', regex=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th. Stemming\n",
    "This part would apply the stemming. I choose LancasterStemmer because it seems to be most efficient to find the root and would reduce the number of tokens, which make the last cluster easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Create an instance of LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Apply LancasterStemmer to the pn_history column\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([stemmer.stem(word) for word in re.findall(r'\\w+', x)]))\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th. Lemmatizer\n",
    "Apply lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in re.findall(r'\\w+', x)]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7th. Stop list\n",
    "Apply a stop word list to filter out unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Apply stop word filtering to the pn_history column\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([word for word in re.findall(r'\\w+', x) if word not in stop_words]))\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "### a.CountVectorizer\n",
    "Use CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge scikit-learn\n",
    "#pip install -U scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the \"pn_history\" column\n",
    "dtm = vectorizer.fit_transform(df['pn_history'])\n",
    "\n",
    "# Convert the DTM to a DataFrame\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DTM DataFrame\n",
    "print(dtm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = dtm.shape[0] * dtm.shape[1]\n",
    "nonzero_elements = dtm.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "dtm_df.info()\n",
    "print('Memory Usage:', dtm.data.nbytes/ (1024**3), 'GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DTM has 42,146 documents(patient's notes) and 29,081 unique terms. With a sparsity level of approximately 99.72%, it shows that the majority of the matrix elements are zero, meaning each document contains only a small subset of the total terms. Each element represents the frequency of a term in this document and is stored as an int64 type, and the total memory usage of the matrix is around 9.1 GB (If it is stored in csr_matrix, only occupy about 0.026 GB). This highlights the DTM as a highly sparse and memory-intensive structure, which is typical in text analysis involving large text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.TfidfVectorizer\n",
    "Use TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the \"pn_history\" column\n",
    "dtm_tfidf = vectorizer.fit_transform(df['pn_history'])\n",
    "\n",
    "# Convert the DTM to a DataFrame\n",
    "dtm_tfidf_df = pd.DataFrame(dtm_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF DTM DataFrame\n",
    "print(dtm_tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = dtm_tfidf.shape[0] * dtm_tfidf.shape[1]\n",
    "nonzero_elements = dtm_tfidf.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "dtm_tfidf_df.info()\n",
    "print('Memory Usage:', dtm_tfidf.data.nbytes/ (1024**3), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has 42,146 rows and 29,081 columns, corresponding to the documents(patient's notes) and terms respectively. The data type of all cells is float64. Each cell represents the TF-IDF score(which is in default setting) of a term in a specific document. The memory usage is substantial at 9.1 GB (If it is stored in csr_matrix, only occupy about 0.026 GB), indicating that while the matrix is sparse, it still occupies a significant amount of memory when converted to a dense format. The sparsity of the DTM is about 99.72%, which means the vast majority of the matrix elements are zeros. This reflects the common case in text data where most terms appear in only a small number of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.n-gram\n",
    "After studying the n-gram, I think we could try to find gram from bigger to smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Load your CSV file\n",
    "# Assuming the CSV has a column 'patient_notes' with the text data\n",
    "patient_notes = df['pn_history'].astype(str)\n",
    "\n",
    "# Optional: Download NLTK stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 4), stop_words=stopwords.words('english'))\n",
    "\n",
    "# Fit and transform the patient notes\n",
    "X = vectorizer.fit_transform(patient_notes)\n",
    "\n",
    "# Convert to DataFrame for better readability (optional)\n",
    "#dtm = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Now dtm is your document-term matrix with unigrams, bigrams, trigrams, and 4-grams\n",
    "#print(dtm.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = X.shape[0] * X.shape[1]\n",
    "nonzero_elements = X.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "print('DTM Shape:', X.shape)\n",
    "print('Data Types:', X.dtype)\n",
    "print('Memory Usage:', X.data.nbytes/ (1024**3), 'GB')  # Convert bytes to GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DTM, created using CountVectorizer with an n-gram range of 1 to 4 and English stop words, comprises 42,146 documents(patient's notes) and spans 3,928,802 unique n-grams. Each cell is int64 type and a cell's value indicates how many times the corresponding n-gram appears in that document. It exhibits an extremely high sparsity level of approximately 99.9904%, indicating that the vast majority of the matrix entries are zeros. Despite its vast dimensionality, the actual memory footprint is notably low at around 0.118 GB, thanks to its sparse matrix format, making it efficient for storing and processing such a large-scale text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "### a.Normalization and Outlier Analysis\n",
    "When we generate TF-IDF, we used 'TfidfVectorizer' from scikit-learn and by defualt, it normalizes all data using L2 norm which means the vector for each document is divided by its Euclidean norm, making the sum of the squares of the vector's elements equal to 1. We don't need to do other normalization again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier analysis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate Document Lengths\n",
    "document_lengths = dtm_tfidf_df.sum(axis=1)\n",
    "\n",
    "# Identify Outlier Documents\n",
    "Q1 = document_lengths.quantile(0.25)\n",
    "Q3 = document_lengths.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = document_lengths[(document_lengths < lower_bound) | (document_lengths > upper_bound)]\n",
    "\n",
    "print(outliers)\n",
    "\n",
    "#  Visualize Outliers\n",
    "sns.boxplot(document_lengths)\n",
    "plt.show()\n",
    "\n",
    "# Address Outliers\n",
    "# if you want to remove outliers:\n",
    "# non_outlier_indices = document_lengths[(document_lengths >= lower_bound) & (document_lengths <= upper_bound)].index\n",
    "# filtered_dtm_tfidf_df = dtm_tfidf_df.loc[non_outlier_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing the outlier, we use the sum of TF-IDF scores of each document. Most of documents have a sum of TF-IDF scores that fall within the range represented by the box. The list shows the indices and corresponding TF-IDF sums of the outlier documents. These documents have a TF-IDF sum significantly lower than the rest, falling below the lower bound defined by the whiskers, and hence are flagged as outliers. Possibly because they are shorter in length (number of words) or contain words that are uncommon in the rest of the documents, resulting in lower TF-IDF sums. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.Dimension Reduction\n",
    "Using dimension reduction techniques such as UMAP, high-dimensional datasets can be reduced into two-dimensional space for visualization and understanding biologically meaningful clusters present in high-dimensional datasets.\n",
    "#### Parameters:\n",
    "##### random_state:\n",
    "As UMAP is a stochastic algorithm, it may produce slightly different results if run multiple times. To reproduce similar results, we use the random_state parameter in UMAP() function.\n",
    "##### n_neighbors:\n",
    "This parameter controls how UMAP balances local versus global structure in the data. It does this by constraining the size of the local neighborhood UMAP will look at when attempting to learn the manifold structure of the data.As n_neighbors is increased UMAP manages to see more of the overall structure of the data, gluing more components together, and better coverying the broader structure of the data. By the stage of n_neighbors=15 we have a fairly good overall view of the data showing how they interelate to each other over the whole dataset.\n",
    "##### min_dist:\n",
    "The min_dist parameter controls how tightly UMAP is allowed to pack points together. It, quite literally, provides the minimum distance apart that points are allowed to be in the low dimensional representation. This means that low values of min_dist will result in clumpier embeddings.The default value for min_dist is 0.1.\n",
    "##### n_components:\n",
    "This parameter allows the user to determine the dimensionality of the reduced dimension space we will be embedding the data into. We can use it for more than just visualisation in 2- or 3-dimensions. If you are interested in (density based) clustering, or other machine learning techniques, it can be beneficial to pick a larger embedding dimension (say 10, or 50) closer to the the dimension of the underlying manifold on which your data lies.\n",
    "##### metric:\n",
    "This parameter controls how distance is computed in the ambient space of the input data. The Euclidean metric, specified as metric='euclidean', is one of the most commonly used distance metrics. It calculates the straight-line distance (“as the crow flies”) between two points in space. This metric often makes sense when your features are all measured in compatible units and scales, and when there’s a clear concept of “direction” in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "# Create an instance of the UMAP model\n",
    "reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1,  n_components=2, metric='euclidean')\n",
    "\n",
    "# Fit the model to your TF-IDF data and transform the data\n",
    "umap_embedding = reducer.fit_transform(dtm_tfidf_df)\n",
    "\n",
    "print(umap_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(umap_embedding[:, 0], umap_embedding[:, 1])\n",
    "plt.title('UMAP Results', fontsize=20)\n",
    "plt.xlabel('UMAP_1')\n",
    "plt.ylabel('UMAP_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Measures\n",
    "mean_distance = np.mean(np.linalg.norm(umap_embedding, axis=1))\n",
    "median_distance = np.median(np.linalg.norm(umap_embedding, axis=1))\n",
    "\n",
    "print(f'Mean distance from origin: {mean_distance}')\n",
    "print(f'Median distance from origin: {median_distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean distance from origin: 10.902891159057617: This is the average Euclidean distance of all data points from the origin in your UMAP reduced space. It’s calculated by summing up the distances of all points from the origin and then dividing by the total number of points. In the context of your assignment, this could indicate that, on average, the physician’s notes (after being processed and reduced in dimensionality) are spread out from the origin in the reduced dimensional space. This might suggest a diversity in the content of the notes.\n",
    "\n",
    "Median distance from origin: 10.838888168334961: This is the middle value of the Euclidean distances of all points from the origin when these distances are arranged in ascending order. Half of the distances are less than this value and half are greater. The median is a robust measure that is not affected by outliers or extreme values. In the context of your assignment, this tells you that half of your processed notes are within a distance of approximately 10.84 units from the origin in the reduced dimensional space.\n",
    "\n",
    "These measures give you a sense of how your data (the physician’s notes) is distributed in the reduced dimensional space after applying UMAP. If the mean and median are close to each other (as they are in this case), it suggests that your data might be symmetrically distributed around the origin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the UMAP model\n",
    "reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1, n_components=3, metric='euclidean')\n",
    "\n",
    "# Fit the model to your TF-IDF data and transform the data\n",
    "umap_embedding = reducer.fit_transform(dtm_tfidf_df)\n",
    "\n",
    "print(umap_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(umap_embedding[:, 0], umap_embedding[:, 1], umap_embedding[:, 2], s=100)\n",
    "plt.title('UMAP Results', fontsize=20)\n",
    "plt.xlabel('UMAP_1')\n",
    "plt.ylabel('UMAP_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Measures\n",
    "mean_distance = np.mean(np.linalg.norm(umap_embedding, axis=1))\n",
    "median_distance = np.median(np.linalg.norm(umap_embedding, axis=1))\n",
    "\n",
    "print(f'Mean distance from origin: {mean_distance}')\n",
    "print(f'Median distance from origin: {median_distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean distance from origin: 11.531579971313477: This is the average Euclidean distance of all data points from the origin in your UMAP reduced space. It’s calculated by summing up the distances of all points from the origin and then dividing by the total number of points. In the context of your assignment, this could indicate that, on average, the physician’s notes (after being processed and reduced in dimensionality) are spread out from the origin in the reduced dimensional space. This might suggest a diversity in the content of the notes.\n",
    "\n",
    "Median distance from origin: 12.881805419921875: This is the middle value of the Euclidean distances of all points from the origin when these distances are arranged in ascending order. Half of the distances are less than this value and half are greater. The median is a robust measure that is not affected by outliers or extreme values. In the context of your assignment, this tells you that half of your processed notes are within a distance of approximately 10.84 units from the origin in the reduced dimensional space.\n",
    "\n",
    "These measures give you a sense of how your data (the physician’s notes) is distributed in the reduced dimensional space after applying UMAP. If the mean and median are close to each other (as they are in this case), it suggests that your data might be symmetrically distributed around the origin. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS6120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
