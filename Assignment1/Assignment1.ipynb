{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Read data from patient notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  17-year-old male, has come to the student heal...\n",
      "1           1         0  17 yo male with recurrent palpitations for the...\n",
      "2           2         0  dillon cleveland is a 17 y.o. male patient wit...\n",
      "3           3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
      "4           4         0  17yo male with no pmh here for evaluation of p...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms. madden is a 20 yo female presenting w/ the...\n",
      "42142   95331         9  a 20 yo f came complain a dull 8/10 headache t...\n",
      "42143   95332         9  ms. madden is a 20yo female who presents with ...\n",
      "42144   95333         9  stephanie madden is a 20 year old woman compla...\n",
      "42145   95334         9  patient is a 20 yo f who presents with a heada...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#%pip install textblob\n",
    "import pandas as pd\n",
    "import re\n",
    "#from textblob import TextBlob\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('patient_notes.csv')\n",
    "\n",
    "# Case Conversion\n",
    "df['pn_history'] = df['pn_history'].str.lower()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Case Conversion and iii.\tCorrecting Typos and Spelling\n",
    "Removing Puncutation and Special Characters seems would influence standardizing formatsand handling contractions parts, so it is moved to later.\n",
    "\n",
    "We tried the TextBlob. But after taking 17 minutes that the first 100 rows have not been handled, we plan to try some other ways.\n",
    "\n",
    "Then I tried the symspellpy. However, it cannot be installed by conda, only can be dealt with by pip. So I move to SpaCy.\n",
    "\n",
    "Then I found the pyspellchecker is something I must use with SpaCy and also not able to installed by conda. So I return to symspellpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yue\\AppData\\Local\\Temp\\ipykernel_22048\\605495768.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  17-year-old male, has come to the student heal...\n",
      "1           1         0  17 yo male with recurrent palpitations for the...\n",
      "2           2         0  dillon cleveland is a 17 y.o. male patient wit...\n",
      "3           3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
      "4           4         0  17yo male with no pmh here for evaluation of p...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms. madden is a 20 yo female presenting w/ the...\n",
      "42142   95331         9  a 20 yo f came complain a dull 8/10 headache t...\n",
      "42143   95332         9  ms. madden is a 20yo female who presents with ...\n",
      "42144   95333         9  stephanie madden is a 20 year old woman compla...\n",
      "42145   95334         9  patient is a 20 yo f who presents with a heada...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#pip install symspellpy\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "# Initialize and load SymSpell\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# Function to correct spelling in a sentence\n",
    "def correct_spelling(text):\n",
    "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else text\n",
    "\n",
    "# Function to apply correct_spelling to a Series\n",
    "def apply_correct_spelling(series):\n",
    "    return series.apply(correct_spelling)\n",
    "\n",
    "# Function to parallelize\n",
    "def parallelize_dataframe(df, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df_combined = pd.concat(pool.map(apply_correct_spelling, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df_combined\n",
    "\n",
    "# Applying the function in parallel\n",
    "# df_head = df.head(1000)\n",
    "if __name__ == 'main':\n",
    "\tdf['pn_history'] = parallelize_dataframe(df['pn_history'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th Standardizing Formats 5th Handling Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  seventeen-year-old male, has come to the stude...\n",
      "1           1         0  seventeen year old male with recurrent palpita...\n",
      "2           2         0  dillon cleveland is a seventeen year old male ...\n",
      "3           3         0  a seventeen year old male complains of palpita...\n",
      "4           4         0  seventeen year old male with no past medical h...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms. madden is a twenty year old female present...\n",
      "42142   95331         9  a twenty year old female came complain a dull ...\n",
      "42143   95332         9  ms. madden is a twenty year old female who pre...\n",
      "42144   95333         9  stephanie madden is a twenty year old woman co...\n",
      "42145   95334         9  patient is a twenty year old female who presen...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# pip install inflect\n",
    "# pip install contractions\n",
    "import inflect\n",
    "import contractions\n",
    "\n",
    "# Create an engine for inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "# Standardizing Formats \"17\", “17-year-old”, “3-4”, or “17yo”\n",
    "def convert_numbers_in_string(s):\n",
    "    # Find all numbers in the string\n",
    "    numbers_in_words = re.findall(r'\\b\\d+\\b', s)\n",
    "    numbers_in_parts = re.findall(r'\\d+', s)\n",
    "\n",
    "    # For each number\n",
    "    for number in numbers_in_words:\n",
    "        # Convert the number to words\n",
    "        word = p.number_to_words(number)\n",
    "        # Replace the number with the word in the string\n",
    "        s = re.sub(r'\\b' + number + r'\\b', word, s)\n",
    "\n",
    "    for number in numbers_in_parts:\n",
    "        # Convert the number to words\n",
    "        word = p.number_to_words(number)\n",
    "        # Replace the number with the word (followed by a space) in the string\n",
    "        s = re.sub(number, word + \" \", s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# Handling Contractions: Expanding contractions\" \n",
    "# \"yo\", \"y.o.\" \"y/o\" to \"year old\",  \n",
    "# \"y\" to \"year\"\n",
    "# \"f\" to \"female\", \n",
    "# \"m\" to \"male\", \n",
    "# \"mo\" to \"month\", \n",
    "# \"yr\" to \"year\"\n",
    "# \"c/o\" \"c/of\" \"c/m\" to \"complains of\",\n",
    "# \"cc\" to \"chief complaint\"\n",
    "# \"h/o\" to \"history of\"\n",
    "# \"pt\" to \"patient\"\n",
    "# \"w\", \"wk\" to \"week\", \"hrs\" to \"hours\"\"\n",
    "# \"x\" to times?\n",
    "def expand_contractions(text):\n",
    "    # Define a dictionary of contractions and their expanded forms\n",
    "    custom_contractions = {\n",
    "        \"yo\": \"year old\",\n",
    "        \"y.o.\": \"year old\",\n",
    "        \"y/o\": \"year old\",\n",
    "        \"y\": \"year\",\n",
    "        \"yr\": \"year\",\n",
    "        \"f\": \"female\",\n",
    "        \"m\": \"male\",\n",
    "        \"mo\": \"month\",\n",
    "        \"yr\": \"year\",\n",
    "        \"c/o\": \"complains of\",\n",
    "        \"c/of\": \"complains of\",\n",
    "        \"c/m\": \"complains of\",\n",
    "        \"cc\": \"chief complaint\",\n",
    "        \"h/o\": \"history of\",\n",
    "        \"pt\": \"patient\",\n",
    "        \"w\": \"week\",\n",
    "        \"wk\": \"week\",\n",
    "        \"hrs\": \"hours\",\n",
    "        \"hx\": \"history\",\n",
    "        \"pmh\": \"past medical history\",\n",
    "        \"pmhx\": \"past medical history\",\n",
    "        \"psh\": \"past surgical history\",\n",
    "        \"psurghx\": \"past surgical history\",\n",
    "        \"pshh\": \"past surgical history\",\n",
    "        \"meds\": \"medications\",\n",
    "        \"hosp\": \"hosipital\",\n",
    "        \"fh\": \"Family history\",\n",
    "        \"fhx\": \"Family history\",\n",
    "        \"fmh\": \"Family history\",\n",
    "        \"sh\":\"social history\",\n",
    "        \"soc\": \"social history\",\n",
    "        \"Rx\": \"prescription\",\n",
    "        \"ros\":\"review of systems\",\n",
    "        \"hpi\": \"history of present illness\"\n",
    "    }\n",
    "\n",
    "    # First, use the contractions library to expand common English contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Then, handle the custom contractions\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # For each word in the text\n",
    "    for i in range(len(words)):\n",
    "        # If the word is a contraction\n",
    "        if words[i] in custom_contractions:\n",
    "            # Replace the contraction with its expanded form in the text\n",
    "            words[i] = custom_contractions[words[i]]\n",
    "    # Join the words back into a text string\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    # Fill NaN values with a default value\n",
    "    df.fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Convert columns to string type\n",
    "    df['pn_history'] = df['pn_history'].astype(str)\n",
    "\n",
    "    df['pn_history'] = df['pn_history'].apply(convert_numbers_in_string)\n",
    "\n",
    "    # Apply the function to the 'pn_history' column\n",
    "    df['pn_history'] = df['pn_history'].apply(expand_contractions)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = process_data(df)\n",
    "\n",
    "# Write the data back to the CSV file\n",
    "# df.to_csv('data.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd. Removing Puncutation and Special Characters parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  seventeen year old male  has come to the stude...\n",
      "1           1         0  seventeen year old male with recurrent palpita...\n",
      "2           2         0  dillon cleveland is a seventeen year old male ...\n",
      "3           3         0  a seventeen year old male complains of palpita...\n",
      "4           4         0  seventeen year old male with no past medical h...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms  madden is a twenty year old female present...\n",
      "42142   95331         9  a twenty year old female came complain a dull ...\n",
      "42143   95332         9  ms  madden is a twenty year old female who pre...\n",
      "42144   95333         9  stephanie madden is a twenty year old woman co...\n",
      "42145   95334         9  patient is a twenty year old female who presen...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Removing Punctuation and Special Characters\n",
    "df['pn_history'] = df['pn_history'].str.replace(r'[^a-zA-Z0-9 ]', ' ', regex=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th. Stemming\n",
    "This part would apply the stemming. I choose LancasterStemmer because it seems to be most efficient to find the root and would reduce the number of tokens, which make the last cluster easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  seventeen year old mal has com to the stud hea...\n",
      "1           1         0  seventeen year old mal with recur palpit for t...\n",
      "2           2         0  dillon cleveland is a seventeen year old mal p...\n",
      "3           3         0  a seventeen year old mal complain of palpit st...\n",
      "4           4         0  seventeen year old mal with no past med hist h...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms mad is a twenty year old fem pres w the wor...\n",
      "42142   95331         9  a twenty year old fem cam complain a dul eight...\n",
      "42143   95332         9  ms mad is a twenty year old fem who pres with ...\n",
      "42144   95333         9  stephany mad is a twenty year old wom complain...\n",
      "42145   95334         9  paty is a twenty year old fem who pres with a ...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Create an instance of LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Apply LancasterStemmer to the pn_history column\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([stemmer.stem(word) for word in re.findall(r'\\w+', x)]))\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th. Lemmatizer\n",
    "Apply lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  seventeen year old mal ha com to the stud heal...\n",
      "1           1         0  seventeen year old mal with recur palpit for t...\n",
      "2           2         0  dillon cleveland is a seventeen year old mal p...\n",
      "3           3         0  a seventeen year old mal complain of palpit st...\n",
      "4           4         0  seventeen year old mal with no past med hist h...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  m mad is a twenty year old fem pres w the wors...\n",
      "42142   95331         9  a twenty year old fem cam complain a dul eight...\n",
      "42143   95332         9  m mad is a twenty year old fem who pres with a...\n",
      "42144   95333         9  stephany mad is a twenty year old wom complain...\n",
      "42145   95334         9  paty is a twenty year old fem who pres with a ...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in re.findall(r'\\w+', x)]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7th. Stop list\n",
    "Apply a stop word list to filter out unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  seventeen year old mal ha com stud heal clin c...\n",
      "1           1         0  seventeen year old mal recur palpit past three...\n",
      "2           2         0  dillon cleveland seventeen year old mal paty s...\n",
      "3           3         0  seventeen year old mal complain palpit start t...\n",
      "4           4         0  seventeen year old mal past med hist evalu pal...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  mad twenty year old fem pres w worst ha lif un...\n",
      "42142   95331         9  twenty year old fem cam complain dul eight ten...\n",
      "42143   95332         9  mad twenty year old fem pres headach day dur w...\n",
      "42144   95333         9  stephany mad twenty year old wom complain head...\n",
      "42145   95334         9  paty twenty year old fem pres headach said ha ...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Apply stop word filtering to the pn_history column\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([word for word in re.findall(r'\\w+', x) if word not in stop_words]))\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "### a.CountVectorizer\n",
    "Use CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00  000  000mg  00four  00mg  00mgr  00mgxtwo  00ml  00two  00xtwo  \\\n",
      "0       0    0      0       0     0      0         0     0      0       0   \n",
      "1       0    0      0       0     0      0         0     0      0       0   \n",
      "2       0    0      0       0     0      0         0     0      0       0   \n",
      "3       0    0      0       0     0      0         0     0      0       0   \n",
      "4       0    0      0       0     0      0         0     0      0       0   \n",
      "...    ..  ...    ...     ...   ...    ...       ...   ...    ...     ...   \n",
      "42141   0    0      0       0     0      0         0     0      0       0   \n",
      "42142   0    0      0       0     0      0         0     0      0       0   \n",
      "42143   0    0      0       0     0      0         0     0      0       0   \n",
      "42144   0    0      0       0     0      0         0     0      0       0   \n",
      "42145   0    0      0       0     0      0         0     0      0       0   \n",
      "\n",
      "       ...  zero  zex  zig  ziminopril  zno  zo  zolpidem  zon  zyb  zzz  \n",
      "0      ...     0    0    0           0    0   0         0    0    0    0  \n",
      "1      ...     0    0    0           0    0   0         0    0    0    0  \n",
      "2      ...     0    0    0           0    0   0         0    0    0    0  \n",
      "3      ...     1    0    0           0    0   0         0    0    0    0  \n",
      "4      ...     0    0    0           0    0   0         0    0    0    0  \n",
      "...    ...   ...  ...  ...         ...  ...  ..       ...  ...  ...  ...  \n",
      "42141  ...     0    0    0           0    0   0         0    0    0    0  \n",
      "42142  ...     0    0    0           0    0   0         0    0    0    0  \n",
      "42143  ...     0    0    0           0    0   0         0    0    0    0  \n",
      "42144  ...     0    0    0           0    0   0         0    0    0    0  \n",
      "42145  ...     0    0    0           0    0   0         0    0    0    0  \n",
      "\n",
      "[42146 rows x 29081 columns]\n"
     ]
    }
   ],
   "source": [
    "#conda install -c conda-forge scikit-learn\n",
    "#pip install -U scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the \"pn_history\" column\n",
    "dtm = vectorizer.fit_transform(df['pn_history'])\n",
    "\n",
    "# Convert the DTM to a DataFrame\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DTM DataFrame\n",
    "print(dtm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of DTM: 0.9971788135819694\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42146 entries, 0 to 42145\n",
      "Columns: 29081 entries, 00 to zzz\n",
      "dtypes: int64(29081)\n",
      "memory usage: 9.1 GB\n",
      "Memory Usage: 0.02576247602701187 GB\n"
     ]
    }
   ],
   "source": [
    "total_elements = dtm.shape[0] * dtm.shape[1]\n",
    "nonzero_elements = dtm.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "dtm_df.info()\n",
    "print('Memory Usage:', dtm.data.nbytes/ (1024**3), 'GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DTM has 42,146 documents(patient's notes) and 29,081 unique terms. With a sparsity level of approximately 99.72%, it shows that the majority of the matrix elements are zero, meaning each document contains only a small subset of the total terms. Each element represents the frequency of a term in this document and is stored as an int64 type, and the total memory usage of the matrix is around 9.1 GB (If it is stored in csr_matrix, only occupy about 0.026 GB). This highlights the DTM as a highly sparse and memory-intensive structure, which is typical in text analysis involving large text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.TfidfVectorizer\n",
    "Use TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        00  000  000mg  00four  00mg  00mgr  00mgxtwo  00ml  00two  00xtwo  \\\n",
      "0      0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "1      0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "2      0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "3      0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "4      0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "...    ...  ...    ...     ...   ...    ...       ...   ...    ...     ...   \n",
      "42141  0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "42142  0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "42143  0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "42144  0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "42145  0.0  0.0    0.0     0.0   0.0    0.0       0.0   0.0    0.0     0.0   \n",
      "\n",
      "       ...      zero  zex  zig  ziminopril  zno   zo  zolpidem  zon  zyb  zzz  \n",
      "0      ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "1      ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "2      ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "3      ...  0.108212  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "4      ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "...    ...       ...  ...  ...         ...  ...  ...       ...  ...  ...  ...  \n",
      "42141  ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "42142  ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "42143  ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "42144  ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "42145  ...  0.000000  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "\n",
      "[42146 rows x 29081 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the \"pn_history\" column\n",
    "dtm_tfidf = vectorizer.fit_transform(df['pn_history'])\n",
    "\n",
    "# Convert the DTM to a DataFrame\n",
    "dtm_tfidf_df = pd.DataFrame(dtm_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF DTM DataFrame\n",
    "print(dtm_tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of DTM: 0.9971788135819694\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42146 entries, 0 to 42145\n",
      "Columns: 29081 entries, 00 to zzz\n",
      "dtypes: float64(29081)\n",
      "memory usage: 9.1 GB\n",
      "Memory Usage: 0.02576247602701187 GB\n"
     ]
    }
   ],
   "source": [
    "total_elements = dtm_tfidf.shape[0] * dtm_tfidf.shape[1]\n",
    "nonzero_elements = dtm_tfidf.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "dtm_tfidf_df.info()\n",
    "print('Memory Usage:', dtm_tfidf.data.nbytes/ (1024**3), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has 42,146 rows and 29,081 columns, corresponding to the documents(patient's notes) and terms respectively. The data type of all cells is float64. Each cell represents the TF-IDF score(which is in default setting) of a term in a specific document. The memory usage is substantial at 9.1 GB (If it is stored in csr_matrix, only occupy about 0.026 GB), indicating that while the matrix is sparse, it still occupies a significant amount of memory when converted to a dense format. The sparsity of the DTM is about 99.72%, which means the vast majority of the matrix elements are zeros. This reflects the common case in text data where most terms appear in only a small number of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a.Normalization and Outlier Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generate TF-IDF, we used 'TfidfVectorizer' from scikit-learn and by defualt, it normalizes all data using L2 norm which means the vector for each document is divided by its Euclidean norm, making the sum of the squares of the vector's elements equal to 1. We don't need to do other normalization again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        5.964949\n",
      "66       5.222436\n",
      "212      4.591200\n",
      "229      6.150649\n",
      "367      5.569603\n",
      "           ...   \n",
      "42042    4.079161\n",
      "42061    6.062651\n",
      "42073    6.104805\n",
      "42099    3.603849\n",
      "42121    5.846785\n",
      "Length: 994, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGKCAYAAACsHiO8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAe/UlEQVR4nO3df3BU1f3/8ddmE5YAyUoU8qNuMEEBDaYTB4YBFOuIdCgqOB1UJv36G1uCWqp1aqaBCGrjj+ow2kANdQAhYFsrKC2Uqi1ExkCh0AaZqUBNSSoEtJjdGGCDu/v9o5MtWwImH+89d388HzN3mr33nT3vGa33lXPPvdcViUQiAgAAMCTN6QYAAEBqIXwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMCrd6Qb+Vzgc1uHDh5WVlSWXy+V0OwAAoBcikYg6OjpUUFCgtLTzz23EXfg4fPiwfD6f020AAID/g9bWVl188cXnrYm78JGVlSXpP81nZ2c73A0AAOiNQCAgn88XPY+fT9yFj+5LLdnZ2YQPAAASTG+WTLDgFAAAGNXn8NHQ0KCbbrpJBQUFcrlcWr9+fczxSCSiBQsWKD8/X5mZmZo8ebIOHDhgVb8AACDB9Tl8dHZ26utf/7pqa2t7PP7ss8/qxRdf1M9//nPt2LFDAwcO1De/+U2dOnXqKzcLAAASX5/XfEydOlVTp07t8VgkEtHixYtVVVWl6dOnS5JeffVV5ebmav369br99tu/WrcAACDhWbrmo7m5WW1tbZo8eXJ0n9fr1bhx49TY2Njj7wSDQQUCgZgNAAAkL0vDR1tbmyQpNzc3Zn9ubm702P+qqamR1+uNbjzjAwCA5Ob43S6VlZXy+/3RrbW11emWAACAjSwNH3l5eZKko0ePxuw/evRo9Nj/8ng80Wd68GwPAACSn6Xho6ioSHl5eXr33Xej+wKBgHbs2KHx48dbORSABBMKhbRnzx69++672rNnj0KhkNMtAXBIn+92+fzzz3Xw4MHo5+bmZv31r39VTk6OCgsLNW/ePD355JO67LLLVFRUpPnz56ugoEAzZsywsm8ACaShoUEvvfSSPvnkk+i+IUOG6MEHH9SkSZMc7AyAE1yRSCTSl1/YsmWLrrvuurP233nnnVqxYoUikYiqq6tVV1en9vZ2XX311VqyZIlGjBjRq+8PBALyer3y+/1cggGSQENDgxYsWHDO44sWLSKAAEmgL+fvPocPuxE+gOQRCoU0depUdXV1nbOmX79+2rRpk9xut8HOAFitL+dvx+92AZC8du3aFQ0e48aNU21trTZu3Kja2lqNGzdOktTV1aVdu3Y52SYAwwgfAGzzy1/+UtJ/7oSrqalRSUmJBgwYoJKSEtXU1ETvguuuA5AaCB8AbHPs2DFJ0rRp05SWFvufm7S0tOirGrrrAKSGPt/tAiSaU6dOqaWlxek2UlJWVpYkaf369Ro7dmxMAAmHw3rrrbeidfv373ekR0iFhYXq37+/020ghbDgFElv//79uv/++51uA4hbdXV1vb4jETiXvpy/mflA0issLFRdXZ3TbaSkcDisBx98UKdPnz5nTUZGhl566aWzLsvAnMLCQqdbQIohfCDp9e/fn7/qHDR//vzzPudj/vz5GjVqlMGOADiNPzUA2GrSpElatGiRhgwZErN/6NChPGAMSFGs+QBgRCgU0saNG/X888/rkUce0be+9S0eLAYkER4yBiDuuN1ujRw5UpI0cuRIggeQwggfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMMqW8NHR0aF58+Zp2LBhyszM1IQJE7Rz5047hgIAAAnGlvBx33336e2339aqVau0d+9eTZkyRZMnT9bHH39sx3AAACCBWB4+Tp48qd/85jd69tlnNWnSJF166aV6/PHHdemll2rp0qVWDwcAABJMutVf+MUXXygUCql///4x+zMzM7Vt27az6oPBoILBYPRzIBCwuiUAABBHLJ/5yMrK0vjx4/XEE0/o8OHDCoVCWr16tRobG3XkyJGz6mtqauT1eqObz+ezuiUAABBHbFnzsWrVKkUiEX3ta1+Tx+PRiy++qFmzZikt7ezhKisr5ff7o1tra6sdLQEAgDhh+WUXSRo+fLi2bt2qzs5OBQIB5efn67bbblNxcfFZtR6PRx6Px442AABAHLL1OR8DBw5Ufn6+PvvsM23evFnTp0+3czgAAJAAbJn52Lx5syKRiEaOHKmDBw/q0Ucf1ahRo3T33XfbMRwAAEggtsx8+P1+zZ07V6NGjdIdd9yhq6++Wps3b1ZGRoYdwwEAgARiy8zHrbfeqltvvdWOrwYAAAmOd7sAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIxKd7qBZHb06FH5/X6n2wDixqFDh2L+F8B/eb1e5ebmOt2GEa5IJBJxuokzBQIBeb1e+f1+ZWdnO93O/9nRo0f1nf93h053BZ1uBQCQADL6ebR61asJG0D6cv5m5sMmfr9fp7uCOll8rcL9vU63AwCIY2mn/NJHW+X3+xM2fPQF4cNm4f5ehQde5HQbAADEDRacAgAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjLI8fIRCIc2fP19FRUXKzMzU8OHD9cQTTygSiVg9FAAASECWv9X2mWee0dKlS7Vy5UqVlJRo165duvvuu+X1evXQQw9ZPVzcSzvZ7nQLAIA4l2rnCsvDx/vvv6/p06dr2rRpkqRLLrlEa9eu1Z///Gerh0oImc0NTrcAAEBcsTx8TJgwQXV1ddq/f79GjBihv/3tb9q2bZteeOGFHuuDwaCCwWD0cyAQsLolR50smqRw5gVOtwEAiGNpJ9tT6o9Vy8PHY489pkAgoFGjRsntdisUCumpp55SeXl5j/U1NTVauHCh1W3EjXDmBQoPvMjpNgAAiBuWLzj91a9+pfr6eq1Zs0a7d+/WypUr9dOf/lQrV67ssb6yslJ+vz+6tba2Wt0SAACII5bPfDz66KN67LHHdPvtt0uSrrzySh06dEg1NTW68847z6r3eDzyeDxWtwEAAOKU5TMfJ06cUFpa7Ne63W6Fw2GrhwIAAAnI8pmPm266SU899ZQKCwtVUlKiPXv26IUXXtA999xj9VAAACABWR4+XnrpJc2fP18VFRU6duyYCgoK9N3vflcLFiyweigAAJCALA8fWVlZWrx4sRYvXmz1VwMAgCTAu10AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYZfm7XRAr7ZTf6RYAAHEu1c4VhA+beL1eZfTzSB9tdboVAEACyOjnkdfrdboNIwgfNsnNzdXqVa/K70+tNAucz6FDh/TUU0/pxz/+sYYNG+Z0O0Bc8Xq9ys3NdboNIwgfNsrNzU2Zf5GAvhg2bJhGjBjhdBsAHMKCUwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABglOXh45JLLpHL5Tprmzt3rtVDAQCABJRu9Rfu3LlToVAo+vmDDz7QDTfcoJkzZ1o9FAAASECWh48hQ4bEfH766ac1fPhwXXvttVYPBQAAEpDl4eNMXV1dWr16tR5++GG5XK4ea4LBoILBYPRzIBCwsyUAAOAwWxecrl+/Xu3t7brrrrvOWVNTUyOv1xvdfD6fnS0BAACH2Ro+XnnlFU2dOlUFBQXnrKmsrJTf749ura2tdrYEAAAcZttll0OHDumdd97RG2+8cd46j8cjj8djVxsAACDO2DbzsXz5cg0dOlTTpk2zawgAAJCAbAkf4XBYy5cv15133qn0dFvXtAIAgARjS/h455131NLSonvuuceOrwcAAAnMlmmJKVOmKBKJ2PHVAAAgwfFuFwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QMAABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYRfgAAABGET4AAIBRhA8AAGAU4QOAEbt379b9998vSbr//vu1e/duhzsC4JR0pxsAkPy+8Y1vnLXv4YcfliRt2bLFbDMAHMfMBwBb9RQ8+nIcQPIhfACwTW8vrXAJBkgtrkgkEnG6iTMFAgF5vV75/X5lZ2c73Q6SwKlTp9TS0uJ0Gympe41Hb9TV1dnYCc6nsLBQ/fv3d7oNJLi+nL9Z84Gk19LS0qeTIJzBPyPn1NXVacSIEU63gRRC+EDSKyws5K9qhzDzkRgKCwudbgEphvCBpNe/f3/+qksA/DMCUgcLTgEAgFGEDwAAYBThAwAAGGVL+Pj444/1ne98RxdeeKEyMzN15ZVXateuXXYMBQAAEozlC04/++wzTZw4Udddd502bdqkIUOG6MCBAxo8eLDVQwEAgARkefh45pln5PP5tHz58ui+oqIiq4cBAAAJyvLLLm+99ZbGjBmjmTNnaujQoSorK9OyZcvOWR8MBhUIBGI2AACQvCwPHx999JGWLl2qyy67TJs3b9acOXP00EMPaeXKlT3W19TUyOv1Rjefz2d1SwAAII5Y/m6Xfv36acyYMXr//fej+x566CHt3LlTjY2NZ9UHg0EFg8Ho50AgIJ/Px7tdgCTQlzfWbtmyxbY+ANivL+92sXzmIz8/X1dccUXMvssvv/ycL/byeDzKzs6O2QAAQPKyPHxMnDhRH374Ycy+/fv3a9iwYVYPBQAAEpDl4eMHP/iBtm/frp/85Cc6ePCg1qxZo7q6Os2dO9fqoQAAQAKyPHyMHTtW69at09q1azV69Gg98cQTWrx4scrLy60eCgAAJCBb3mp744036sYbb7TjqwEAQILj3S4AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAoywPH48//rhcLlfMNmrUKKuHAQAACSrdji8tKSnRO++8899B0m0ZBkCcc7lcikQivaoDkDpsSQXp6enKy8uz46sBJBDCB4Ce2LLm48CBAyooKFBxcbHKy8vV0tJyztpgMKhAIBCzAUgOaWm9+09Mb+sAJAfL/x8/btw4rVixQr///e+1dOlSNTc365prrlFHR0eP9TU1NfJ6vdHN5/NZ3RIAh7jdbkvrACQHV6Q3c6JfQXt7u4YNG6YXXnhB995771nHg8GggsFg9HMgEJDP55Pf71d2dradrQGw2fXXX69QKPSldW63W++++66BjgDYJRAIyOv19ur8bftK0AsuuEAjRozQwYMHezzu8Xjk8XjsbgMAAMQJ2y+0fv755/rHP/6h/Px8u4cCEGd6O7Fq8wQsgDhjefj44Q9/qK1bt+qf//yn3n//fd1yyy1yu92aNWuW1UMBiHMsOAXQE8svu/zrX//SrFmz9O9//1tDhgzR1Vdfre3bt2vIkCFWDwUgznk8Hn3xxRe9qgOQOiwPH6+99prVXwkgQQ0YMECdnZ29qgOQOpjrBGAb1nwA6AnhA4BtenObbV/qACQHwgcA25w8edLSOgDJgfABwDa9fWcL73YBUgvhA4Bt+vfvb2kdgORA+ABgm9OnT1taByA5ED4A2KY3t9n2pQ5AciB8ALANt9oC6AnhA4BtWPMBoCeEDwC2GTRokKV1AJID4QOAbfx+v6V1AJID4QOAbXjCKYCeED4A2CYzM9PSOgDJgfABwDa5ubmW1gFIDoQPALZh5gNATwgfAGyTl5dnaR2A5ED4AGCb4cOHW1oHIDkQPgDYpqOjw9I6AMmB8AHANh9++KGldQCSA+EDgG1OnTplaR2A5ED4AGAbj8cT/blfv34xx878fGYdgORH+ABgG5fLFf359OnTMcfO/HxmHYDkR/gAYJsz31YbiURijp35mbfaAqmF8AHANhdddJGldQCSA+EDgG0uv/xyS+sAJAfCBwDbBAIBS+sAJAfCBwDb+P1+S+sAJId0pxsAkLza2tqiP48dO1aFhYUKBoPyeDxqaWnRzp07z6oDkPwIHwBs09raKknKysrSoUOHomFDknJzc5WVlaWOjo5oHYDUQPgAYJvu53d0dHSoq6sr5lh7e7uCwWBMHYDUwJoPALY58y6W8z1kjLtdgNRC+ABgm9mzZ0d/DofDMcfO/HxmHYDkZ3v4ePrpp+VyuTRv3jy7hwIQZw4cOGBpHYDkYGv42Llzp15++WWVlpbaOQyAOPXpp59Kki644IIej3fv764DkBpsCx+ff/65ysvLtWzZMg0ePNiuYQDEsfb2dknSfffdp02bNmnGjBkaM2aMZsyYoU2bNunee++NqQOQGmwLH3PnztW0adM0efLk89YFg0EFAoGYDUBy6J7ZeO+995Senq5rr71WU6dO1bXXXqv09HRt27Ytpg5AarDlVtvXXntNu3fvjrmn/1xqamq0cOFCO9oA4LDuF8bt2LFDN954Y/TWWknyeDzRz7xYDkgtls98tLa26vvf/77q6+t79ZrsyspK+f3+6MbDhoDkUVpa+qWzGoMHD2ZdGJBiLJ/5+Mtf/qJjx47pqquuiu4LhUJqaGjQz372MwWDQbnd7ugxj8cjj8djdRsA4kxZWZnGjRsXnfHYsWOHtm/frkgk4nRrAAyzPHxcf/312rt3b8y+u+++W6NGjdKPfvSjmOABILk1NTWpvb1ds2fP1oYNG7R9+/bosfz8fM2ePVvLli1TU1OTysrKHOwUgEmWh4+srCyNHj06Zt/AgQN14YUXnrUfQHI7fvy4JOmWW27R7bffrqamJh0/flw5OTkqLS1VMBjUsmXLonUAUgPvdgFgm5ycHElSc3OzSkpKzprdaG5ujqkDkBqMhI8tW7aYGAZAnCktLVVeXp7q6+u1cOFCffDBB9GZj9GjR6u+vl75+fksOAVSDDMfAGzjdrtVUVGh6urqHm+17erq0sKFC1kLBqQYXiwHwHbnuqOFO12A1MTMBwDbhEIhLVmyRBMmTOjxskt1dbWWLl2qiRMnMvsBpBBmPgDYpqmpSW1tbSovL1dGRobKysp0/fXXq6ysTBkZGSovL9eRI0fU1NTkdKsADGLmA4Btum+hLSoqUigUOutW26Kiopg6AKmB8AHANt230K5bt05vvvmmjh07Fj02dOhQ3XzzzTF1AFKDKxJnK74CgYC8Xq/8fr+ys7OdbgfAVxAKhfTtb39b7e3t56wZPHiwXn/9ddZ8AAmuL+dv1nwAsNWJEyckSRkZGZo1a5ZWrVqlWbNmKSMjQ5LU2dnpZHsAHMBlFwC22b17t7q6upSZmans7GytXbtWa9eulSTl5eXJ7/fr5MmT2r17t8aOHetwtwBMYeYDgG3+8Ic/SJJmz56tFStWaMaMGRozZoxmzJih5cuX67777oupA5AamPkAYJuTJ09K+s8MSG1trUKhkCRp165d2rBhg8aPHx9TByA1MPMBwDZXXnmlJGnbtm0Kh8Mxx8LhsLZt2xZTByA1ED4A2GbatGnRnwcNGqRHHnlEr7/+uh555BENGjSoxzoAyY/LLgBs87vf/S76c2dnp55//vno57S0tJi62267zWhvAJzDzAcA2+zdu1eSdM0118jlcsUcc7lcuuaaa2LqAKQGZj4A2CYzM1OSVFZWpvnz5+vNN9/U4cOHVVBQoOnTp2vDhg167733onUAUgPhA4BtpkyZorffflvLly/XzTffrJkzZ0aPffHFF1qxYkW0DkDq4LILANtcddVVGjBggDo6OjRz5kxt2LBBn376qTZs2KCZM2eqo6NDAwYM0FVXXeV0qwAM4t0uAGzV0NCgBQsWnPP4okWLNGnSJIMdAbAD73YBEDcmTZqkRYsWaejQoTH7c3NzCR5AimLmA4ARoVBITU1NOn78uHJyclRaWsqbbIEk0pfzNwtOARjhdrtVVlbmdBsA4gCXXQAAgFGEDwAAYBSXXQAYwZoPAN0IHwBs19DQoCVLlqitrS26Ly8vTxUVFdztAqQgLrsAsFVDQ4Oqq6tVXFys2tpabdy4UbW1tSouLlZ1dbUaGhqcbhGAYdxqC8A2oVBI5eXlKi4u1pNPPhnzJttwOKyqqio1Nzdr9erVXIIBEhwPGQMQF5qamtTW1qby8vKY4CFJaWlpKi8v15EjR9TU1ORQhwCcQPgAYJvjx49LkoqKino83r2/uw5AaiB8ALBNTk6OJKm5ubnH4937u+sApAbCBwDblJaWKi8vT/X19QqHwzHHwuGw6uvrlZ+fr9LSUoc6BOAEy8PH0qVLVVpaquzsbGVnZ2v8+PHatGmT1cMASABut1sVFRVqbGxUVVWV9u3bpxMnTmjfvn2qqqpSY2Oj5syZw2JTIMVYfrfLhg0b5Ha7ddlllykSiWjlypV67rnntGfPHpWUlHzp73O3C5B8enrOR35+vubMmcNzPoAk0Zfzt5FbbXNycvTcc8/p3nvv/dJawgeQnHjCKZDc4uattqFQSL/+9a/V2dmp8ePH91gTDAYVDAajnwOBgJ0tAXAIb7UF0M2WBad79+7VoEGD5PF49L3vfU/r1q3TFVdc0WNtTU2NvF5vdPP5fHa0BAAA4oQtl126urrU0tIiv9+v119/Xb/4xS+0devWHgNITzMfPp+Pyy4AACSQuFvzMXnyZA0fPlwvv/zyl9ay5gMAgMQTd49XD4fDMbMbAAAgdVm+4LSyslJTp05VYWGhOjo6tGbNGm3ZskWbN2+2eigAAJCALA8fx44d0x133KEjR47I6/WqtLRUmzdv1g033GD1UAAAIAFZHj5eeeUVq78SAAAkEd7tAgAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMAowgcAADCK8AEAAIwifAAAAKMIHwAAwCjCBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMCodKcbAJAaQqGQmpqadPz4ceXk5Ki0tFRut9vptgA4wPLwUVNTozfeeEN///vflZmZqQkTJuiZZ57RyJEjrR4KQIJoaGjQkiVL1NbWFt2Xl5eniooKTZo0ycHOADjB8ssuW7du1dy5c7V9+3a9/fbbOn36tKZMmaLOzk6rhwKQABoaGlRdXa3i4mLV1tZq48aNqq2tVXFxsaqrq9XQ0OB0iwAMc0UikYidA3zyyScaOnSotm7d2qu/cAKBgLxer/x+v7Kzs+1sDYDNQqGQysvLVVxcrCeffFJpaf/9eyccDquqqkrNzc1avXo1l2CABNeX87ftC079fr8kKScnp8fjwWBQgUAgZgOQHJqamtTW1qby8vKY4CFJaWlpKi8v15EjR9TU1ORQhwCcYGv4CIfDmjdvniZOnKjRo0f3WFNTUyOv1xvdfD6fnS0BMOj48eOSpKKioh6Pd+/vrgOQGmwNH3PnztUHH3yg11577Zw1lZWV8vv90a21tdXOlgAY1D3j2dzc3OPx7v3nmhkFkJxsCx8PPPCAfvvb3+pPf/qTLr744nPWeTweZWdnx2wAkkNpaany8vJUX1+vcDgccywcDqu+vl75+fkqLS11qEMATrA8fEQiET3wwANat26d/vjHP55zuhVA8nO73aqoqFBjY6Oqqqq0b98+nThxQvv27VNVVZUaGxs1Z84cFpsCKcbyu10qKiq0Zs0avfnmmzHP9vB6vcrMzPzS3+duFyD59PScj/z8fM2ZM4fnfABJoi/nb8vDh8vl6nH/8uXLddddd33p7xM+gOTEE06B5NaX87flTzi1+bEhABKU2+1WWVmZ020AiAO8WA4AABhF+AAAAEYRPgAAgFGEDwAAYBThAwAAGEX4AAAARhE+AACAUYQPAABgFOEDAAAYZfkTTr+q7iekBgIBhzsBAAC91X3e7s2TzuMufHR0dEiSfD6fw50AAIC+6ujokNfrPW+N5S+W+6rC4bAOHz6srKysc76kDkBiCgQC8vl8am1t5cWRQJKJRCLq6OhQQUGB0tLOv6oj7sIHgOTFW6sBSCw4BQAAhhE+AACAUYQPAMZ4PB5VV1fL4/E43QoAB7HmAwAAGMXMBwAAMIrwAQAAjCJ8AAAAowgfAADAKMIHAAAwivABAACMInwAAACjCB8AAMCo/w9WDCA1HuL3rgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Outlier analysis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate Document Lengths\n",
    "document_lengths = dtm_tfidf_df.sum(axis=1)\n",
    "\n",
    "# Identify Outlier Documents\n",
    "Q1 = document_lengths.quantile(0.25)\n",
    "Q3 = document_lengths.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = document_lengths[(document_lengths < lower_bound) | (document_lengths > upper_bound)]\n",
    "\n",
    "print(outliers)\n",
    "\n",
    "#  Visualize Outliers\n",
    "sns.boxplot(document_lengths)\n",
    "plt.show()\n",
    "\n",
    "# Address Outliers\n",
    "# if you want to remove outliers:\n",
    "# non_outlier_indices = document_lengths[(document_lengths >= lower_bound) & (document_lengths <= upper_bound)].index\n",
    "# filtered_dtm_tfidf_df = dtm_tfidf_df.loc[non_outlier_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing the outlier, we use the sum of TF-IDF scores of each document. Most of documents have a sum of TF-IDF scores that fall within the range represented by the box. The list shows the indices and corresponding TF-IDF sums of the outlier documents. These documents have a TF-IDF sum significantly lower than the rest, falling below the lower bound defined by the whiskers, and hence are flagged as outliers. Possibly because they are shorter in length (number of words) or contain words that are uncommon in the rest of the documents, resulting in lower TF-IDF sums. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.n-gram\n",
    "After studying the n-gram, I think we could try to find gram from bigger to smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Load your CSV file\n",
    "# Assuming the CSV has a column 'patient_notes' with the text data\n",
    "patient_notes = df['pn_history'].astype(str)\n",
    "\n",
    "# Optional: Download NLTK stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 4), stop_words=stopwords.words('english'))\n",
    "\n",
    "# Fit and transform the patient notes\n",
    "X = vectorizer.fit_transform(patient_notes)\n",
    "\n",
    "# Convert to DataFrame for better readability (optional)\n",
    "#dtm = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Now dtm is your document-term matrix with unigrams, bigrams, trigrams, and 4-grams\n",
    "#print(dtm.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of DTM: 0.9999043515376047\n",
      "DTM Shape: (42146, 3928802)\n",
      "Data Types: int64\n",
      "Memory Usage: 0.11800070852041245 GB\n"
     ]
    }
   ],
   "source": [
    "total_elements = X.shape[0] * X.shape[1]\n",
    "nonzero_elements = X.nnz\n",
    "sparsity = 1 - (nonzero_elements / total_elements)\n",
    "print('Sparsity of DTM:', sparsity)\n",
    "print('DTM Shape:', X.shape)\n",
    "print('Data Types:', X.dtype)\n",
    "print('Memory Usage:', X.data.nbytes/ (1024**3), 'GB')  # Convert bytes to GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DTM, created using CountVectorizer with an n-gram range of 1 to 4 and English stop words, comprises 42,146 documents(patient's notes) and spans 3,928,802 unique n-grams. Each cell is int64 type and a cell's value indicates how many times the corresponding n-gram appears in that document. It exhibits an extremely high sparsity level of approximately 99.9904%, indicating that the vast majority of the matrix entries are zeros. Despite its vast dimensionality, the actual memory footprint is notably low at around 0.118 GB, thanks to its sparse matrix format, making it efficient for storing and processing such a large-scale text dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS6120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
