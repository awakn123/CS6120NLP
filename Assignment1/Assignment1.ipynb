{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Read data from patient notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  17-year-old male, has come to the student heal...\n",
      "1           1         0  17 yo male with recurrent palpitations for the...\n",
      "2           2         0  dillon cleveland is a 17 y.o. male patient wit...\n",
      "3           3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
      "4           4         0  17yo male with no pmh here for evaluation of p...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms. madden is a 20 yo female presenting w/ the...\n",
      "42142   95331         9  a 20 yo f came complain a dull 8/10 headache t...\n",
      "42143   95332         9  ms. madden is a 20yo female who presents with ...\n",
      "42144   95333         9  stephanie madden is a 20 year old woman compla...\n",
      "42145   95334         9  patient is a 20 yo f who presents with a heada...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#%pip install textblob\n",
    "import pandas as pd\n",
    "import re\n",
    "#from textblob import TextBlob\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('patient_notes.csv')\n",
    "\n",
    "# Case Conversion\n",
    "df['pn_history'] = df['pn_history'].str.lower()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Case Conversion and iii.\tCorrecting Typos and Spelling\n",
    "Removing Puncutation and Special Characters seems would influence standardizing formatsand handling contractions parts, so it is moved to later.\n",
    "\n",
    "We tried the TextBlob. But after taking 17 minutes that the first 100 rows have not been handled, we plan to try some other ways.\n",
    "\n",
    "Then I tried the symspellpy. However, it cannot be installed by conda, only can be dealt with by pip. So I move to SpaCy.\n",
    "\n",
    "Then I found the pyspellchecker is something I must use with SpaCy and also not able to installed by conda. So I return to symspellpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/nf_fc8zd4b51my52w5q8z68m0000gn/T/ipykernel_10073/605495768.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  17-year-old male, has come to the student heal...\n",
      "1           1         0  17 yo male with recurrent palpitations for the...\n",
      "2           2         0  dillon cleveland is a 17 y.o. male patient wit...\n",
      "3           3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
      "4           4         0  17yo male with no pmh here for evaluation of p...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms. madden is a 20 yo female presenting w/ the...\n",
      "42142   95331         9  a 20 yo f came complain a dull 8/10 headache t...\n",
      "42143   95332         9  ms. madden is a 20yo female who presents with ...\n",
      "42144   95333         9  stephanie madden is a 20 year old woman compla...\n",
      "42145   95334         9  patient is a 20 yo f who presents with a heada...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#pip install symspellpy\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "# Initialize and load SymSpell\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# Function to correct spelling in a sentence\n",
    "def correct_spelling(text):\n",
    "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else text\n",
    "\n",
    "# Function to apply correct_spelling to a Series\n",
    "def apply_correct_spelling(series):\n",
    "    return series.apply(correct_spelling)\n",
    "\n",
    "# Function to parallelize\n",
    "def parallelize_dataframe(df, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df_combined = pd.concat(pool.map(apply_correct_spelling, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df_combined\n",
    "\n",
    "# Applying the function in parallel\n",
    "# df_head = df.head(1000)\n",
    "if __name__ == 'main':\n",
    "\tdf['pn_history'] = parallelize_dataframe(df['pn_history'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th Standardizing Formats 5th Handling Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  seventeen-year-old male, has come to the stude...\n",
      "1           1         0  seventeen year old male with recurrent palpita...\n",
      "2           2         0  dillon cleveland is a seventeen year old male ...\n",
      "3           3         0  a seventeen year old male complains of palpita...\n",
      "4           4         0  seventeen year old male with no past medical h...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms. madden is a twenty year old female present...\n",
      "42142   95331         9  a twenty year old female came complain a dull ...\n",
      "42143   95332         9  ms. madden is a twenty year old female who pre...\n",
      "42144   95333         9  stephanie madden is a twenty year old woman co...\n",
      "42145   95334         9  patient is a twenty year old female who presen...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# pip install inflect\n",
    "# pip install contractions\n",
    "import inflect\n",
    "import contractions\n",
    "\n",
    "# Create an engine for inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "# Standardizing Formats \"17\", “17-year-old”, “3-4”, or “17yo”\n",
    "def convert_numbers_in_string(s):\n",
    "    # Find all numbers in the string\n",
    "    numbers_in_words = re.findall(r'\\b\\d+\\b', s)\n",
    "    numbers_in_parts = re.findall(r'\\d+', s)\n",
    "\n",
    "    # For each number\n",
    "    for number in numbers_in_words:\n",
    "        # Convert the number to words\n",
    "        word = p.number_to_words(number)\n",
    "        # Replace the number with the word in the string\n",
    "        s = re.sub(r'\\b' + number + r'\\b', word, s)\n",
    "\n",
    "    for number in numbers_in_parts:\n",
    "        # Convert the number to words\n",
    "        word = p.number_to_words(number)\n",
    "        # Replace the number with the word (followed by a space) in the string\n",
    "        s = re.sub(number, word + \" \", s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# Handling Contractions: Expanding contractions\" \n",
    "# \"yo\", \"y.o.\" \"y/o\" to \"year old\",  \n",
    "# \"y\" to \"year\"\n",
    "# \"f\" to \"female\", \n",
    "# \"m\" to \"male\", \n",
    "# \"mo\" to \"month\", \n",
    "# \"yr\" to \"year\"\n",
    "# \"c/o\" \"c/of\" \"c/m\" to \"complains of\",\n",
    "# \"cc\" to \"chief complaint\"\n",
    "# \"h/o\" to \"history of\"\n",
    "# \"pt\" to \"patient\"\n",
    "# \"w\", \"wk\" to \"week\", \"hrs\" to \"hours\"\"\n",
    "# \"x\" to times?\n",
    "def expand_contractions(text):\n",
    "    # Define a dictionary of contractions and their expanded forms\n",
    "    custom_contractions = {\n",
    "        \"yo\": \"year old\",\n",
    "        \"y.o.\": \"year old\",\n",
    "        \"y/o\": \"year old\",\n",
    "        \"y\": \"year\",\n",
    "        \"yr\": \"year\",\n",
    "        \"f\": \"female\",\n",
    "        \"m\": \"male\",\n",
    "        \"mo\": \"month\",\n",
    "        \"yr\": \"year\",\n",
    "        \"c/o\": \"complains of\",\n",
    "        \"c/of\": \"complains of\",\n",
    "        \"c/m\": \"complains of\",\n",
    "        \"cc\": \"chief complaint\",\n",
    "        \"h/o\": \"history of\",\n",
    "        \"pt\": \"patient\",\n",
    "        \"w\": \"week\",\n",
    "        \"wk\": \"week\",\n",
    "        \"hrs\": \"hours\",\n",
    "        \"hx\": \"history\",\n",
    "        \"pmh\": \"past medical history\",\n",
    "        \"pmhx\": \"past medical history\",\n",
    "        \"psh\": \"past surgical history\",\n",
    "        \"psurghx\": \"past surgical history\",\n",
    "        \"pshh\": \"past surgical history\",\n",
    "        \"meds\": \"medications\",\n",
    "        \"hosp\": \"hosipital\",\n",
    "        \"fh\": \"Family history\",\n",
    "        \"fhx\": \"Family history\",\n",
    "        \"fmh\": \"Family history\",\n",
    "        \"sh\":\"social history\",\n",
    "        \"soc\": \"social history\",\n",
    "        \"Rx\": \"prescription\",\n",
    "        \"ros\":\"review of systems\",\n",
    "        \"hpi\": \"history of present illness\"\n",
    "    }\n",
    "\n",
    "    # First, use the contractions library to expand common English contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Then, handle the custom contractions\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # For each word in the text\n",
    "    for i in range(len(words)):\n",
    "        # If the word is a contraction\n",
    "        if words[i] in custom_contractions:\n",
    "            # Replace the contraction with its expanded form in the text\n",
    "            words[i] = custom_contractions[words[i]]\n",
    "    # Join the words back into a text string\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    # Fill NaN values with a default value\n",
    "    df.fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Convert columns to string type\n",
    "    df['pn_history'] = df['pn_history'].astype(str)\n",
    "\n",
    "    df['pn_history'] = df['pn_history'].apply(convert_numbers_in_string)\n",
    "\n",
    "    # Apply the function to the 'pn_history' column\n",
    "    df['pn_history'] = df['pn_history'].apply(expand_contractions)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = process_data(df)\n",
    "\n",
    "# Write the data back to the CSV file\n",
    "# df.to_csv('data.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd. Removing Puncutation and Special Characters parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  seventeen year old male  has come to the stude...\n",
      "1           1         0  seventeen year old male with recurrent palpita...\n",
      "2           2         0  dillon cleveland is a seventeen year old male ...\n",
      "3           3         0  a seventeen year old male complains of palpita...\n",
      "4           4         0  seventeen year old male with no past medical h...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms  madden is a twenty year old female present...\n",
      "42142   95331         9  a twenty year old female came complain a dull ...\n",
      "42143   95332         9  ms  madden is a twenty year old female who pre...\n",
      "42144   95333         9  stephanie madden is a twenty year old woman co...\n",
      "42145   95334         9  patient is a twenty year old female who presen...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Removing Punctuation and Special Characters\n",
    "df['pn_history'] = df['pn_history'].str.replace(r'[^a-zA-Z0-9 ]', ' ', regex=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th. Stemming\n",
    "This part would apply the stemming. I choose LancasterStemmer because it seems to be most efficient to find the root and would reduce the number of tokens, which make the last cluster easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  17 year old mal has com to the stud heal clin ...\n",
      "1           1         0  17 yo mal with recur palpit for the past 3 mo ...\n",
      "2           2         0  dillon cleveland is a 17 y o mal paty with no ...\n",
      "3           3         0  a 17 yo m c o palpit start 3 mos ago noth impr...\n",
      "4           4         0  17yo mal with no pmh her for evalu of palpit s...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  ms mad is a 20 yo fem pres w the worst ha of h...\n",
      "42142   95331         9  a 20 yo f cam complain a dul 8 10 headach that...\n",
      "42143   95332         9  ms mad is a 20yo fem who pres with a headach o...\n",
      "42144   95333         9  stephany mad is a 20 year old wom complain of ...\n",
      "42145   95334         9  paty is a 20 yo f who pres with a headach she ...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('patient_notes.csv')\n",
    "\n",
    "# Create an instance of LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Apply LancasterStemmer to the pn_history column\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([stemmer.stem(word) for word in re.findall(r'\\w+', x)]))\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th. Lemmatizer\n",
    "Apply lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/caoyun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  17 year old mal ha com to the stud heal clin c...\n",
      "1           1         0  17 yo mal with recur palpit for the past 3 mo ...\n",
      "2           2         0  dillon cleveland is a 17 y o mal paty with no ...\n",
      "3           3         0  a 17 yo m c o palpit start 3 mo ago noth impro...\n",
      "4           4         0  17yo mal with no pmh her for evalu of palpit s...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  m mad is a 20 yo fem pres w the worst ha of he...\n",
      "42142   95331         9  a 20 yo f cam complain a dul 8 10 headach that...\n",
      "42143   95332         9  m mad is a 20yo fem who pres with a headach of...\n",
      "42144   95333         9  stephany mad is a 20 year old wom complain of ...\n",
      "42145   95334         9  paty is a 20 yo f who pres with a headach she ...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in re.findall(r'\\w+', x)]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7th. Stop list\n",
    "Apply a stop word list to filter out unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/caoyun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pn_num  case_num                                         pn_history\n",
      "0           0         0  17 year old mal ha com stud heal clin complain...\n",
      "1           1         0  17 yo mal recur palpit past 3 mo last 3 4 min ...\n",
      "2           2         0  dillon cleveland 17 mal paty sign pmh pres com...\n",
      "3           3         0  17 yo c palpit start 3 mo ago noth improv exac...\n",
      "4           4         0  17yo mal pmh evalu palpit stat last 3 4mo ha f...\n",
      "...       ...       ...                                                ...\n",
      "42141   95330         9  mad 20 yo fem pres w worst ha lif unlik anyth ...\n",
      "42142   95331         9  20 yo f cam complain dul 8 10 headach assocy n...\n",
      "42143   95332         9  mad 20yo fem pres headach 1 day dur wok yester...\n",
      "42144   95333         9  stephany mad 20 year old wom complain headach ...\n",
      "42145   95334         9  paty 20 yo f pres headach said ha nev somethig...\n",
      "\n",
      "[42146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Apply stop word filtering to the pn_history column\n",
    "df['pn_history'] = df['pn_history'].apply(lambda x: ' '.join([word for word in re.findall(r'\\w+', x) if word not in stop_words]))\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "### a.CountVectorizer\n",
    "Use CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00  000  0000  00am  00amto4  00h  00pm  01  01pack  02  ...  zero  \\\n",
      "0       0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "1       0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "2       0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "3       0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "4       0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "...    ..  ...   ...   ...      ...  ...   ...  ..     ...  ..  ...   ...   \n",
      "42141   0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "42142   0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "42143   0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "42144   0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "42145   0    0     0     0        0    0     0   0       0   0  ...     0   \n",
      "\n",
      "       zex  zig  ziminopril  zno  zo  zolpidem  zon  zyb  zzz  \n",
      "0        0    0           0    0   0         0    0    0    0  \n",
      "1        0    0           0    0   0         0    0    0    0  \n",
      "2        0    0           0    0   0         0    0    0    0  \n",
      "3        0    0           0    0   0         0    0    0    0  \n",
      "4        0    0           0    0   0         0    0    0    0  \n",
      "...    ...  ...         ...  ...  ..       ...  ...  ...  ...  \n",
      "42141    0    0           0    0   0         0    0    0    0  \n",
      "42142    0    0           0    0   0         0    0    0    0  \n",
      "42143    0    0           0    0   0         0    0    0    0  \n",
      "42144    0    0           0    0   0         0    0    0    0  \n",
      "42145    0    0           0    0   0         0    0    0    0  \n",
      "\n",
      "[42146 rows x 30478 columns]\n"
     ]
    }
   ],
   "source": [
    "#conda install -c conda-forge scikit-learn\n",
    "#pip install -U scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the \"pn_history\" column\n",
    "dtm = vectorizer.fit_transform(df['pn_history'])\n",
    "\n",
    "# Convert the DTM to a DataFrame\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DTM DataFrame\n",
    "print(dtm_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.TfidfVectorizer\n",
    "Use TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        00  000  0000  00am  00amto4  00h  00pm   01  01pack   02  ...  zero  \\\n",
      "0      0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "1      0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "2      0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "3      0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "4      0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "...    ...  ...   ...   ...      ...  ...   ...  ...     ...  ...  ...   ...   \n",
      "42141  0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "42142  0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "42143  0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "42144  0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "42145  0.0  0.0   0.0   0.0      0.0  0.0   0.0  0.0     0.0  0.0  ...   0.0   \n",
      "\n",
      "       zex  zig  ziminopril  zno   zo  zolpidem  zon  zyb  zzz  \n",
      "0      0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "1      0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "2      0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "3      0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "4      0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "...    ...  ...         ...  ...  ...       ...  ...  ...  ...  \n",
      "42141  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "42142  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "42143  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "42144  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "42145  0.0  0.0         0.0  0.0  0.0       0.0  0.0  0.0  0.0  \n",
      "\n",
      "[42146 rows x 30478 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the \"pn_history\" column\n",
    "dtm_tfidf = vectorizer.fit_transform(df['pn_history'])\n",
    "\n",
    "# Convert the DTM to a DataFrame\n",
    "dtm_tfidf_df = pd.DataFrame(dtm_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF DTM DataFrame\n",
    "print(dtm_tfidf_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.n-gram\n",
    "After studying the n-gram, I think we could try to find gram from bigger to smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  00 00  00 00 ha  00 00 ha wak  00 14  00 14 00  00 14 00 ro  00 4am  \\\n",
      "0   0      0         0             0      0         0            0       0   \n",
      "1   0      0         0             0      0         0            0       0   \n",
      "2   0      0         0             0      0         0            0       0   \n",
      "3   0      0         0             0      0         0            0       0   \n",
      "4   0      0         0             0      0         0            0       0   \n",
      "\n",
      "   00 4am excess  00 4am excess caffy  ...  zon rady oth  zon rady oth part  \\\n",
      "0              0                    0  ...             0                  0   \n",
      "1              0                    0  ...             0                  0   \n",
      "2              0                    0  ...             0                  0   \n",
      "3              0                    0  ...             0                  0   \n",
      "4              0                    0  ...             0                  0   \n",
      "\n",
      "   zyb  zyb ndka  zyb ndka fh  zyb ndka fh fath  zzz  zzz paty  zzz paty lost  \\\n",
      "0    0         0            0                 0    0         0              0   \n",
      "1    0         0            0                 0    0         0              0   \n",
      "2    0         0            0                 0    0         0              0   \n",
      "3    0         0            0                 0    0         0              0   \n",
      "4    0         0            0                 0    0         0              0   \n",
      "\n",
      "   zzz paty lost son  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n",
      "\n",
      "[5 rows x 4022508 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Load your CSV file\n",
    "# Assuming the CSV has a column 'patient_notes' with the text data\n",
    "patient_notes = df['pn_history'].astype(str)\n",
    "\n",
    "# Optional: Download NLTK stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 4), stop_words=stopwords.words('english'))\n",
    "\n",
    "# Fit and transform the patient notes\n",
    "X = vectorizer.fit_transform(patient_notes)\n",
    "\n",
    "# Convert to DataFrame for better readability (optional)\n",
    "dtm = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Now dtm is your document-term matrix with unigrams, bigrams, trigrams, and 4-grams\n",
    "print(dtm.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS6120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
