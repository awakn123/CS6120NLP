{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.948005199432373),\n",
       " ('tweet', 0.9403423070907593),\n",
       " ('fb', 0.9342359900474548),\n",
       " ('instagram', 0.9104824066162109),\n",
       " ('chat', 0.8964963555335999),\n",
       " ('hashtag', 0.8885936737060547),\n",
       " ('tweets', 0.8878158330917358),\n",
       " ('tl', 0.8778461217880249),\n",
       " ('link', 0.877821147441864),\n",
       " ('internet', 0.8753897547721863)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "#  Download \"glove-twitter-25\" embeddings\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "glove_vectors.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:18:24\n",
      "I bought my first HP12C in about 1984 or so, and it served me faithfully until 2002 when I lost it while travelling.  I searched for another one to replace it, but found one difficult to come by in my area.  So, I decided to buy up and purchased an HP 49G.  What a mistake!  I know that many people view the HP 49G (now 49G+) as the flagship of the HP line, but for me that was a disaster.The 49G may be powerful, but using it is extremely counterintuitive...and the manual was sketchy at best.  The 12C, on the other hand, does what I need in a way that makes good sense to me.If you are looking for a solid, reliable calculator, the HP12C may be for you.  It's programmable.  It does basic statistics well, and many business applications too.  The manual makes sense; you will be up and running in short order.I'm ready to set my 49G aside and move back to my old friend.  I didn't even have to replace the batteries in well over a decade of use!HP 12C, I'm coming home!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "\n",
    "# Import amazon reviews dataset \n",
    "\n",
    "filepath = '/Users/p.mittal/Library/Mobile Documents/com~apple~CloudDocs/Roux/Courses/NLP/Assignments/Assignment 3 SA/Amazon Reviews/reviews_Office_Products_5.json'\n",
    "\n",
    "data = []\n",
    "with open(filepath\n",
    "            , 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df[\"length_review\"] = df[\"reviewText\"].apply(lambda x: len(x.split()))\n",
    "df[\"length_summary\"] = df[\"summary\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(df[\"reviewText\"][0])\n",
    "df.head()\n",
    "\n",
    "\n",
    "# SUbset 1000 reviews for faster processing\n",
    "df2  = df.sample(1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>length_review</th>\n",
       "      <th>length_summary</th>\n",
       "      <th>reviewText2</th>\n",
       "      <th>summary2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40122</th>\n",
       "      <td>A3KZEGBTPH6MMF</td>\n",
       "      <td>B006IB35GG</td>\n",
       "      <td>Lucy Cat \"Mandy\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I am a bit of a pen connoisseur -- I've litera...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cheap feeling barrel - underwhelming performance</td>\n",
       "      <td>1331683200</td>\n",
       "      <td>03 14, 2012</td>\n",
       "      <td>159</td>\n",
       "      <td>6</td>\n",
       "      <td>bit pen connoisseur ive literally tried spent ...</td>\n",
       "      <td>cheap feeling barrel underwhelming performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42089</th>\n",
       "      <td>A3EP26M3QHINXT</td>\n",
       "      <td>B00886N6YA</td>\n",
       "      <td>Nathan G</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I use this not for the RFID but for the protec...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Card cover</td>\n",
       "      <td>1357171200</td>\n",
       "      <td>01 3, 2013</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>use rfid protection notice however blocked sca...</td>\n",
       "      <td>card cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>A16PYNWJEUN5TL</td>\n",
       "      <td>B0008GQ26C</td>\n",
       "      <td>SANDYEGGO \"SALLY GOSSELIN\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Love the refills in Medium.  I have always bou...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LOVE THE MEDIUM REFILLS!!</td>\n",
       "      <td>1385769600</td>\n",
       "      <td>11 30, 2013</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>love refills medium always bought fine found s...</td>\n",
       "      <td>love medium refills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20232</th>\n",
       "      <td>A3EZEP0FX5BC1P</td>\n",
       "      <td>B00290N0RO</td>\n",
       "      <td>A Conrad \"Ask Conrad\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a great sketchbook.  The papers are th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great SketchBook!</td>\n",
       "      <td>1403827200</td>\n",
       "      <td>06 27, 2014</td>\n",
       "      <td>113</td>\n",
       "      <td>2</td>\n",
       "      <td>great sketchbook papers thick textured daughte...</td>\n",
       "      <td>great sketchbook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13503</th>\n",
       "      <td>A2065HBMYDXJ1S</td>\n",
       "      <td>B0010T3QT2</td>\n",
       "      <td>Jenn B \"Happy Mom\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These are good enevelopes, convenient to use. ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good product</td>\n",
       "      <td>1278979200</td>\n",
       "      <td>07 13, 2010</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>good enevelopes convenient use sometimes pull ...</td>\n",
       "      <td>good product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID        asin                reviewerName helpful  \\\n",
       "40122  A3KZEGBTPH6MMF  B006IB35GG            Lucy Cat \"Mandy\"  [0, 0]   \n",
       "42089  A3EP26M3QHINXT  B00886N6YA                    Nathan G  [0, 0]   \n",
       "7610   A16PYNWJEUN5TL  B0008GQ26C  SANDYEGGO \"SALLY GOSSELIN\"  [0, 0]   \n",
       "20232  A3EZEP0FX5BC1P  B00290N0RO       A Conrad \"Ask Conrad\"  [0, 0]   \n",
       "13503  A2065HBMYDXJ1S  B0010T3QT2          Jenn B \"Happy Mom\"  [0, 0]   \n",
       "\n",
       "                                              reviewText  overall  \\\n",
       "40122  I am a bit of a pen connoisseur -- I've litera...      3.0   \n",
       "42089  I use this not for the RFID but for the protec...      5.0   \n",
       "7610   Love the refills in Medium.  I have always bou...      5.0   \n",
       "20232  This is a great sketchbook.  The papers are th...      5.0   \n",
       "13503  These are good enevelopes, convenient to use. ...      4.0   \n",
       "\n",
       "                                                summary  unixReviewTime  \\\n",
       "40122  Cheap feeling barrel - underwhelming performance      1331683200   \n",
       "42089                                        Card cover      1357171200   \n",
       "7610                          LOVE THE MEDIUM REFILLS!!      1385769600   \n",
       "20232                                 Great SketchBook!      1403827200   \n",
       "13503                                      Good product      1278979200   \n",
       "\n",
       "        reviewTime  length_review  length_summary  \\\n",
       "40122  03 14, 2012            159               6   \n",
       "42089   01 3, 2013             37               2   \n",
       "7610   11 30, 2013             77               4   \n",
       "20232  06 27, 2014            113               2   \n",
       "13503  07 13, 2010             33               2   \n",
       "\n",
       "                                             reviewText2  \\\n",
       "40122  bit pen connoisseur ive literally tried spent ...   \n",
       "42089  use rfid protection notice however blocked sca...   \n",
       "7610   love refills medium always bought fine found s...   \n",
       "20232  great sketchbook papers thick textured daughte...   \n",
       "13503  good enevelopes convenient use sometimes pull ...   \n",
       "\n",
       "                                             summary2  \n",
       "40122  cheap feeling barrel underwhelming performance  \n",
       "42089                                      card cover  \n",
       "7610                              love medium refills  \n",
       "20232                                great sketchbook  \n",
       "13503                                    good product  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stopwords.words('english')]\n",
    "    # text = [WordNetLemmatizer().lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df2[\"reviewText2\"] = df2[\"reviewText\"].apply(lambda x: clean_text(x))\n",
    "df2[\"summary2\"] = df2[\"summary\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a bit of a pen connoisseur -- I've literally tried them all (and spent way too much money in the process).  For ballpoint pens, its important that the ink runs smooth and consistently (I hate when the ink density changes mid-stroke.)  For all pens, the barrel needs to be comfortable and sturdy in your grip.  So, how do these \"InkJoy\"'s stack up?  For me they are just \"O.K.\"  The ink runs smooth and is very consistent for a ball point, but the construction of the pen itself feels extremely cheap.  The barrel is literally bends when squeezed between my index finger and thumb and the pen tip itself feels/looks cheap.  I find these to be a little thick in diameter for my grip as well.  Overall, not a terribly impressive pen.  I have relegated these to the pens I keep out in the open at my office.  If someone walks off with them I wouldn't be upset....\n",
      "bit pen connoisseur ive literally tried spent way much money process ballpoint pens important ink runs smooth consistently hate ink density changes midstroke pens barrel needs comfortable sturdy grip inkjoys stack ok ink runs smooth consistent ball point construction pen feels extremely cheap barrel literally bends squeezed index finger thumb pen tip feelslooks cheap find little thick diameter grip well overall terribly impressive pen relegated pens keep open office someone walks wouldnt upset\n"
     ]
    }
   ],
   "source": [
    "# print the first review and its cleaned version\n",
    "print(df2[\"reviewText\"].reset_index(drop=True)[0])\n",
    "print(df2[\"reviewText2\"].reset_index(drop=True) [0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bit', 'pen', 'connoisseur', 'ive', 'literally', 'tried', 'spent', 'way', 'much', 'money', 'process', 'ballpoint', 'pens', 'important', 'ink', 'runs', 'smooth', 'consistently', 'hate', 'ink', 'density', 'changes', 'midstroke', 'pens', 'barrel', 'needs', 'comfortable', 'sturdy', 'grip', 'inkjoys', 'stack', 'ok', 'ink', 'runs', 'smooth', 'consistent', 'ball', 'point', 'construction', 'pen', 'feels', 'extremely', 'cheap', 'barrel', 'literally', 'bends', 'squeezed', 'index', 'finger', 'thumb', 'pen', 'tip', 'feelslooks', 'cheap', 'find', 'little', 'thick', 'diameter', 'grip', 'well', 'overall', 'terribly', 'impressive', 'pen', 'relegated', 'pens', 'keep', 'open', 'office', 'someone', 'walks', 'wouldnt', 'upset']\n",
      "[-0.483687    0.6683833   0.43632442 -0.08098825 -0.01126154 -1.2808864\n",
      "  0.34584472  1.6455834  -0.38377935 -0.50351876 -0.31598938 -1.1642517\n",
      " -0.01870729  0.12697385  0.20496343 -0.46728417  0.13358793 -0.88426554\n",
      " -0.04302572 -1.4824363   0.1142078   0.28431258  0.33795682 -0.35601884\n",
      " -0.29773033 -0.094568   -0.33741197 -0.6216574  -0.4388934  -0.10293388\n",
      "  0.6531518   0.10786255  0.26016796 -0.41347447 -0.3220475   0.7839238\n",
      "  0.28004444 -0.821557   -0.3761048  -1.2729313   0.15165798 -0.704274\n",
      " -0.324365    0.09312496  0.6683509  -0.46454352 -0.5886878  -0.13666698\n",
      "  0.26208618  0.73057175  0.5987203  -0.6209031  -0.09796047 -0.08706918\n",
      " -0.6348762   0.38091007  0.6071874   0.00283846 -0.6701034   0.3290721\n",
      "  0.38075218  0.22828054 -0.17111486 -0.14341898 -0.96829104  0.61770016\n",
      "  0.33936504  0.43857652 -1.1546836   0.74387485 -0.4759511   0.7479842\n",
      "  0.8447653  -0.26978514  0.6147964   0.38195992  0.13779819 -0.1477413\n",
      " -0.62912107  0.28272995 -0.44971928 -0.18760633 -0.51154965  0.91269904\n",
      " -0.2782719   0.24237645  0.01013697  1.2143055   0.6798357   0.3198928\n",
      "  0.756081    0.41565794  0.07044017  0.2312795   1.1843814   0.62202126\n",
      "  0.39348653 -0.804551    0.26213837 -0.05690441]\n",
      "[('nice', 0.9998626708984375), ('great', 0.9998622536659241), ('get', 0.999859094619751), ('also', 0.9998572468757629), ('really', 0.9998539090156555), ('like', 0.9998521208763123), ('still', 0.9998520016670227), ('well', 0.999850332736969), ('use', 0.9998458027839661), ('little', 0.999842643737793)]\n",
      "10085\n"
     ]
    }
   ],
   "source": [
    "# Create word embeddings using word2vec for the cleaned reviews\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Create a list of list of words\n",
    "sentences = [review.split() for review in df2[\"reviewText2\"]]\n",
    "print(sentences[0])\n",
    "\n",
    "# Create the word2vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Save the model\n",
    "# model.save(\"word2vec.model\")\n",
    "\n",
    "# # Load the model\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Get the word vector for a word\n",
    "print(model.wv['good'])\n",
    "\n",
    "# Get the most similar words\n",
    "print(model.wv.most_similar('good'))\n",
    "\n",
    "# print total number of words in the model\n",
    "print(len(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-0bc46b168efa>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"w2v_emb_\" + str(i)] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model)[i])\n",
      "<ipython-input-74-0bc46b168efa>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"w2v_emb_\" + str(i)] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model)[i])\n",
      "<ipython-input-74-0bc46b168efa>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"w2v_emb_\" + str(i)] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model)[i])\n",
      "<ipython-input-74-0bc46b168efa>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"w2v_emb_\" + str(i)] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model)[i])\n",
      "<ipython-input-74-0bc46b168efa>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"w2v_emb_\" + str(i)] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model)[i])\n",
      "<ipython-input-74-0bc46b168efa>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"w2v_emb_\" + str(i)] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model)[i])\n",
      "<ipython-input-74-0bc46b168efa>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[\"w2v_emb_\" + str(i)] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model)[i])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>length_review</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_emb_90</th>\n",
       "      <th>w2v_emb_91</th>\n",
       "      <th>w2v_emb_92</th>\n",
       "      <th>w2v_emb_93</th>\n",
       "      <th>w2v_emb_94</th>\n",
       "      <th>w2v_emb_95</th>\n",
       "      <th>w2v_emb_96</th>\n",
       "      <th>w2v_emb_97</th>\n",
       "      <th>w2v_emb_98</th>\n",
       "      <th>w2v_emb_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40122</th>\n",
       "      <td>A3KZEGBTPH6MMF</td>\n",
       "      <td>B006IB35GG</td>\n",
       "      <td>Lucy Cat \"Mandy\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I am a bit of a pen connoisseur -- I've litera...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cheap feeling barrel - underwhelming performance</td>\n",
       "      <td>1331683200</td>\n",
       "      <td>03 14, 2012</td>\n",
       "      <td>159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314545</td>\n",
       "      <td>0.174229</td>\n",
       "      <td>0.029583</td>\n",
       "      <td>0.094286</td>\n",
       "      <td>0.506055</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.159431</td>\n",
       "      <td>-0.336931</td>\n",
       "      <td>0.109561</td>\n",
       "      <td>-0.018319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42089</th>\n",
       "      <td>A3EP26M3QHINXT</td>\n",
       "      <td>B00886N6YA</td>\n",
       "      <td>Nathan G</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I use this not for the RFID but for the protec...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Card cover</td>\n",
       "      <td>1357171200</td>\n",
       "      <td>01 3, 2013</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291546</td>\n",
       "      <td>0.162226</td>\n",
       "      <td>0.025777</td>\n",
       "      <td>0.084267</td>\n",
       "      <td>0.468024</td>\n",
       "      <td>0.248007</td>\n",
       "      <td>0.146568</td>\n",
       "      <td>-0.312164</td>\n",
       "      <td>0.103209</td>\n",
       "      <td>-0.019049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>A16PYNWJEUN5TL</td>\n",
       "      <td>B0008GQ26C</td>\n",
       "      <td>SANDYEGGO \"SALLY GOSSELIN\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Love the refills in Medium.  I have always bou...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LOVE THE MEDIUM REFILLS!!</td>\n",
       "      <td>1385769600</td>\n",
       "      <td>11 30, 2013</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336221</td>\n",
       "      <td>0.190737</td>\n",
       "      <td>0.032658</td>\n",
       "      <td>0.103419</td>\n",
       "      <td>0.545792</td>\n",
       "      <td>0.284632</td>\n",
       "      <td>0.169230</td>\n",
       "      <td>-0.362457</td>\n",
       "      <td>0.115924</td>\n",
       "      <td>-0.018728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20232</th>\n",
       "      <td>A3EZEP0FX5BC1P</td>\n",
       "      <td>B00290N0RO</td>\n",
       "      <td>A Conrad \"Ask Conrad\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a great sketchbook.  The papers are th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great SketchBook!</td>\n",
       "      <td>1403827200</td>\n",
       "      <td>06 27, 2014</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355736</td>\n",
       "      <td>0.200301</td>\n",
       "      <td>0.035174</td>\n",
       "      <td>0.108587</td>\n",
       "      <td>0.573154</td>\n",
       "      <td>0.304417</td>\n",
       "      <td>0.180462</td>\n",
       "      <td>-0.381223</td>\n",
       "      <td>0.121036</td>\n",
       "      <td>-0.022490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13503</th>\n",
       "      <td>A2065HBMYDXJ1S</td>\n",
       "      <td>B0010T3QT2</td>\n",
       "      <td>Jenn B \"Happy Mom\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These are good enevelopes, convenient to use. ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good product</td>\n",
       "      <td>1278979200</td>\n",
       "      <td>07 13, 2010</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470854</td>\n",
       "      <td>0.263674</td>\n",
       "      <td>0.046560</td>\n",
       "      <td>0.140443</td>\n",
       "      <td>0.757075</td>\n",
       "      <td>0.397847</td>\n",
       "      <td>0.240332</td>\n",
       "      <td>-0.502891</td>\n",
       "      <td>0.161757</td>\n",
       "      <td>-0.028047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID        asin                reviewerName helpful  \\\n",
       "40122  A3KZEGBTPH6MMF  B006IB35GG            Lucy Cat \"Mandy\"  [0, 0]   \n",
       "42089  A3EP26M3QHINXT  B00886N6YA                    Nathan G  [0, 0]   \n",
       "7610   A16PYNWJEUN5TL  B0008GQ26C  SANDYEGGO \"SALLY GOSSELIN\"  [0, 0]   \n",
       "20232  A3EZEP0FX5BC1P  B00290N0RO       A Conrad \"Ask Conrad\"  [0, 0]   \n",
       "13503  A2065HBMYDXJ1S  B0010T3QT2          Jenn B \"Happy Mom\"  [0, 0]   \n",
       "\n",
       "                                              reviewText  overall  \\\n",
       "40122  I am a bit of a pen connoisseur -- I've litera...      3.0   \n",
       "42089  I use this not for the RFID but for the protec...      5.0   \n",
       "7610   Love the refills in Medium.  I have always bou...      5.0   \n",
       "20232  This is a great sketchbook.  The papers are th...      5.0   \n",
       "13503  These are good enevelopes, convenient to use. ...      4.0   \n",
       "\n",
       "                                                summary  unixReviewTime  \\\n",
       "40122  Cheap feeling barrel - underwhelming performance      1331683200   \n",
       "42089                                        Card cover      1357171200   \n",
       "7610                          LOVE THE MEDIUM REFILLS!!      1385769600   \n",
       "20232                                 Great SketchBook!      1403827200   \n",
       "13503                                      Good product      1278979200   \n",
       "\n",
       "        reviewTime  length_review  ...  w2v_emb_90 w2v_emb_91 w2v_emb_92  \\\n",
       "40122  03 14, 2012            159  ...    0.314545   0.174229   0.029583   \n",
       "42089   01 3, 2013             37  ...    0.291546   0.162226   0.025777   \n",
       "7610   11 30, 2013             77  ...    0.336221   0.190737   0.032658   \n",
       "20232  06 27, 2014            113  ...    0.355736   0.200301   0.035174   \n",
       "13503  07 13, 2010             33  ...    0.470854   0.263674   0.046560   \n",
       "\n",
       "       w2v_emb_93  w2v_emb_94  w2v_emb_95  w2v_emb_96  w2v_emb_97  w2v_emb_98  \\\n",
       "40122    0.094286    0.506055    0.265732    0.159431   -0.336931    0.109561   \n",
       "42089    0.084267    0.468024    0.248007    0.146568   -0.312164    0.103209   \n",
       "7610     0.103419    0.545792    0.284632    0.169230   -0.362457    0.115924   \n",
       "20232    0.108587    0.573154    0.304417    0.180462   -0.381223    0.121036   \n",
       "13503    0.140443    0.757075    0.397847    0.240332   -0.502891    0.161757   \n",
       "\n",
       "       w2v_emb_99  \n",
       "40122   -0.018319  \n",
       "42089   -0.019049  \n",
       "7610    -0.018728  \n",
       "20232   -0.022490  \n",
       "13503   -0.028047  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then the word2vec embeddings are averaged for each review\n",
    "# The average word2vec embeddings are used as input to a classifier \n",
    "\n",
    "# # Create the average word2vec embeddings for each review\n",
    "# def get_average_word2vec_embeddings(review, model):\n",
    "#     words = review.split()\n",
    "#     embeddings = [model.wv[word] for word in words if word in model.wv.key_to_index]\n",
    "#     if len(embeddings) == 0:\n",
    "#         return np.zeros(model.vector_size)\n",
    "#     return np.mean(embeddings, axis=0)\n",
    "\n",
    "# df2[\"average_word2vec_embeddings\"] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model))\n",
    "# df2.head()\n",
    "\n",
    "\n",
    "# Create the average word2vec embeddings for each review and save them to the dataframe as columns  \n",
    "def get_average_word2vec_embeddings(review, model):\n",
    "    words = review.split()\n",
    "    embeddings = [model.wv[word] for word in words if word in model.wv.key_to_index]\n",
    "    if len(embeddings) == 0:\n",
    "        return [0]*model.vector_size\n",
    "    return list(np.mean(embeddings, axis=0))\n",
    "\n",
    "\n",
    "# save the embeddings as columns in the dataframe\n",
    "for i in range(model.vector_size):\n",
    "    df2[\"w2v_emb_\" + str(i)] = df2[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, model)[i])\n",
    "\n",
    "df2.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9351351351351351\n",
      "12:15:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/keras2/lib/python3.9/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis using the word2vec embeddings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# convert Overall column into binary sentiment  1 for positive and 0 for negative \n",
    "# 1 and 2 are negative and 4 and 5 are positive, category 3 is removed from the data \n",
    "# start by removing the rows with overall 3\n",
    "df2 = df2[df2[\"overall\"] != 3]\n",
    "\n",
    "# convert the overall column into binary sentiment\n",
    "df2[\"response\"] = df2[\"overall\"].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "df2[\"response\"].value_counts()\n",
    "\n",
    "# print list of columns with the column number\n",
    "# for i, col in enumerate(df2.columns):\n",
    "#     print(i, col)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets. Features are the word2vec embeddings and the target is the response column\n",
    "X = df2.iloc[:, 13:113]\n",
    "y = df2[\"response\"]\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
