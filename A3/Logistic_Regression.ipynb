{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment3\n",
    "Repository Link: [Github](https://github.com/awakn123/CS6120NLP/tree/main)\n",
    "\n",
    "Members: Yun Cao, Yue Liu, Nan Chen, Muyang Cheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing:\n",
    "1.1 Load the dataset and perform initial exploration to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin                 reviewerName helpful  \\\n",
      "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
      "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
      "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
      "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
      "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  This is a great tutu and at a really great pri...      5.0   \n",
      "1  I bought this for my 4 yr old daughter for dan...      5.0   \n",
      "2  What can I say... my daughters have it in oran...      5.0   \n",
      "3  We bought several tutus at once, and they are ...      5.0   \n",
      "4  Thank you Halo Heaven great product for Little...      5.0   \n",
      "\n",
      "                         summary  unixReviewTime   reviewTime  \n",
      "0  Great tutu-  not cheaply made      1297468800  02 12, 2011  \n",
      "1                    Very Cute!!      1358553600  01 19, 2013  \n",
      "2       I have buy more than one      1357257600   01 4, 2013  \n",
      "3               Adorable, Sturdy      1398556800  04 27, 2014  \n",
      "4        Grammy's Angels Love it      1394841600  03 15, 2014  \n"
     ]
    }
   ],
   "source": [
    "# data link: https://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open('./Clothing_Shoes_and_Jewelry_5.json'\n",
    "            , 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "#df = df.sample(1000)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Clean the text data, including removing special characters, stopwords, applying lowercasing, and other tasks as\n",
    "you deem necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nanchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nanchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pkg_resources\n",
    "import inflect\n",
    "import contractions\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p = inflect.engine()\n",
    "\n",
    "def standardize_numbers(text):\n",
    "    return ' '.join([p.number_to_words(word) if word.isdigit() else word for word in text.split()])\n",
    "\n",
    "def handle_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # standardize\n",
    "    text = standardize_numbers(text)\n",
    "    # handle contractions\n",
    "    text = handle_contractions(text)\n",
    "    text = text.strip()\n",
    "    # correct typos\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)\n",
    "        corrected_words.append(suggestions[0].term if suggestions else word)\n",
    "    text = ' '.join(corrected_words)\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # remove stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # rejoin words\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "df[\"reviewText2\"] = df[\"reviewText\"].apply(lambda x: clean_text(x))\n",
    "df[\"summary2\"] = df[\"summary\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word2Vec, fasttext embeddings\n",
    "2.1 Create 100D vectors using both Word2Vec (CBOW and SkipGram separately), and fasttext algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [review.split() for review in df[\"reviewText2\"]]\n",
    "\n",
    "# Word2Vec CBOW Model\n",
    "cbow_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "#cbow_model.save(\"cbow_word2vec.model\")\n",
    "\n",
    "# Word2Vec Skip-gram Model\n",
    "skipgram_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "#skipgram_model.save(\"skipgram_word2vec.model\")\n",
    "\n",
    "# FastText Model\n",
    "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "#fasttext_model.save(\"fasttext.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Average the vectors to create new average vector columns in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cbow_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cbow_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cbow_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin                 reviewerName helpful  \\\n",
      "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
      "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
      "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
      "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
      "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  This is a great tutu and at a really great pri...      5.0   \n",
      "1  I bought this for my 4 yr old daughter for dan...      5.0   \n",
      "2  What can I say... my daughters have it in oran...      5.0   \n",
      "3  We bought several tutus at once, and they are ...      5.0   \n",
      "4  Thank you Halo Heaven great product for Little...      5.0   \n",
      "\n",
      "                         summary  unixReviewTime   reviewTime  \\\n",
      "0  Great tutu-  not cheaply made      1297468800  02 12, 2011   \n",
      "1                    Very Cute!!      1358553600  01 19, 2013   \n",
      "2       I have buy more than one      1357257600   01 4, 2013   \n",
      "3               Adorable, Sturdy      1398556800  04 27, 2014   \n",
      "4        Grammy's Angels Love it      1394841600  03 15, 2014   \n",
      "\n",
      "                                         reviewText2  ... w2v_emb_90  \\\n",
      "0  great tutu really great price look cheap glad ...  ...   0.294041   \n",
      "1  bought four old daughter dance class wore toda...  ...   0.098229   \n",
      "2  say daughter orange black white pink thinking ...  ...   0.143071   \n",
      "3  bought several tutu got high review sturdy see...  ...  -0.036704   \n",
      "4  thank halo heaven great product little girl gr...  ...   0.354553   \n",
      "\n",
      "   w2v_emb_91  w2v_emb_92  w2v_emb_93  w2v_emb_94  w2v_emb_95  w2v_emb_96  \\\n",
      "0   -0.120445   -0.197830   -0.178954   -0.977380   -0.497021   -0.489830   \n",
      "1    0.504496    0.187078   -0.280830   -0.065657    0.165452   -0.205845   \n",
      "2    0.487754   -0.195949   -0.440255   -0.634742   -0.171475   -0.280737   \n",
      "3   -0.080966    0.396431   -0.738515   -0.422752    0.486443   -0.400427   \n",
      "4    0.232924   -0.096165   -0.121132   -0.484024   -0.277721    0.173312   \n",
      "\n",
      "   w2v_emb_97  w2v_emb_98  w2v_emb_99  \n",
      "0    0.499380   -0.351498   -0.884413  \n",
      "1    0.687781   -0.036331   -0.061464  \n",
      "2    0.355474    0.190785    0.522369  \n",
      "3   -0.125408   -0.110500    0.380247  \n",
      "4    0.332112    0.236647   -0.130602  \n",
      "\n",
      "[5 rows x 111 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_skipgram_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_skipgram_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_skipgram_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin                 reviewerName helpful  \\\n",
      "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
      "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
      "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
      "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
      "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  This is a great tutu and at a really great pri...      5.0   \n",
      "1  I bought this for my 4 yr old daughter for dan...      5.0   \n",
      "2  What can I say... my daughters have it in oran...      5.0   \n",
      "3  We bought several tutus at once, and they are ...      5.0   \n",
      "4  Thank you Halo Heaven great product for Little...      5.0   \n",
      "\n",
      "                         summary  unixReviewTime   reviewTime  \\\n",
      "0  Great tutu-  not cheaply made      1297468800  02 12, 2011   \n",
      "1                    Very Cute!!      1358553600  01 19, 2013   \n",
      "2       I have buy more than one      1357257600   01 4, 2013   \n",
      "3               Adorable, Sturdy      1398556800  04 27, 2014   \n",
      "4        Grammy's Angels Love it      1394841600  03 15, 2014   \n",
      "\n",
      "                                         reviewText2  ... w2v_emb_90  \\\n",
      "0  great tutu really great price look cheap glad ...  ...   0.241528   \n",
      "1  bought four old daughter dance class wore toda...  ...   0.225947   \n",
      "2  say daughter orange black white pink thinking ...  ...   0.192308   \n",
      "3  bought several tutu got high review sturdy see...  ...   0.197181   \n",
      "4  thank halo heaven great product little girl gr...  ...   0.168201   \n",
      "\n",
      "   w2v_emb_91  w2v_emb_92  w2v_emb_93  w2v_emb_94  w2v_emb_95  w2v_emb_96  \\\n",
      "0    0.129186    0.000738    0.078560    0.504264    0.020075    0.011737   \n",
      "1    0.175916    0.048717    0.057112    0.550466    0.120765    0.160151   \n",
      "2    0.153766   -0.055079    0.072293    0.469271    0.138732    0.170359   \n",
      "3    0.129775    0.033536   -0.063423    0.520231    0.160741    0.142975   \n",
      "4    0.136322   -0.021825    0.084355    0.544928    0.076211    0.182383   \n",
      "\n",
      "   w2v_emb_97  w2v_emb_98  w2v_emb_99  \n",
      "0   -0.308710    0.042648   -0.186743  \n",
      "1   -0.299378    0.215881   -0.100509  \n",
      "2   -0.368036    0.249865   -0.129469  \n",
      "3   -0.284224    0.217417   -0.040975  \n",
      "4   -0.392101    0.182893   -0.131282  \n",
      "\n",
      "[5 rows x 111 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_fasttext_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_fasttext_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin                 reviewerName helpful  \\\n",
      "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
      "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
      "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
      "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
      "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  This is a great tutu and at a really great pri...      5.0   \n",
      "1  I bought this for my 4 yr old daughter for dan...      5.0   \n",
      "2  What can I say... my daughters have it in oran...      5.0   \n",
      "3  We bought several tutus at once, and they are ...      5.0   \n",
      "4  Thank you Halo Heaven great product for Little...      5.0   \n",
      "\n",
      "                         summary  unixReviewTime   reviewTime  \\\n",
      "0  Great tutu-  not cheaply made      1297468800  02 12, 2011   \n",
      "1                    Very Cute!!      1358553600  01 19, 2013   \n",
      "2       I have buy more than one      1357257600   01 4, 2013   \n",
      "3               Adorable, Sturdy      1398556800  04 27, 2014   \n",
      "4        Grammy's Angels Love it      1394841600  03 15, 2014   \n",
      "\n",
      "                                         reviewText2  ... w2v_emb_90  \\\n",
      "0  great tutu really great price look cheap glad ...  ...  -0.237940   \n",
      "1  bought four old daughter dance class wore toda...  ...  -0.284403   \n",
      "2  say daughter orange black white pink thinking ...  ...  -0.343621   \n",
      "3  bought several tutu got high review sturdy see...  ...  -0.369845   \n",
      "4  thank halo heaven great product little girl gr...  ...  -0.365828   \n",
      "\n",
      "   w2v_emb_91  w2v_emb_92  w2v_emb_93  w2v_emb_94  w2v_emb_95  w2v_emb_96  \\\n",
      "0    0.019655   -0.091165    0.219527    0.111780    0.184421    0.098848   \n",
      "1   -0.094563   -0.099353    0.304655    0.003859    0.148078    0.138870   \n",
      "2   -0.055822   -0.134367    0.237949   -0.004880    0.122978    0.221271   \n",
      "3   -0.041572   -0.035714    0.215813    0.047958    0.121645    0.081073   \n",
      "4    0.017824   -0.151376    0.259455   -0.017317    0.165561    0.124667   \n",
      "\n",
      "   w2v_emb_97  w2v_emb_98  w2v_emb_99  \n",
      "0    0.071237   -0.006235    0.404495  \n",
      "1    0.059291   -0.077056    0.494698  \n",
      "2    0.076228   -0.079934    0.480858  \n",
      "3    0.076235   -0.049222    0.597342  \n",
      "4   -0.008211   -0.117554    0.382680  \n",
      "\n",
      "[5 rows x 111 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_fasttext_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_average_word2vec_embeddings(review, model):\n",
    "    words = review.split()\n",
    "    embeddings = [model.wv[word] for word in words if word in model.wv.key_to_index]\n",
    "    if len(embeddings) == 0:\n",
    "        return [0]*model.vector_size\n",
    "    return list(np.mean(embeddings, axis=0))\n",
    "\n",
    "df_cbow_model = df.copy()\n",
    "embeddings = df_cbow_model[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, cbow_model))\n",
    "for i in range(cbow_model.vector_size):\n",
    "    df_cbow_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i]) \n",
    "print(df_cbow_model.head())\n",
    "\n",
    "df_skipgram_model = df.copy()\n",
    "embeddings = df_skipgram_model[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, skipgram_model))\n",
    "for i in range(skipgram_model.vector_size):\n",
    "    df_skipgram_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
    "print(df_skipgram_model.head())\n",
    "\n",
    "df_fasttext_model = df.copy()\n",
    "embeddings = df_fasttext_model[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, fasttext_model))\n",
    "for i in range(fasttext_model.vector_size):\n",
    "    df_fasttext_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
    "print(df_fasttext_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Perform EDA to analyze associations between vectors from the three methods above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 common words in three models\n",
    "words = list(cbow_model.wv.key_to_index.keys())[:100]\n",
    "\n",
    "# Extract the vectors for these common words from each model\n",
    "cbow_vectors = np.array([cbow_model.wv[word] for word in words])\n",
    "skipgram_vectors = np.array([skipgram_model.wv[word] for word in words])\n",
    "fasttext_vectors = np.array([fasttext_model.wv[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with t-SNE\n",
    "#Use t-SNE to reduce the dimensionality of your word vectors to two or three dimensions.\n",
    "#Plot the results to see how words cluster together.\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_word_vectors(model_vectors, words, title='Word Vectors Visualization'):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    vectors_tsne = tsne.fit_transform(model_vectors)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, word in enumerate(words):\n",
    "        plt.scatter(vectors_tsne[i, 0], vectors_tsne[i, 1])\n",
    "        plt.annotate(word, (vectors_tsne[i, 0], vectors_tsne[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.title(title)\n",
    "\n",
    "visualize_word_vectors(cbow_vectors, words, 'CBOW t-SNE Visualization')\n",
    "visualize_word_vectors(skipgram_vectors, words, 'Skip-gram t-SNE Visualization')\n",
    "visualize_word_vectors(fasttext_vectors, words, 'FastText t-SNE Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with KMeans\n",
    "# Apply clustering algorithms like K-Means on the word vectors.\n",
    "# Analyze the clusters to see which words are grouped together by the model.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def cluster_and_plot_words(model_vectors, common_words, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(model_vectors)\n",
    "    cluster_map = {i: [] for i in range(n_clusters)}\n",
    "    for word, label in zip(common_words, labels):\n",
    "        cluster_map[label].append(word)\n",
    "\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]\n",
    "        cluster_points = model_vectors[cluster_indices]\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=colors[cluster_id], label=f'Cluster {cluster_id}', alpha=0.5)\n",
    "    \n",
    "    for i, word in enumerate(common_words):\n",
    "        plt.text(model_vectors[i, 0], model_vectors[i, 1], word, fontsize=9)\n",
    "    \n",
    "    plt.title('Word Vectors Clustering')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return cluster_map\n",
    "\n",
    "cbow_clusters = cluster_and_plot_words(cbow_vectors, words, n_clusters=10)\n",
    "skipgram_clusters = cluster_and_plot_words(skipgram_vectors, words, n_clusters=10)\n",
    "fasttext_clusters = cluster_and_plot_words(fasttext_vectors, words, n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Analysis\n",
    "# Compute the cosine similarity between word vectors\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def compare_similarity(word1, word2, model1, model2, model3, model_name1='CBOW', model_name2='Skip-gram', model_name3='FastText'):\n",
    "    similarity1 = 1 - cosine(model1.wv[word1], model1.wv[word2])\n",
    "    similarity2 = 1 - cosine(model2.wv[word1], model2.wv[word2])\n",
    "    similarity3 = 1 - cosine(model3.wv[word1], model3.wv[word2])\n",
    "    print(f'{word1} & {word2} similarity in {model_name1}: {similarity1:.4f}')\n",
    "    print(f'{word1} & {word2} similarity in {model_name2}: {similarity2:.4f}')\n",
    "    print(f'{word1} & {word2} similarity in {model_name3}: {similarity3:.4f}')\n",
    "\n",
    "compare_similarity('shirt', 'hoodie', cbow_model, skipgram_model, fasttext_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Convert the ratings to 0 for negative and 1 for positive\n",
    "def convert_rating_to_sentiment(overall):\n",
    "  if overall in [1, 2]:\n",
    "      return 0  # Negative sentiment\n",
    "  elif overall in [4, 5]:\n",
    "      return 1  # Positive sentiment\n",
    "  # Optionally handle unexpected cases, though all cases should be covered\n",
    "  return None\n",
    "\n",
    "def convertAndReduceDimension(df):\n",
    "\t\tdf_filtered = df[df['overall'] != 3]\n",
    "\t\tdf_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
    "\t\tvector_cols = [col for col in df_filtered.columns if col.startswith('w2v_emb_')]\n",
    "\t\tX = df_filtered[vector_cols]\n",
    "\t\tscaler = StandardScaler()\n",
    "\t\tX_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\t\t# Step 2: Apply PCA to retain 90% of the variance\n",
    "\t\tpca = PCA(n_components=0.9)  # n_components set to 0.9 means PCA will select the minimum number of components that retain 90% of the variance\n",
    "\t\tX_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\t\t# Create a DataFrame with the PCA features\n",
    "\t\tdf_pca = pd.DataFrame(X_pca, columns=[f'PCA_{i+1}' for i in range(X_pca.shape[1])])\n",
    "\n",
    "\t\t# Optionally, if you want to include the sentiment back into the PCA-transformed dataset\n",
    "\t\tdf_pca['sentiment'] = df_filtered['sentiment'].values\n",
    "\n",
    "\t\treturn df_pca\n",
    "  \n",
    "df_cbow = convertAndReduceDimension(df_cbow_model)\n",
    "df_skipgram = convertAndReduceDimension(df_skipgram_model)\n",
    "df_fasttext = convertAndReduceDimension(df_fasttext_model)\n",
    "\n",
    "# Define your DataFrame to hold the results\n",
    "results_df = pd.DataFrame(columns=['model', 'data_model', 'accuracy', 'precision', 'f1-score', 'tpr', 'fpr', 'auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use GridSearchCV for hyperparameter tuning to find the best parameters for Logistic Regression model.\n",
    "\n",
    "In this code, GridSearchCV performs an exhaustive search over the specified parameter grid, \n",
    "and best_params_ gives us the best parameters found during the search. Then can then use \n",
    "these parameters to train a new classifier.\n",
    "\n",
    "For 'C', we choose the value in the list [0.001, 0.01, 0.1, 1, 10, 100, 1000]. For 'max_iter', we choose the value in the list [100, 500, 1000, 5000, 10000, 50000].  For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones. ‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas ‘liblinear’ and ‘saga’ handle L1 penalty.\n",
    "\n",
    "When choosing the values for these parameters in GridSearchCV object. For the value of cv, a common choice is 5 or 10, but considering we have a large dataset, a smaller number can be used to save computational time. So, we choose 5. For \"scoring\", considering we’re dealing with a classification problem, we use ‘accuracy’. For other parameters like 'n_jobs', 'verbose', 'return_train_score', we use the default values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "def train_with_logistic_regression(df, param_grid, data_type):\n",
    "    # Assuming df_final is your final DataFrame after PCA\n",
    "\tX = df.drop('sentiment', axis=1).values  # Features\n",
    "\ty = df['sentiment'].values  # Labels\n",
    "\n",
    "\t# Splitting the data into training and validation sets\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\t# Create a Logistic Regression model\n",
    "\tmodel = LogisticRegression()\n",
    "\n",
    "\t# Create a GridSearchCV object\n",
    "\tgrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "\t# Fit the GridSearchCV object to the data\n",
    "\tgrid_search.fit(X_train, y_train)\n",
    "\n",
    "\t# Get the best parameters\n",
    "\tbest_params = grid_search.best_params_\n",
    "\n",
    "\t# Print the best parameters\n",
    "\tprint(f\"Best parameters for {data_type}: {best_params}\")\n",
    "\n",
    "\t# Fit the model with the best parameters to the training data\n",
    "\tmodel_best = LogisticRegression(**best_params)\n",
    "\tmodel_best.fit(X_train, y_train)\n",
    "\n",
    "\t# Make predictions on the test set\n",
    "\ty_pred = model_best.predict(X_val)\n",
    "\n",
    "\t# Evaluate the model's performance\n",
    "\taccuracy = accuracy_score(y_val, y_pred)\n",
    "\tprecision = precision_score(y_val, y_pred, average='weighted', zero_division=0)  # Handling division by zero for classes with no predictions\n",
    "\trecall = recall_score(y_val, y_pred, average='weighted')\n",
    "\tf1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\t# Calculate the probability estimates of the positive class\n",
    "\ty_pred_proba = model_best.predict_proba(X_val)[:, 1]\n",
    "\n",
    "\t# Calculate the ROC AUC\n",
    "\tauc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "\t# Calculate the FPR, TPR, and thresholds\n",
    "\tfpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
    "\n",
    "\t# Append new results\n",
    "\tmetrics_dict = {\n",
    "\t    'model': 'Logistic Regression',\n",
    "\t    'data_model': data_type,\n",
    "\t    'accuracy': accuracy,\n",
    "\t    'precision': precision,\n",
    "\t    'f1-score': f1,\n",
    "\t    'tpr': recall,\n",
    "\t    'fpr': fpr[1],\n",
    "\t    'auc': auc\n",
    "\t}\n",
    "\tbest_params_dict = {\n",
    "\t\t'model': 'Logistic Regression', \n",
    "\t\t'data_mode': data_type, \n",
    "\t\t'C':best_params['C'], \n",
    "\t\t'max_iter':best_params['max_iter'], \n",
    "\t\t'penalty':best_params['penalty'], \n",
    "\t\t'solver':best_params['solver']\n",
    "\t}\n",
    "\treturn metrics_dict, best_params_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logistic_regression(param_grids):\n",
    "    best_parameters_df = pd.DataFrame(columns=['model', 'data_mode', 'C', 'max_iter', 'penalty', 'solver'])\n",
    "    results_df = pd.DataFrame(columns=['model', 'data_model', 'accuracy', 'precision', 'f1-score', 'tpr', 'fpr', 'auc'])\n",
    "\n",
    "    for data_model, df in [('cbow', df_cbow), ('skipgram', df_skipgram), ('fasttext', df_fasttext)]:\n",
    "        param_grid = param_grids[data_model]\n",
    "        data, logistic_regression_best_params_dict = train_with_logistic_regression(df, param_grid, data_model)\n",
    "        results_df = results_df._append(data, ignore_index=True)\n",
    "        best_parameters_df = best_parameters_df._append(logistic_regression_best_params_dict, ignore_index=True)\n",
    "\n",
    "    return best_parameters_df, results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:542: FitFailedWarning: \n",
      "630 fits failed out of a total of 2100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.91552913        nan 0.91594202 0.92104269\n",
      " 0.92101248 0.92046868 0.92104269 0.92104269        nan        nan\n",
      " 0.91552913        nan 0.91594202 0.92104269 0.92101248 0.92046868\n",
      " 0.92104269 0.92104773        nan        nan 0.91552913        nan\n",
      " 0.91593698 0.92104269 0.92101248 0.92046868 0.92104269 0.92104269\n",
      "        nan        nan 0.91552913        nan 0.91593698 0.92104269\n",
      " 0.92101248 0.92046868 0.92104773 0.92104773        nan        nan\n",
      " 0.91552913        nan 0.91593698 0.92104269 0.92101248 0.92046868\n",
      " 0.92104269 0.92104773        nan        nan 0.91552913        nan\n",
      " 0.91593195 0.92104269 0.92101248 0.92046868 0.92104773 0.92104773\n",
      "        nan        nan 0.92158146        nan 0.92161167 0.92202456\n",
      " 0.9220447  0.92186847 0.92203463 0.92203463        nan        nan\n",
      " 0.92158146        nan 0.92161671 0.92202456 0.9220447  0.92186847\n",
      " 0.92203463 0.92203463        nan        nan 0.92158146        nan\n",
      " 0.92162174 0.92202456 0.9220447  0.92186847 0.92203463 0.92203463\n",
      "        nan        nan 0.92158146        nan 0.92161167 0.92202456\n",
      " 0.9220447  0.92186847 0.92203463 0.92203463        nan        nan\n",
      " 0.92158146        nan 0.92162174 0.92202456 0.9220447  0.92186847\n",
      " 0.92203463 0.92203463        nan        nan 0.92158146        nan\n",
      " 0.92161167 0.92202456 0.9220447  0.92186847 0.92203463 0.92203967\n",
      "        nan        nan 0.92207995        nan 0.92205981 0.92207492\n",
      " 0.92207492 0.92206988 0.92206485 0.92206485        nan        nan\n",
      " 0.92207995        nan 0.92205981 0.92207492 0.92207492 0.92206988\n",
      " 0.92206485 0.92206988        nan        nan 0.92207995        nan\n",
      " 0.92206484 0.92207492 0.92207492 0.92206988 0.92206485 0.92206988\n",
      "        nan        nan 0.92207995        nan 0.92205981 0.92207492\n",
      " 0.92207492 0.92206988 0.92206485 0.92206988        nan        nan\n",
      " 0.92207995        nan 0.92205981 0.92207492 0.92207492 0.92206988\n",
      " 0.92206485 0.92206988        nan        nan 0.92207995        nan\n",
      " 0.92205981 0.92207492 0.92207492 0.92206988 0.92206988 0.92206988\n",
      "        nan        nan 0.92210009        nan 0.92209506 0.92211016\n",
      " 0.92210513 0.92209506 0.92209506 0.92210009        nan        nan\n",
      " 0.92210009        nan 0.92210009 0.92211016 0.92210513 0.92209506\n",
      " 0.92210009 0.92210009        nan        nan 0.92210009        nan\n",
      " 0.92209506 0.92211016 0.92210513 0.92209506 0.92209506 0.92210513\n",
      "        nan        nan 0.92210009        nan 0.92210009 0.92211016\n",
      " 0.92210513 0.92209506 0.92210009 0.92210009        nan        nan\n",
      " 0.92210009        nan 0.92210009 0.92211016 0.92210513 0.92209506\n",
      " 0.92210009 0.92210009        nan        nan 0.92210009        nan\n",
      " 0.92209506 0.92211016 0.92210513 0.92209506 0.92210009 0.92209506\n",
      "        nan        nan 0.92210009        nan 0.92210009 0.92211016\n",
      " 0.92211016 0.92210009 0.92210009 0.92210513        nan        nan\n",
      " 0.92210009        nan 0.92210513 0.92211016 0.92211016 0.92210009\n",
      " 0.92210513 0.92210513        nan        nan 0.92210009        nan\n",
      " 0.92210513 0.92211016 0.92211016 0.92210009 0.92210513 0.92210513\n",
      "        nan        nan 0.92210009        nan 0.92210513 0.92211016\n",
      " 0.92211016 0.92210009 0.92210009 0.92210009        nan        nan\n",
      " 0.92210009        nan 0.92210009 0.92211016 0.92211016 0.92210009\n",
      " 0.92210513 0.92210513        nan        nan 0.92210009        nan\n",
      " 0.92210513 0.92211016 0.92211016 0.92210009 0.92210009 0.92210513\n",
      "        nan        nan 0.92210513        nan 0.92210513 0.92211016\n",
      " 0.92211016 0.92210009 0.92210009 0.92210513        nan        nan\n",
      " 0.92210513        nan 0.92211016 0.92211016 0.92211016 0.92210009\n",
      " 0.92210513 0.92210009        nan        nan 0.92210513        nan\n",
      " 0.92210009 0.92211016 0.92211016 0.92210009 0.92210009 0.92210513\n",
      "        nan        nan 0.92210513        nan 0.92210009 0.92211016\n",
      " 0.92211016 0.92210009 0.92210009 0.92210513        nan        nan\n",
      " 0.92210009        nan 0.92210513 0.92211016 0.92211016 0.92210009\n",
      " 0.92210009 0.92210513        nan        nan 0.92210009        nan\n",
      " 0.92210513 0.92211016 0.92211016 0.92210009 0.92210513 0.92209506\n",
      "        nan        nan 0.92210009        nan 0.92210009 0.92211016\n",
      " 0.92211016 0.92210009 0.92210009 0.92210513        nan        nan\n",
      " 0.92210513        nan 0.92210513 0.92211016 0.92211016 0.92210009\n",
      " 0.92210009 0.92210513        nan        nan 0.92210009        nan\n",
      " 0.92210513 0.92211016 0.92211016 0.92210009 0.92210009 0.92210513\n",
      "        nan        nan 0.92210009        nan 0.92210513 0.92211016\n",
      " 0.92211016 0.92210009 0.92210513 0.92210009        nan        nan\n",
      " 0.92210513        nan 0.92210513 0.92211016 0.92211016 0.92210009\n",
      " 0.92210009 0.92210009        nan        nan 0.92210009        nan\n",
      " 0.92210009 0.92211016 0.92211016 0.92210009 0.92210513 0.92210009]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for cbow: {'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/485907921.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = results_df._append(data, ignore_index=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:542: FitFailedWarning: \n",
      "630 fits failed out of a total of 2100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.9207708         nan 0.92101249 0.92760359\n",
      " 0.92760863 0.92721588 0.92760359 0.92760359        nan        nan\n",
      " 0.9207708         nan 0.92101249 0.92760359 0.92760863 0.92721588\n",
      " 0.92760359 0.92760359        nan        nan 0.9207708         nan\n",
      " 0.92101249 0.92760359 0.92760863 0.92721588 0.92760359 0.92760359\n",
      "        nan        nan 0.9207708         nan 0.92101753 0.92760359\n",
      " 0.92760863 0.92721588 0.92760359 0.92760863        nan        nan\n",
      " 0.9207708         nan 0.92101753 0.92760359 0.92760863 0.92721588\n",
      " 0.92760359 0.92760359        nan        nan 0.9207708         nan\n",
      " 0.92101249 0.92760359 0.92760863 0.92721588 0.92760359 0.92760359\n",
      "        nan        nan 0.92840923        nan 0.92837902 0.92887247\n",
      " 0.92890771 0.92889764 0.92888254 0.92888757        nan        nan\n",
      " 0.92840923        nan 0.92837398 0.92887247 0.92890771 0.92889764\n",
      " 0.92888757 0.92888254        nan        nan 0.92840923        nan\n",
      " 0.92837902 0.92887247 0.92890771 0.92889764 0.92888254 0.92888757\n",
      "        nan        nan 0.92840923        nan 0.92837902 0.92887247\n",
      " 0.92890771 0.92889764 0.92888254 0.92888757        nan        nan\n",
      " 0.92840923        nan 0.92838405 0.92887247 0.92890771 0.92889764\n",
      " 0.9288775  0.92888757        nan        nan 0.92840923        nan\n",
      " 0.92837902 0.92887247 0.92890771 0.92889764 0.92888757 0.9288775\n",
      "        nan        nan 0.92909402        nan 0.92907388 0.92909402\n",
      " 0.92912926 0.92910912 0.92909905 0.92910409        nan        nan\n",
      " 0.92909905        nan 0.92906884 0.92909402 0.92912926 0.92910912\n",
      " 0.92909905 0.92910409        nan        nan 0.92909402        nan\n",
      " 0.92906884 0.92909402 0.92912926 0.92910912 0.92909905 0.92909905\n",
      "        nan        nan 0.92909905        nan 0.92906884 0.92909402\n",
      " 0.92912926 0.92910912 0.92909905 0.92910409        nan        nan\n",
      " 0.92909905        nan 0.92907388 0.92909402 0.92912926 0.92910912\n",
      " 0.92910409 0.92909905        nan        nan 0.92909905        nan\n",
      " 0.92906381 0.92909402 0.92912926 0.92910912 0.92909905 0.92909905\n",
      "        nan        nan 0.92909905        nan 0.92910912 0.92908395\n",
      " 0.92912926 0.92910912 0.92910409 0.92910409        nan        nan\n",
      " 0.92910409        nan 0.92909905 0.92908395 0.92912926 0.92910912\n",
      " 0.92910409 0.92910409        nan        nan 0.92910409        nan\n",
      " 0.92909905 0.92908395 0.92912926 0.92910912 0.92910409 0.92910409\n",
      "        nan        nan 0.92910409        nan 0.92909905 0.92908395\n",
      " 0.92912926 0.92910912 0.92910912 0.92910409        nan        nan\n",
      " 0.92910409        nan 0.92910409 0.92908395 0.92912926 0.92910912\n",
      " 0.92910409 0.92910409        nan        nan 0.92910409        nan\n",
      " 0.92910912 0.92908395 0.92912926 0.92910912 0.92910409 0.92909905\n",
      "        nan        nan 0.92910409        nan 0.92910409 0.92907891\n",
      " 0.92912423 0.92910409 0.92910409 0.92910409        nan        nan\n",
      " 0.92910409        nan 0.92909905 0.92907891 0.92912423 0.92910409\n",
      " 0.92910409 0.92909905        nan        nan 0.92910409        nan\n",
      " 0.92910409 0.92907891 0.92912423 0.92910409 0.92909905 0.92909905\n",
      "        nan        nan 0.92910409        nan 0.92910409 0.92907891\n",
      " 0.92912423 0.92910409 0.92910409 0.92909905        nan        nan\n",
      " 0.92910409        nan 0.92909402 0.92907891 0.92912423 0.92910409\n",
      " 0.92910409 0.92909905        nan        nan 0.92910409        nan\n",
      " 0.92909905 0.92907891 0.92912423 0.92910409 0.92909905 0.92909905\n",
      "        nan        nan 0.92910409        nan 0.92910409 0.92908395\n",
      " 0.92912423 0.92910409 0.92910409 0.92909905        nan        nan\n",
      " 0.92910409        nan 0.92909905 0.92908395 0.92912423 0.92910409\n",
      " 0.92910409 0.92909905        nan        nan 0.92910409        nan\n",
      " 0.92910409 0.92908395 0.92912423 0.92910409 0.92910409 0.92909905\n",
      "        nan        nan 0.92910409        nan 0.92910409 0.92908395\n",
      " 0.92912423 0.92910409 0.92909905 0.92910409        nan        nan\n",
      " 0.92910409        nan 0.92909402 0.92908395 0.92912423 0.92910409\n",
      " 0.92910409 0.92909905        nan        nan 0.92910409        nan\n",
      " 0.92910409 0.92908395 0.92912423 0.92910409 0.92909905 0.92909905\n",
      "        nan        nan 0.92910409        nan 0.92910409 0.92908395\n",
      " 0.92912423 0.92910409 0.92909905 0.92909905        nan        nan\n",
      " 0.92910409        nan 0.92909402 0.92908395 0.92912423 0.92910409\n",
      " 0.92910409 0.92909402        nan        nan 0.92909905        nan\n",
      " 0.92909905 0.92908395 0.92912423 0.92910409 0.92910409 0.92909905\n",
      "        nan        nan 0.92910409        nan 0.92910912 0.92908395\n",
      " 0.92912423 0.92910409 0.92910409 0.92909905        nan        nan\n",
      " 0.92910409        nan 0.92909905 0.92908395 0.92912423 0.92910409\n",
      " 0.92910409 0.92910409        nan        nan 0.92910409        nan\n",
      " 0.92909905 0.92908395 0.92912423 0.92910409 0.92909905 0.92909905]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for skipgram: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:542: FitFailedWarning: \n",
      "630 fits failed out of a total of 2100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.92022196        nan 0.92041833 0.92709503\n",
      " 0.92709503 0.9269037  0.92710007 0.9271051         nan        nan\n",
      " 0.92022196        nan 0.92042337 0.92709503 0.92709503 0.9269037\n",
      " 0.92710007 0.9271051         nan        nan 0.92022196        nan\n",
      " 0.92041833 0.92709503 0.92709503 0.9269037  0.92710007 0.92710007\n",
      "        nan        nan 0.92022196        nan 0.9204284  0.92709503\n",
      " 0.92709503 0.9269037  0.92710007 0.9271051         nan        nan\n",
      " 0.92022196        nan 0.92041833 0.92709503 0.92709503 0.9269037\n",
      " 0.92710007 0.9271051         nan        nan 0.92022699        nan\n",
      " 0.92042337 0.92709503 0.92709503 0.9269037  0.92710007 0.9271051\n",
      "        nan        nan 0.92790067        nan 0.92793088 0.92823803\n",
      " 0.92823299 0.92822292 0.92823299 0.92823299        nan        nan\n",
      " 0.92790067        nan 0.92793088 0.92823803 0.92823299 0.92822292\n",
      " 0.92823803 0.92823299        nan        nan 0.92790067        nan\n",
      " 0.92793088 0.92823803 0.92823299 0.92822292 0.92823299 0.92823803\n",
      "        nan        nan 0.92790067        nan 0.92793088 0.92823803\n",
      " 0.92823299 0.92822292 0.92823299 0.92823299        nan        nan\n",
      " 0.92790067        nan 0.92793088 0.92823803 0.92823299 0.92822292\n",
      " 0.92823299 0.92823299        nan        nan 0.92790067        nan\n",
      " 0.92792584 0.92823803 0.92823299 0.92822292 0.92823299 0.92823299\n",
      "        nan        nan 0.92824306        nan 0.9282481  0.92829845\n",
      " 0.92828838 0.92825817 0.92829341 0.92828838        nan        nan\n",
      " 0.92824306        nan 0.92825313 0.92829845 0.92828838 0.92825817\n",
      " 0.92829845 0.92829845        nan        nan 0.92824306        nan\n",
      " 0.92824306 0.92829845 0.92828838 0.92825817 0.92829341 0.92829845\n",
      "        nan        nan 0.92824306        nan 0.92825313 0.92829845\n",
      " 0.92828838 0.92825817 0.92828838 0.92830349        nan        nan\n",
      " 0.92824306        nan 0.92825313 0.92829845 0.92828838 0.92825817\n",
      " 0.92829341 0.92829341        nan        nan 0.92824306        nan\n",
      " 0.92825313 0.92829845 0.92828838 0.92825817 0.92829845 0.92829341\n",
      "        nan        nan 0.92831356        nan 0.92831356 0.92831859\n",
      " 0.92829845 0.92831859 0.92831356 0.92831356        nan        nan\n",
      " 0.92831356        nan 0.92831859 0.92831859 0.92829845 0.92831859\n",
      " 0.92831356 0.92831356        nan        nan 0.92831356        nan\n",
      " 0.92831859 0.92831859 0.92829845 0.92831859 0.92831356 0.92831356\n",
      "        nan        nan 0.92831356        nan 0.92831859 0.92831859\n",
      " 0.92829845 0.92831859 0.92831356 0.92831356        nan        nan\n",
      " 0.92831356        nan 0.92832363 0.92831859 0.92829845 0.92831859\n",
      " 0.92831356 0.92831356        nan        nan 0.92831356        nan\n",
      " 0.92831356 0.92831859 0.92829845 0.92831859 0.92831356 0.92831356\n",
      "        nan        nan 0.92831356        nan 0.92831356 0.92831859\n",
      " 0.92829341 0.92831356 0.92831356 0.92831859        nan        nan\n",
      " 0.92831356        nan 0.92831356 0.92831859 0.92829341 0.92831356\n",
      " 0.92831356 0.92831356        nan        nan 0.92831356        nan\n",
      " 0.92831356 0.92831859 0.92829341 0.92831356 0.92831356 0.92831859\n",
      "        nan        nan 0.92831356        nan 0.92831356 0.92831859\n",
      " 0.92829341 0.92831356 0.92831356 0.92831356        nan        nan\n",
      " 0.92831356        nan 0.92831356 0.92831859 0.92829341 0.92831356\n",
      " 0.92831356 0.92831356        nan        nan 0.92831356        nan\n",
      " 0.92831859 0.92831859 0.92829341 0.92831356 0.92831356 0.92831859\n",
      "        nan        nan 0.92831356        nan 0.92831356 0.92831859\n",
      " 0.92829341 0.92831356 0.92831356 0.92831859        nan        nan\n",
      " 0.92831356        nan 0.92831356 0.92831859 0.92829341 0.92831356\n",
      " 0.92831356 0.92831356        nan        nan 0.92831356        nan\n",
      " 0.92831356 0.92831859 0.92829341 0.92831356 0.92831356 0.92831356\n",
      "        nan        nan 0.92831356        nan 0.92830852 0.92831859\n",
      " 0.92829341 0.92831356 0.92831859 0.92831356        nan        nan\n",
      " 0.92831356        nan 0.92831356 0.92831859 0.92829341 0.92831356\n",
      " 0.92831859 0.92831356        nan        nan 0.92831356        nan\n",
      " 0.92831356 0.92831859 0.92829341 0.92831356 0.92831356 0.92831356\n",
      "        nan        nan 0.92831356        nan 0.92831356 0.92831859\n",
      " 0.92829341 0.92831356 0.92831356 0.92831356        nan        nan\n",
      " 0.92831356        nan 0.92831356 0.92831859 0.92829341 0.92831356\n",
      " 0.92831356 0.92831356        nan        nan 0.92831356        nan\n",
      " 0.92831356 0.92831859 0.92829341 0.92831356 0.92831356 0.92831356\n",
      "        nan        nan 0.92831356        nan 0.92831356 0.92831859\n",
      " 0.92829341 0.92831356 0.92831356 0.92831356        nan        nan\n",
      " 0.92831356        nan 0.92831859 0.92831859 0.92829341 0.92831356\n",
      " 0.92831356 0.92831356        nan        nan 0.92831356        nan\n",
      " 0.92831356 0.92831859 0.92829341 0.92831356 0.92831356 0.92831356]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for fasttext: {'C': 1, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data_mode</th>\n",
       "      <th>C</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>cbow</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>fasttext</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model data_mode   C max_iter penalty solver\n",
       "0  Logistic Regression      cbow  10      100      l2  lbfgs\n",
       "1  Logistic Regression  skipgram   1      100      l2  lbfgs\n",
       "2  Logistic Regression  fasttext   1    10000      l1   saga"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data_model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.925339</td>\n",
       "      <td>0.917041</td>\n",
       "      <td>0.916936</td>\n",
       "      <td>0.925339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>0.932408</td>\n",
       "      <td>0.926140</td>\n",
       "      <td>0.925933</td>\n",
       "      <td>0.932408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>fasttext</td>\n",
       "      <td>0.931723</td>\n",
       "      <td>0.925284</td>\n",
       "      <td>0.925165</td>\n",
       "      <td>0.931723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.932683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model data_model  accuracy  precision  f1-score       tpr  \\\n",
       "0  Logistic Regression       cbow  0.925339   0.917041  0.916936  0.925339   \n",
       "1  Logistic Regression   skipgram  0.932408   0.926140  0.925933  0.932408   \n",
       "2  Logistic Regression   fasttext  0.931723   0.925284  0.925165  0.931723   \n",
       "\n",
       "   fpr       auc  \n",
       "0  0.0  0.922340  \n",
       "1  0.0  0.933002  \n",
       "2  0.0  0.932683  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage:\n",
    "param_grids = {\n",
    "    'cbow': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'max_iter': [100, 500, 1000, 5000, 10000, 50000],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    },\n",
    "    'skipgram': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'max_iter': [100, 500, 1000, 5000, 10000, 50000],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    },\n",
    "    'fasttext': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'max_iter': [100, 500, 1000, 5000, 10000, 50000],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "}\n",
    "best_parameters_df, results_df = perform_logistic_regression(param_grids)\n",
    "display(best_parameters_df)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took 25 mins to get the best parameters.\n",
    "```\n",
    "param_space = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [100, 500, 1000, 5000, 10000, 50000],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "```\n",
    "The best params are: \n",
    "```\n",
    "{'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "```\n",
    "We got the evaluation metrics.\n",
    "```\n",
    "    Accuracy: 0.9265674407363397\n",
    "    Precision: 0.9186826145148944\n",
    "    Recall: 0.9265674407363397\n",
    "    F1 Score: 0.9187615596666782\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "param_grids = {\n",
    "    'cbow': {\n",
    "        'C': np.logspace(-1, 1, 10),\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': np.random.randint(50, 250, size=10),\n",
    "        'solver': ['lbfgs']\n",
    "    },\n",
    "    'skipgram': {\n",
    "        'C': np.logspace(0, 2, 10),\n",
    "        'penalty': ['l1'],\n",
    "        'max_iter': np.random.randint(250, 750, size=10),\n",
    "        'solver': ['saga']\n",
    "    },\n",
    "    'fasttext': {\n",
    "        'C': np.logspace(-1, 1, 10),\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': np.random.randint(50, 250, size=10),\n",
    "        'solver': ['newton-cg']\n",
    "    }\n",
    "}\n",
    "best_parameters_df, results_df = perform_logistic_regression(param_grids)\n",
    "display(best_parameters_df)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We revise our param_grid to improve the best parameters further so that we can avoid overfitting and underfitting. We choose 'l2' for 'penalty' and 'lbfgs' for 'solver'. For 'C', we generate 10 numbers between 0.01 and 1. For 'max_iter', we generate random integers between 50 and 250.\n",
    "\n",
    "We got the evaluation metrics.\n",
    "```\n",
    "    Accuracy: 0.9265875813175968\n",
    "    Precision: 0.9187091269813016\n",
    "    Recall: 0.9265875813175968\n",
    "    F1 Score: 0.9187888187481864\n",
    "```\n",
    "The performance is good. The second results has minor improvement compared to the first one and the running time for getting the best parameters is much less than the first one. In this case, after tuning hyperparameters, we found that \n",
    "```\n",
    "    Best parameters: {'C': 0.1291549665014884, 'max_iter': 88, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
