{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment3\n",
    "Repository Link: [Github](https://github.com/awakn123/CS6120NLP/tree/main)\n",
    "\n",
    "Members: Yun Cao, Yue Liu, Nan Chen, Muyang Cheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing:\n",
    "1.1 Load the dataset and perform initial exploration to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin                 reviewerName helpful  \\\n",
      "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
      "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
      "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
      "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
      "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  This is a great tutu and at a really great pri...      5.0   \n",
      "1  I bought this for my 4 yr old daughter for dan...      5.0   \n",
      "2  What can I say... my daughters have it in oran...      5.0   \n",
      "3  We bought several tutus at once, and they are ...      5.0   \n",
      "4  Thank you Halo Heaven great product for Little...      5.0   \n",
      "\n",
      "                         summary  unixReviewTime   reviewTime  \n",
      "0  Great tutu-  not cheaply made      1297468800  02 12, 2011  \n",
      "1                    Very Cute!!      1358553600  01 19, 2013  \n",
      "2       I have buy more than one      1357257600   01 4, 2013  \n",
      "3               Adorable, Sturdy      1398556800  04 27, 2014  \n",
      "4        Grammy's Angels Love it      1394841600  03 15, 2014  \n"
     ]
    }
   ],
   "source": [
    "# data link: https://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open('./Clothing_Shoes_and_Jewelry_5.json'\n",
    "            , 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "#df = df.sample(1000)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Clean the text data, including removing special characters, stopwords, applying lowercasing, and other tasks as\n",
    "you deem necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nanchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nanchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pkg_resources\n",
    "import inflect\n",
    "import contractions\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p = inflect.engine()\n",
    "\n",
    "def standardize_numbers(text):\n",
    "    return ' '.join([p.number_to_words(word) if word.isdigit() else word for word in text.split()])\n",
    "\n",
    "def handle_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # standardize\n",
    "    text = standardize_numbers(text)\n",
    "    # handle contractions\n",
    "    text = handle_contractions(text)\n",
    "    text = text.strip()\n",
    "    # correct typos\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)\n",
    "        corrected_words.append(suggestions[0].term if suggestions else word)\n",
    "    text = ' '.join(corrected_words)\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # remove stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # rejoin words\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "df[\"reviewText2\"] = df[\"reviewText\"].apply(lambda x: clean_text(x))\n",
    "df[\"summary2\"] = df[\"summary\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word2Vec, fasttext embeddings\n",
    "2.1 Create 100D vectors using both Word2Vec (CBOW and SkipGram separately), and fasttext algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [review.split() for review in df[\"reviewText2\"]]\n",
    "\n",
    "# Word2Vec CBOW Model\n",
    "cbow_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "#cbow_model.save(\"cbow_word2vec.model\")\n",
    "\n",
    "# Word2Vec Skip-gram Model\n",
    "skipgram_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "#skipgram_model.save(\"skipgram_word2vec.model\")\n",
    "\n",
    "# FastText Model\n",
    "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "#fasttext_model.save(\"fasttext.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Average the vectors to create new average vector columns in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cbow_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cbow_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cbow_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin                 reviewerName helpful  \\\n",
      "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
      "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
      "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
      "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
      "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  This is a great tutu and at a really great pri...      5.0   \n",
      "1  I bought this for my 4 yr old daughter for dan...      5.0   \n",
      "2  What can I say... my daughters have it in oran...      5.0   \n",
      "3  We bought several tutus at once, and they are ...      5.0   \n",
      "4  Thank you Halo Heaven great product for Little...      5.0   \n",
      "\n",
      "                         summary  unixReviewTime   reviewTime  \\\n",
      "0  Great tutu-  not cheaply made      1297468800  02 12, 2011   \n",
      "1                    Very Cute!!      1358553600  01 19, 2013   \n",
      "2       I have buy more than one      1357257600   01 4, 2013   \n",
      "3               Adorable, Sturdy      1398556800  04 27, 2014   \n",
      "4        Grammy's Angels Love it      1394841600  03 15, 2014   \n",
      "\n",
      "                                         reviewText2  ... w2v_emb_90  \\\n",
      "0  great tutu really great price look cheap glad ...  ...  -0.014552   \n",
      "1  bought four old daughter dance class wore toda...  ...  -0.216611   \n",
      "2  say daughter orange black white pink thinking ...  ...  -0.318469   \n",
      "3  bought several tutu got high review sturdy see...  ...  -0.501420   \n",
      "4  thank halo heaven great product little girl gr...  ...  -0.129155   \n",
      "\n",
      "   w2v_emb_91  w2v_emb_92  w2v_emb_93  w2v_emb_94  w2v_emb_95  w2v_emb_96  \\\n",
      "0    0.312498   -0.855269    0.410758   -0.400386   -0.632724   -0.765946   \n",
      "1   -0.172295    0.139556   -0.470434    0.119795   -0.023674   -0.380279   \n",
      "2    0.153304   -0.101640   -0.456703   -0.215727    0.059210   -0.433959   \n",
      "3   -0.277605    0.510712   -0.727998   -0.222080    0.324138   -0.662878   \n",
      "4    0.101879   -0.120557   -0.102506    0.031794   -0.297982   -0.172094   \n",
      "\n",
      "   w2v_emb_97  w2v_emb_98  w2v_emb_99  \n",
      "0    0.695324    0.142276   -0.798558  \n",
      "1    0.473972    0.220220    0.026089  \n",
      "2    0.559716    0.183868    0.147502  \n",
      "3    0.244971   -0.102717    0.113174  \n",
      "4    0.729817    0.287275   -0.224222  \n",
      "\n",
      "[5 rows x 111 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_skipgram_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_skipgram_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_skipgram_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin                 reviewerName helpful  \\\n",
      "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
      "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
      "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
      "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
      "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  This is a great tutu and at a really great pri...      5.0   \n",
      "1  I bought this for my 4 yr old daughter for dan...      5.0   \n",
      "2  What can I say... my daughters have it in oran...      5.0   \n",
      "3  We bought several tutus at once, and they are ...      5.0   \n",
      "4  Thank you Halo Heaven great product for Little...      5.0   \n",
      "\n",
      "                         summary  unixReviewTime   reviewTime  \\\n",
      "0  Great tutu-  not cheaply made      1297468800  02 12, 2011   \n",
      "1                    Very Cute!!      1358553600  01 19, 2013   \n",
      "2       I have buy more than one      1357257600   01 4, 2013   \n",
      "3               Adorable, Sturdy      1398556800  04 27, 2014   \n",
      "4        Grammy's Angels Love it      1394841600  03 15, 2014   \n",
      "\n",
      "                                         reviewText2  ... w2v_emb_90  \\\n",
      "0  great tutu really great price look cheap glad ...  ...   0.325734   \n",
      "1  bought four old daughter dance class wore toda...  ...   0.303797   \n",
      "2  say daughter orange black white pink thinking ...  ...   0.296935   \n",
      "3  bought several tutu got high review sturdy see...  ...   0.158560   \n",
      "4  thank halo heaven great product little girl gr...  ...   0.299949   \n",
      "\n",
      "   w2v_emb_91  w2v_emb_92  w2v_emb_93  w2v_emb_94  w2v_emb_95  w2v_emb_96  \\\n",
      "0    0.164042   -0.053214   -0.086103    0.448459    0.139131    0.102726   \n",
      "1    0.099599    0.093896    0.005502    0.438368    0.207979    0.261265   \n",
      "2    0.095652    0.014586    0.002063    0.407663    0.210150    0.270139   \n",
      "3    0.062751    0.086373   -0.094241    0.463673    0.271966    0.146340   \n",
      "4    0.100352    0.069193    0.031635    0.449455    0.115501    0.228504   \n",
      "\n",
      "   w2v_emb_97  w2v_emb_98  w2v_emb_99  \n",
      "0   -0.319645    0.039604   -0.182529  \n",
      "1   -0.206600    0.129492   -0.100954  \n",
      "2   -0.311487    0.202696   -0.039695  \n",
      "3   -0.235753    0.174454   -0.049649  \n",
      "4   -0.282286    0.070159   -0.112565  \n",
      "\n",
      "[5 rows x 111 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_fasttext_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_fasttext_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin                 reviewerName helpful  \\\n",
      "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
      "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
      "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
      "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
      "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  This is a great tutu and at a really great pri...      5.0   \n",
      "1  I bought this for my 4 yr old daughter for dan...      5.0   \n",
      "2  What can I say... my daughters have it in oran...      5.0   \n",
      "3  We bought several tutus at once, and they are ...      5.0   \n",
      "4  Thank you Halo Heaven great product for Little...      5.0   \n",
      "\n",
      "                         summary  unixReviewTime   reviewTime  \\\n",
      "0  Great tutu-  not cheaply made      1297468800  02 12, 2011   \n",
      "1                    Very Cute!!      1358553600  01 19, 2013   \n",
      "2       I have buy more than one      1357257600   01 4, 2013   \n",
      "3               Adorable, Sturdy      1398556800  04 27, 2014   \n",
      "4        Grammy's Angels Love it      1394841600  03 15, 2014   \n",
      "\n",
      "                                         reviewText2  ... w2v_emb_90  \\\n",
      "0  great tutu really great price look cheap glad ...  ...  -0.305562   \n",
      "1  bought four old daughter dance class wore toda...  ...  -0.474884   \n",
      "2  say daughter orange black white pink thinking ...  ...  -0.443228   \n",
      "3  bought several tutu got high review sturdy see...  ...  -0.467076   \n",
      "4  thank halo heaven great product little girl gr...  ...  -0.484766   \n",
      "\n",
      "   w2v_emb_91  w2v_emb_92  w2v_emb_93  w2v_emb_94  w2v_emb_95  w2v_emb_96  \\\n",
      "0   -0.162145   -0.000468    0.386935    0.061058    0.300372   -0.007219   \n",
      "1   -0.113277   -0.026269    0.444483   -0.104499    0.183902    0.132847   \n",
      "2   -0.099321   -0.070701    0.369236    0.000227    0.237012    0.176269   \n",
      "3   -0.105386    0.055193    0.359684    0.012770    0.203911    0.066095   \n",
      "4   -0.050681   -0.078434    0.352563    0.000108    0.231345    0.103420   \n",
      "\n",
      "   w2v_emb_97  w2v_emb_98  w2v_emb_99  \n",
      "0    0.068563    0.067589    0.264124  \n",
      "1    0.135097    0.018965    0.360308  \n",
      "2    0.088768    0.005930    0.364231  \n",
      "3    0.125693    0.055624    0.389913  \n",
      "4    0.010868   -0.013925    0.338627  \n",
      "\n",
      "[5 rows x 111 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/112724244.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_fasttext_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_average_word2vec_embeddings(review, model):\n",
    "    words = review.split()\n",
    "    embeddings = [model.wv[word] for word in words if word in model.wv.key_to_index]\n",
    "    if len(embeddings) == 0:\n",
    "        return [0]*model.vector_size\n",
    "    return list(np.mean(embeddings, axis=0))\n",
    "\n",
    "df_cbow_model = df.copy()\n",
    "embeddings = df_cbow_model[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, cbow_model))\n",
    "for i in range(cbow_model.vector_size):\n",
    "    df_cbow_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i]) \n",
    "print(df_cbow_model.head())\n",
    "\n",
    "df_skipgram_model = df.copy()\n",
    "embeddings = df_skipgram_model[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, skipgram_model))\n",
    "for i in range(skipgram_model.vector_size):\n",
    "    df_skipgram_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
    "print(df_skipgram_model.head())\n",
    "\n",
    "df_fasttext_model = df.copy()\n",
    "embeddings = df_fasttext_model[\"reviewText2\"].apply(lambda x: get_average_word2vec_embeddings(x, fasttext_model))\n",
    "for i in range(fasttext_model.vector_size):\n",
    "    df_fasttext_model[f\"w2v_emb_{i}\"] = embeddings.apply(lambda x: x[i])\n",
    "print(df_fasttext_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Perform EDA to analyze associations between vectors from the three methods above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 common words in three models\n",
    "words = list(cbow_model.wv.key_to_index.keys())[:100]\n",
    "\n",
    "# Extract the vectors for these common words from each model\n",
    "cbow_vectors = np.array([cbow_model.wv[word] for word in words])\n",
    "skipgram_vectors = np.array([skipgram_model.wv[word] for word in words])\n",
    "fasttext_vectors = np.array([fasttext_model.wv[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with t-SNE\n",
    "#Use t-SNE to reduce the dimensionality of your word vectors to two or three dimensions.\n",
    "#Plot the results to see how words cluster together.\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_word_vectors(model_vectors, words, title='Word Vectors Visualization'):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    vectors_tsne = tsne.fit_transform(model_vectors)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, word in enumerate(words):\n",
    "        plt.scatter(vectors_tsne[i, 0], vectors_tsne[i, 1])\n",
    "        plt.annotate(word, (vectors_tsne[i, 0], vectors_tsne[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.title(title)\n",
    "\n",
    "visualize_word_vectors(cbow_vectors, words, 'CBOW t-SNE Visualization')\n",
    "visualize_word_vectors(skipgram_vectors, words, 'Skip-gram t-SNE Visualization')\n",
    "visualize_word_vectors(fasttext_vectors, words, 'FastText t-SNE Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with KMeans\n",
    "# Apply clustering algorithms like K-Means on the word vectors.\n",
    "# Analyze the clusters to see which words are grouped together by the model.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def cluster_and_plot_words(model_vectors, common_words, n_clusters=10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(model_vectors)\n",
    "    cluster_map = {i: [] for i in range(n_clusters)}\n",
    "    for word, label in zip(common_words, labels):\n",
    "        cluster_map[label].append(word)\n",
    "\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]\n",
    "        cluster_points = model_vectors[cluster_indices]\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=colors[cluster_id], label=f'Cluster {cluster_id}', alpha=0.5)\n",
    "    \n",
    "    for i, word in enumerate(common_words):\n",
    "        plt.text(model_vectors[i, 0], model_vectors[i, 1], word, fontsize=9)\n",
    "    \n",
    "    plt.title('Word Vectors Clustering')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return cluster_map\n",
    "\n",
    "cbow_clusters = cluster_and_plot_words(cbow_vectors, words, n_clusters=10)\n",
    "skipgram_clusters = cluster_and_plot_words(skipgram_vectors, words, n_clusters=10)\n",
    "fasttext_clusters = cluster_and_plot_words(fasttext_vectors, words, n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Analysis\n",
    "# Compute the cosine similarity between word vectors\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def compare_similarity(word1, word2, model1, model2, model3, model_name1='CBOW', model_name2='Skip-gram', model_name3='FastText'):\n",
    "    similarity1 = 1 - cosine(model1.wv[word1], model1.wv[word2])\n",
    "    similarity2 = 1 - cosine(model2.wv[word1], model2.wv[word2])\n",
    "    similarity3 = 1 - cosine(model3.wv[word1], model3.wv[word2])\n",
    "    print(f'{word1} & {word2} similarity in {model_name1}: {similarity1:.4f}')\n",
    "    print(f'{word1} & {word2} similarity in {model_name2}: {similarity2:.4f}')\n",
    "    print(f'{word1} & {word2} similarity in {model_name3}: {similarity3:.4f}')\n",
    "\n",
    "compare_similarity('shirt', 'hoodie', cbow_model, skipgram_model, fasttext_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1889851742.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Convert the ratings to 0 for negative and 1 for positive\n",
    "def convert_rating_to_sentiment(overall):\n",
    "  if overall in [1, 2]:\n",
    "      return 0  # Negative sentiment\n",
    "  elif overall in [4, 5]:\n",
    "      return 1  # Positive sentiment\n",
    "  # Optionally handle unexpected cases, though all cases should be covered\n",
    "  return None\n",
    "\n",
    "def convertAndReduceDimension(df):\n",
    "\t\tdf_filtered = df[df['overall'] != 3]\n",
    "\t\tdf_filtered['sentiment'] = df_filtered['overall'].apply(convert_rating_to_sentiment)\n",
    "\t\tvector_cols = [col for col in df_filtered.columns if col.startswith('w2v_emb_')]\n",
    "\t\tX = df_filtered[vector_cols]\n",
    "\t\tscaler = StandardScaler()\n",
    "\t\tX_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\t\t# Step 2: Apply PCA to retain 90% of the variance\n",
    "\t\tpca = PCA(n_components=0.9)  # n_components set to 0.9 means PCA will select the minimum number of components that retain 90% of the variance\n",
    "\t\tX_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\t\t# Create a DataFrame with the PCA features\n",
    "\t\tdf_pca = pd.DataFrame(X_pca, columns=[f'PCA_{i+1}' for i in range(X_pca.shape[1])])\n",
    "\n",
    "\t\t# Optionally, if you want to include the sentiment back into the PCA-transformed dataset\n",
    "\t\tdf_pca['sentiment'] = df_filtered['sentiment'].values\n",
    "\n",
    "\t\treturn df_pca\n",
    "  \n",
    "df_cbow = convertAndReduceDimension(df_cbow_model)\n",
    "df_skipgram = convertAndReduceDimension(df_skipgram_model)\n",
    "df_fasttext = convertAndReduceDimension(df_fasttext_model)\n",
    "\n",
    "# Define your DataFrame to hold the results\n",
    "results_df = pd.DataFrame(columns=['model', 'data_model', 'accuracy', 'precision', 'f1-score', 'tpr', 'fpr', 'auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use GridSearchCV for hyperparameter tuning to find the best parameters for Logistic Regression model.\n",
    "\n",
    "In this code, GridSearchCV performs an exhaustive search over the specified parameter grid, \n",
    "and best_params_ gives us the best parameters found during the search. Then can then use \n",
    "these parameters to train a new classifier.\n",
    "\n",
    "For 'C', we choose the value in the list [0.001, 0.01, 0.1, 1, 10, 100, 1000]. For 'max_iter', we choose the value in the list [100, 500, 1000, 5000, 10000, 50000].  For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones. ‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas ‘liblinear’ and ‘saga’ handle L1 penalty.\n",
    "\n",
    "When choosing the values for these parameters in GridSearchCV object. For the value of cv, a common choice is 5 or 10, but considering we have a large dataset, a smaller number can be used to save computational time. So, we choose 5. For \"scoring\", considering we’re dealing with a classification problem, we use ‘accuracy’. For other parameters like 'n_jobs', 'verbose', 'return_train_score', we use the default values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "def train_with_logistic_regression(df, param_grid, data_type):\n",
    "    # Assuming df_final is your final DataFrame after PCA\n",
    "\tX = df.drop('sentiment', axis=1).values  # Features\n",
    "\ty = df['sentiment'].values  # Labels\n",
    "\n",
    "\t# Splitting the data into training and validation sets\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\t# Create a Logistic Regression model\n",
    "\tmodel = LogisticRegression()\n",
    "\n",
    "\t# Create a GridSearchCV object\n",
    "\tgrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "\t# Fit the GridSearchCV object to the data\n",
    "\tgrid_search.fit(X_train, y_train)\n",
    "\n",
    "\t# Get the best parameters\n",
    "\tbest_params = grid_search.best_params_\n",
    "\n",
    "\t# Print the best parameters\n",
    "\tprint(f\"Best parameters for {data_type}: {best_params}\")\n",
    "\n",
    "\t# Fit the model with the best parameters to the training data\n",
    "\tmodel_best = LogisticRegression(**best_params)\n",
    "\tmodel_best.fit(X_train, y_train)\n",
    "\n",
    "\t# Make predictions on the test set\n",
    "\ty_pred = model_best.predict(X_val)\n",
    "\n",
    "\t# Evaluate the model's performance\n",
    "\taccuracy = accuracy_score(y_val, y_pred)\n",
    "\tprecision = precision_score(y_val, y_pred, average='weighted', zero_division=0)  # Handling division by zero for classes with no predictions\n",
    "\trecall = recall_score(y_val, y_pred, average='weighted')\n",
    "\tf1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\t# Calculate the probability estimates of the positive class\n",
    "\ty_pred_proba = model_best.predict_proba(X_val)[:, 1]\n",
    "\n",
    "\t# Calculate the ROC AUC\n",
    "\tauc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "\t# Calculate the FPR, TPR, and thresholds\n",
    "\tfpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
    "\n",
    "\t# Append new results\n",
    "\tmetrics_dict = {\n",
    "\t    'model': 'Logistic Regression',\n",
    "\t    'data_model': data_type,\n",
    "\t    'accuracy': accuracy,\n",
    "\t    'precision': precision,\n",
    "\t    'f1-score': f1,\n",
    "\t    'tpr': recall,\n",
    "\t    'fpr': fpr[1],\n",
    "\t    'auc': auc\n",
    "\t}\n",
    "\treturn metrics_dict, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:542: FitFailedWarning: \n",
      "630 fits failed out of a total of 2100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.91606287        nan 0.91643044 0.92267411\n",
      " 0.92268922 0.92212023 0.92267915 0.92267915        nan        nan\n",
      " 0.91606287        nan 0.91643044 0.92267411 0.92268922 0.92212023\n",
      " 0.92267915 0.92267915        nan        nan 0.91606287        nan\n",
      " 0.91643044 0.92267411 0.92268922 0.92212023 0.92267915 0.92267411\n",
      "        nan        nan 0.91606287        nan 0.91643044 0.92267411\n",
      " 0.92268922 0.92212023 0.92267915 0.92267915        nan        nan\n",
      " 0.91606287        nan 0.91643044 0.92267411 0.92268922 0.92212023\n",
      " 0.92267915 0.92267915        nan        nan 0.91606287        nan\n",
      " 0.91643044 0.92267411 0.92268922 0.92212023 0.92267915 0.92267915\n",
      "        nan        nan 0.92333372        nan 0.92335386 0.92380703\n",
      " 0.92382718 0.92378186 0.92379193 0.92379193        nan        nan\n",
      " 0.92332869        nan 0.92335386 0.92380703 0.92382718 0.92378186\n",
      " 0.92379193 0.92379193        nan        nan 0.92333372        nan\n",
      " 0.92335386 0.92380703 0.92382718 0.92378186 0.92379193 0.92379193\n",
      "        nan        nan 0.92332365        nan 0.92335386 0.92380703\n",
      " 0.92382718 0.92378186 0.92379193 0.92379193        nan        nan\n",
      " 0.92332869        nan 0.92335386 0.92380703 0.92382718 0.92378186\n",
      " 0.92379193 0.92379193        nan        nan 0.92332365        nan\n",
      " 0.92335386 0.92380703 0.92382718 0.92378186 0.92379193 0.92379193\n",
      "        nan        nan 0.923802          nan 0.92380703 0.92388256\n",
      " 0.92386242 0.92385235 0.92387249 0.92387249        nan        nan\n",
      " 0.92379696        nan 0.92379696 0.92388256 0.92386242 0.92385235\n",
      " 0.92387249 0.92387249        nan        nan 0.923802          nan\n",
      " 0.92380703 0.92388256 0.92386242 0.92385235 0.92386746 0.92387249\n",
      "        nan        nan 0.92379696        nan 0.923802   0.92388256\n",
      " 0.92386242 0.92385235 0.92387249 0.92387249        nan        nan\n",
      " 0.923802          nan 0.923802   0.92388256 0.92386242 0.92385235\n",
      " 0.92387249 0.92387249        nan        nan 0.923802          nan\n",
      " 0.923802   0.92388256 0.92386242 0.92385235 0.92387249 0.92387249\n",
      "        nan        nan 0.92385235        nan 0.92385739 0.92388256\n",
      " 0.92389767 0.92386746 0.9238876  0.9238876         nan        nan\n",
      " 0.92385235        nan 0.92385739 0.92388256 0.92389767 0.92386746\n",
      " 0.92388256 0.9238876         nan        nan 0.92385235        nan\n",
      " 0.92385739 0.92388256 0.92389767 0.92386746 0.9238876  0.92388256\n",
      "        nan        nan 0.92385235        nan 0.92385739 0.92388256\n",
      " 0.92389767 0.92386746 0.92387753 0.92388256        nan        nan\n",
      " 0.92385235        nan 0.92385739 0.92388256 0.92389767 0.92386746\n",
      " 0.92388256 0.92388256        nan        nan 0.92385235        nan\n",
      " 0.92385739 0.92388256 0.92389767 0.92386746 0.9238876  0.92388256\n",
      "        nan        nan 0.92388256        nan 0.9238876  0.9238876\n",
      " 0.92389767 0.92388256 0.92388256 0.9238876         nan        nan\n",
      " 0.92389263        nan 0.92388256 0.9238876  0.92389767 0.92388256\n",
      " 0.92388256 0.9238876         nan        nan 0.9238876         nan\n",
      " 0.92388256 0.9238876  0.92389767 0.92388256 0.9238876  0.92387753\n",
      "        nan        nan 0.9238876         nan 0.92389263 0.9238876\n",
      " 0.92389767 0.92388256 0.9238876  0.92388256        nan        nan\n",
      " 0.92388256        nan 0.9238876  0.9238876  0.92389767 0.92388256\n",
      " 0.92389263 0.9238876         nan        nan 0.92388256        nan\n",
      " 0.92389263 0.9238876  0.92389767 0.92388256 0.9238876  0.9238876\n",
      "        nan        nan 0.92388256        nan 0.92388256 0.92389263\n",
      " 0.92389767 0.92388256 0.9238876  0.92388256        nan        nan\n",
      " 0.9238876         nan 0.92388256 0.92389263 0.92389767 0.92388256\n",
      " 0.92387753 0.9238876         nan        nan 0.92388256        nan\n",
      " 0.9238876  0.92389263 0.92389767 0.92388256 0.92388256 0.9238876\n",
      "        nan        nan 0.92388256        nan 0.92387753 0.92389263\n",
      " 0.92389767 0.92388256 0.9238876  0.92388256        nan        nan\n",
      " 0.9238876         nan 0.92388256 0.92389263 0.92389767 0.92388256\n",
      " 0.92387753 0.9238876         nan        nan 0.92388256        nan\n",
      " 0.92388256 0.92389263 0.92389767 0.92388256 0.9238876  0.9238876\n",
      "        nan        nan 0.9238876         nan 0.92388256 0.92389263\n",
      " 0.92389767 0.92388256 0.92389263 0.92389263        nan        nan\n",
      " 0.9238876         nan 0.9238876  0.92389263 0.92389767 0.92388256\n",
      " 0.92388256 0.92388256        nan        nan 0.9238876         nan\n",
      " 0.92387753 0.92389263 0.92389767 0.92388256 0.9238876  0.9238876\n",
      "        nan        nan 0.9238876         nan 0.9238876  0.92389263\n",
      " 0.92389767 0.92388256 0.9238876  0.9238876         nan        nan\n",
      " 0.9238876         nan 0.92388256 0.92389263 0.92389767 0.92388256\n",
      " 0.92388256 0.92388256        nan        nan 0.92387753        nan\n",
      " 0.9238876  0.92389263 0.92389767 0.92388256 0.9238876  0.92389263]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for cbow: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wj2mw1_s3qv1nywqkp54rk080000gn/T/ipykernel_62698/1611430407.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = results_df._append(cbow_data, ignore_index=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:542: FitFailedWarning: \n",
      "630 fits failed out of a total of 2100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.92109808        nan 0.92124914 0.92681306\n",
      " 0.92680802 0.92646563 0.92681809 0.92681306        nan        nan\n",
      " 0.92109808        nan 0.92124411 0.92681306 0.92680802 0.92646563\n",
      " 0.92681809 0.92681809        nan        nan 0.92109305        nan\n",
      " 0.92124914 0.92681306 0.92680802 0.92646563 0.92681809 0.92681809\n",
      "        nan        nan 0.92109808        nan 0.92125418 0.92681306\n",
      " 0.92680802 0.92646563 0.92682313 0.92681306        nan        nan\n",
      " 0.92109808        nan 0.92125921 0.92681306 0.92680802 0.92646563\n",
      " 0.92681809 0.92681809        nan        nan 0.92109808        nan\n",
      " 0.92124411 0.92681306 0.92680802 0.92646563 0.92681809 0.92681809\n",
      "        nan        nan 0.92749281        nan 0.9275482  0.92798123\n",
      " 0.92799634 0.92786039 0.9279762  0.92798123        nan        nan\n",
      " 0.92749281        nan 0.9275482  0.92798123 0.92799634 0.92786039\n",
      " 0.9279762  0.9279762         nan        nan 0.92749785        nan\n",
      " 0.9275482  0.92798123 0.92799634 0.92786039 0.9279762  0.9279762\n",
      "        nan        nan 0.92749281        nan 0.9275482  0.92798123\n",
      " 0.92799634 0.92786039 0.9279762  0.9279762         nan        nan\n",
      " 0.92749785        nan 0.9275482  0.92798123 0.92799634 0.92786039\n",
      " 0.9279762  0.9279762         nan        nan 0.92749785        nan\n",
      " 0.9275482  0.92798123 0.92799634 0.92786039 0.9279762  0.9279762\n",
      "        nan        nan 0.92803662        nan 0.92804165 0.92809201\n",
      " 0.92806179 0.92810208 0.92809704 0.92809704        nan        nan\n",
      " 0.92803662        nan 0.92804165 0.92809201 0.92806179 0.92810208\n",
      " 0.92809704 0.92809704        nan        nan 0.92803662        nan\n",
      " 0.92803662 0.92809201 0.92806179 0.92810208 0.92809704 0.92809704\n",
      "        nan        nan 0.92803662        nan 0.92803662 0.92809201\n",
      " 0.92806179 0.92810208 0.92810208 0.92809704        nan        nan\n",
      " 0.92803662        nan 0.92803662 0.92809201 0.92806179 0.92810208\n",
      " 0.92810208 0.92809704        nan        nan 0.92803662        nan\n",
      " 0.92803662 0.92809201 0.92806179 0.92810208 0.92809704 0.92810208\n",
      "        nan        nan 0.92810208        nan 0.92810208 0.92810208\n",
      " 0.92808697 0.92809704 0.92810711 0.92810711        nan        nan\n",
      " 0.92810208        nan 0.92810208 0.92810208 0.92808697 0.92809704\n",
      " 0.92810711 0.92811215        nan        nan 0.92810208        nan\n",
      " 0.92810711 0.92810208 0.92808697 0.92809704 0.92811215 0.92811215\n",
      "        nan        nan 0.92810208        nan 0.92810208 0.92810208\n",
      " 0.92808697 0.92809704 0.92810711 0.92811215        nan        nan\n",
      " 0.92810208        nan 0.92810208 0.92810208 0.92808697 0.92809704\n",
      " 0.92810711 0.92810711        nan        nan 0.92810208        nan\n",
      " 0.92810208 0.92810208 0.92808697 0.92809704 0.92811215 0.92810711\n",
      "        nan        nan 0.92811215        nan 0.92811718 0.92809704\n",
      " 0.9280769  0.92810711 0.92810711 0.92810711        nan        nan\n",
      " 0.92811215        nan 0.92811215 0.92809704 0.9280769  0.92810711\n",
      " 0.92810711 0.92811215        nan        nan 0.92811215        nan\n",
      " 0.92811215 0.92809704 0.9280769  0.92810711 0.92811215 0.92811215\n",
      "        nan        nan 0.92811215        nan 0.92811215 0.92809704\n",
      " 0.9280769  0.92810711 0.92811215 0.92811215        nan        nan\n",
      " 0.92811215        nan 0.92811718 0.92809704 0.9280769  0.92810711\n",
      " 0.92811215 0.92811215        nan        nan 0.92811215        nan\n",
      " 0.92811215 0.92809704 0.9280769  0.92810711 0.92810711 0.92810711\n",
      "        nan        nan 0.92810711        nan 0.92811215 0.92809704\n",
      " 0.92807187 0.92810711 0.92810711 0.92811215        nan        nan\n",
      " 0.92810711        nan 0.92811215 0.92809704 0.92807187 0.92810711\n",
      " 0.92811215 0.92810711        nan        nan 0.92811215        nan\n",
      " 0.92810711 0.92809704 0.92807187 0.92810711 0.92811215 0.92811215\n",
      "        nan        nan 0.92810711        nan 0.92810711 0.92809704\n",
      " 0.92807187 0.92810711 0.92811215 0.92811718        nan        nan\n",
      " 0.92810711        nan 0.92810711 0.92809704 0.92807187 0.92810711\n",
      " 0.92810711 0.92811215        nan        nan 0.92810711        nan\n",
      " 0.92811215 0.92809704 0.92807187 0.92810711 0.92810711 0.92811215\n",
      "        nan        nan 0.92810711        nan 0.92811215 0.92809704\n",
      " 0.92807187 0.92810711 0.92811215 0.92810711        nan        nan\n",
      " 0.92810711        nan 0.92811215 0.92809704 0.92807187 0.92810711\n",
      " 0.92810711 0.92810711        nan        nan 0.92810711        nan\n",
      " 0.92810711 0.92809704 0.92807187 0.92810711 0.92811215 0.92810711\n",
      "        nan        nan 0.92810711        nan 0.92810711 0.92809704\n",
      " 0.92807187 0.92810711 0.92810711 0.92810711        nan        nan\n",
      " 0.92810711        nan 0.92810711 0.92809704 0.92807187 0.92810711\n",
      " 0.92810711 0.92810711        nan        nan 0.92810711        nan\n",
      " 0.92810711 0.92809704 0.92807187 0.92810711 0.92811215 0.92811718]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for skipgram: {'C': 10, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:542: FitFailedWarning: \n",
      "630 fits failed out of a total of 2100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.91894301        nan 0.91914945 0.92688355\n",
      " 0.92692383 0.92627933 0.92688355 0.92688355        nan        nan\n",
      " 0.91894301        nan 0.91914442 0.92688355 0.92692383 0.92627933\n",
      " 0.92688355 0.92688355        nan        nan 0.91894301        nan\n",
      " 0.91914945 0.92688355 0.92692383 0.92627933 0.92688355 0.92688355\n",
      "        nan        nan 0.91894301        nan 0.91914945 0.92688355\n",
      " 0.92692383 0.92627933 0.92688355 0.92688355        nan        nan\n",
      " 0.91894301        nan 0.91914945 0.92688355 0.92692383 0.92627933\n",
      " 0.92688355 0.92688859        nan        nan 0.91894301        nan\n",
      " 0.91914945 0.92688355 0.92692383 0.92627933 0.92688355 0.92687852\n",
      "        nan        nan 0.92740218        nan 0.92741225 0.92804165\n",
      " 0.92804669 0.92799634 0.92806179 0.92805676        nan        nan\n",
      " 0.92740218        nan 0.92742232 0.92804165 0.92804669 0.92799634\n",
      " 0.92806179 0.92806179        nan        nan 0.92740218        nan\n",
      " 0.92741729 0.92804165 0.92804669 0.92799634 0.92806179 0.92805676\n",
      "        nan        nan 0.92740722        nan 0.92742232 0.92804165\n",
      " 0.92804669 0.92799634 0.92806179 0.92806179        nan        nan\n",
      " 0.92740218        nan 0.92741729 0.92804165 0.92804669 0.92799634\n",
      " 0.92806179 0.92806179        nan        nan 0.92740218        nan\n",
      " 0.92742232 0.92804165 0.92804669 0.92799634 0.92806179 0.92806179\n",
      "        nan        nan 0.92810711        nan 0.92811215 0.92814236\n",
      " 0.92813229 0.92809201 0.92813732 0.92813732        nan        nan\n",
      " 0.92811215        nan 0.92811215 0.92814236 0.92813229 0.92809201\n",
      " 0.92813732 0.92814236        nan        nan 0.92810208        nan\n",
      " 0.92811215 0.92814236 0.92813229 0.92809201 0.92813732 0.92813732\n",
      "        nan        nan 0.92810711        nan 0.92811215 0.92814236\n",
      " 0.92813229 0.92809201 0.92814236 0.92813732        nan        nan\n",
      " 0.92810208        nan 0.92810208 0.92814236 0.92813229 0.92809201\n",
      " 0.92813732 0.92813732        nan        nan 0.92810711        nan\n",
      " 0.92811215 0.92814236 0.92813229 0.92809201 0.92814236 0.92813732\n",
      "        nan        nan 0.92813229        nan 0.92813732 0.92815746\n",
      " 0.92812222 0.92811718 0.92812725 0.92812222        nan        nan\n",
      " 0.92813229        nan 0.92812725 0.92815746 0.92812222 0.92811718\n",
      " 0.92812222 0.92812222        nan        nan 0.92813229        nan\n",
      " 0.92813229 0.92815746 0.92812222 0.92811718 0.92812725 0.92812725\n",
      "        nan        nan 0.92813229        nan 0.92813229 0.92815746\n",
      " 0.92812222 0.92811718 0.92812725 0.92812222        nan        nan\n",
      " 0.92813229        nan 0.92813229 0.92815746 0.92812222 0.92811718\n",
      " 0.92813229 0.92812222        nan        nan 0.92813229        nan\n",
      " 0.92813229 0.92815746 0.92812222 0.92811718 0.92812725 0.92812725\n",
      "        nan        nan 0.92812725        nan 0.92812222 0.92815243\n",
      " 0.92812222 0.92813229 0.92813229 0.92812725        nan        nan\n",
      " 0.92812725        nan 0.92812222 0.92815243 0.92812222 0.92813229\n",
      " 0.92812725 0.92813229        nan        nan 0.92812725        nan\n",
      " 0.92812725 0.92815243 0.92812222 0.92813229 0.92812725 0.92812725\n",
      "        nan        nan 0.92812725        nan 0.92812725 0.92815243\n",
      " 0.92812222 0.92813229 0.92812725 0.92812725        nan        nan\n",
      " 0.92813229        nan 0.92812725 0.92815243 0.92812222 0.92813229\n",
      " 0.92812725 0.92813229        nan        nan 0.92812725        nan\n",
      " 0.92812725 0.92815243 0.92812222 0.92813229 0.92813229 0.92812725\n",
      "        nan        nan 0.92812725        nan 0.92812725 0.92814236\n",
      " 0.92812222 0.92812725 0.92812725 0.92812725        nan        nan\n",
      " 0.92813229        nan 0.92812725 0.92814236 0.92812222 0.92812725\n",
      " 0.92812725 0.92812725        nan        nan 0.92813229        nan\n",
      " 0.92812725 0.92814236 0.92812222 0.92812725 0.92812725 0.92813229\n",
      "        nan        nan 0.92812725        nan 0.92813229 0.92814236\n",
      " 0.92812222 0.92812725 0.92812725 0.92812725        nan        nan\n",
      " 0.92813229        nan 0.92813229 0.92814236 0.92812222 0.92812725\n",
      " 0.92812725 0.92813229        nan        nan 0.92813229        nan\n",
      " 0.92812725 0.92814236 0.92812222 0.92812725 0.92813229 0.92812725\n",
      "        nan        nan 0.92813229        nan 0.92813229 0.92814236\n",
      " 0.92812222 0.92812725 0.92813229 0.92812725        nan        nan\n",
      " 0.92813229        nan 0.92813229 0.92814236 0.92812222 0.92812725\n",
      " 0.92812725 0.92813229        nan        nan 0.92812725        nan\n",
      " 0.92812725 0.92814236 0.92812222 0.92812725 0.92812725 0.92813229\n",
      "        nan        nan 0.92812725        nan 0.92812725 0.92814236\n",
      " 0.92812222 0.92812725 0.92812725 0.92813229        nan        nan\n",
      " 0.92812725        nan 0.92812725 0.92814236 0.92812222 0.92812725\n",
      " 0.92813229 0.92812725        nan        nan 0.92812725        nan\n",
      " 0.92812725 0.92814236 0.92812222 0.92812725 0.92813229 0.92813229]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for fasttext: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data_model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>cbow</td>\n",
       "      <td>0.927736</td>\n",
       "      <td>0.920186</td>\n",
       "      <td>0.920083</td>\n",
       "      <td>0.927736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>0.932005</td>\n",
       "      <td>0.925635</td>\n",
       "      <td>0.925465</td>\n",
       "      <td>0.932005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.931820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>fasttext</td>\n",
       "      <td>0.931482</td>\n",
       "      <td>0.924977</td>\n",
       "      <td>0.924837</td>\n",
       "      <td>0.931482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.931695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model data_model  accuracy  precision  f1-score       tpr  \\\n",
       "0  Logistic Regression       cbow  0.927736   0.920186  0.920083  0.927736   \n",
       "1  Logistic Regression   skipgram  0.932005   0.925635  0.925465  0.932005   \n",
       "2  Logistic Regression   fasttext  0.931482   0.924977  0.924837  0.931482   \n",
       "\n",
       "   fpr       auc  \n",
       "0  0.0  0.925999  \n",
       "1  0.0  0.931820  \n",
       "2  0.0  0.931695  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [100, 500, 1000, 5000, 10000, 50000],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(columns=['model', 'data_model', 'accuracy', 'precision', 'f1-score', 'tpr', 'fpr', 'auc'])\n",
    "cbow_data, logistic_regression_cbow_model = train_with_logistic_regression(df_cbow, param_grid, 'cbow')\n",
    "results_df = results_df._append(cbow_data, ignore_index=True)\n",
    "skipgram_data, logistic_regression_skipgram_model = train_with_logistic_regression(df_skipgram, param_grid, 'skipgram')\n",
    "results_df = results_df._append(skipgram_data, ignore_index=True)\n",
    "fasttext_data, logistic_regression_fasttext_model = train_with_logistic_regression(df_fasttext, param_grid, 'fasttext')\n",
    "results_df = results_df._append(fasttext_data, ignore_index=True)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:542: FitFailedWarning: \n",
      "630 fits failed out of a total of 2100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "210 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.91601251        nan 0.91646065 0.92230654\n",
      " 0.92235689 0.92156132 0.9223015  0.92230654        nan        nan\n",
      " 0.91601251        nan 0.91646065 0.92230654 0.92235689 0.92156132\n",
      " 0.92230654 0.9223015         nan        nan 0.91601251        nan\n",
      " 0.91646065 0.92230654 0.92235689 0.92156132 0.9223015  0.9223015\n",
      "        nan        nan 0.91601251        nan 0.91646065 0.92230654\n",
      " 0.92235689 0.92156132 0.92230654 0.9223015         nan        nan\n",
      " 0.91601251        nan 0.91646065 0.92230654 0.92235689 0.92156132\n",
      " 0.92230654 0.92230654        nan        nan 0.91601251        nan\n",
      " 0.91646065 0.92230654 0.92235689 0.92156132 0.9223015  0.9223015\n",
      "        nan        nan 0.92276978        nan 0.92284531 0.9231726\n",
      " 0.92317763 0.92284531 0.9231726  0.9231726         nan        nan\n",
      " 0.92276978        nan 0.92285034 0.9231726  0.92317763 0.92284531\n",
      " 0.9231726  0.9231726         nan        nan 0.92276978        nan\n",
      " 0.92285034 0.9231726  0.92317763 0.92284531 0.9231726  0.9231726\n",
      "        nan        nan 0.92276978        nan 0.92285034 0.9231726\n",
      " 0.92317763 0.92284531 0.9231726  0.9231726         nan        nan\n",
      " 0.92276978        nan 0.92285034 0.9231726  0.92317763 0.92284531\n",
      " 0.9231726  0.9231726         nan        nan 0.92276978        nan\n",
      " 0.92286041 0.9231726  0.92317763 0.92284531 0.9231726  0.9231726\n",
      "        nan        nan 0.92321288        nan 0.92324309 0.92325819\n",
      " 0.92326826 0.92323302 0.92325819 0.92325819        nan        nan\n",
      " 0.92321288        nan 0.92324309 0.92325819 0.92326826 0.92323302\n",
      " 0.92325819 0.92325819        nan        nan 0.92321288        nan\n",
      " 0.92323805 0.92325819 0.92326826 0.92323302 0.92325819 0.92325819\n",
      "        nan        nan 0.92321288        nan 0.92323805 0.92325819\n",
      " 0.92326826 0.92323302 0.92325819 0.92325819        nan        nan\n",
      " 0.92321288        nan 0.92324309 0.92325819 0.92326826 0.92323302\n",
      " 0.92325819 0.92325819        nan        nan 0.92321288        nan\n",
      " 0.92323805 0.92325819 0.92326826 0.92323302 0.92325819 0.92325819\n",
      "        nan        nan 0.92325819        nan 0.92325819 0.92325819\n",
      " 0.92324309 0.92326323 0.92325316 0.92325316        nan        nan\n",
      " 0.92325819        nan 0.92326323 0.92325819 0.92324309 0.92326323\n",
      " 0.92325819 0.92324812        nan        nan 0.92325819        nan\n",
      " 0.92325819 0.92325819 0.92324309 0.92326323 0.92325819 0.92324812\n",
      "        nan        nan 0.92325819        nan 0.92325316 0.92325819\n",
      " 0.92324309 0.92326323 0.92325316 0.92325316        nan        nan\n",
      " 0.92325819        nan 0.92325819 0.92325819 0.92324309 0.92326323\n",
      " 0.92325819 0.92325819        nan        nan 0.92325819        nan\n",
      " 0.92325316 0.92325819 0.92324309 0.92326323 0.92325819 0.92325316\n",
      "        nan        nan 0.92326323        nan 0.92325819 0.92325316\n",
      " 0.92325316 0.92325819 0.92326323 0.92325819        nan        nan\n",
      " 0.92326323        nan 0.92325316 0.92325316 0.92325316 0.92325819\n",
      " 0.92325819 0.92325819        nan        nan 0.92326323        nan\n",
      " 0.92325819 0.92325316 0.92325316 0.92325819 0.92325819 0.92325819\n",
      "        nan        nan 0.92326323        nan 0.92325819 0.92325316\n",
      " 0.92325316 0.92325819 0.92325819 0.92325819        nan        nan\n",
      " 0.92326323        nan 0.92325819 0.92325316 0.92325316 0.92325819\n",
      " 0.92326323 0.92326323        nan        nan 0.92325819        nan\n",
      " 0.92325316 0.92325316 0.92325316 0.92325819 0.92325819 0.92325819\n",
      "        nan        nan 0.92326323        nan 0.92325819 0.92325316\n",
      " 0.92325316 0.92325819 0.92325316 0.92326323        nan        nan\n",
      " 0.92326323        nan 0.92325819 0.92325316 0.92325316 0.92325819\n",
      " 0.92325819 0.92325316        nan        nan 0.92325819        nan\n",
      " 0.92325819 0.92325316 0.92325316 0.92325819 0.92325819 0.92325316\n",
      "        nan        nan 0.92325819        nan 0.92325819 0.92325316\n",
      " 0.92325316 0.92325819 0.92325819 0.92325316        nan        nan\n",
      " 0.92326323        nan 0.92325819 0.92325316 0.92325316 0.92325819\n",
      " 0.92325316 0.92325316        nan        nan 0.92326323        nan\n",
      " 0.92325819 0.92325316 0.92325316 0.92325819 0.92326323 0.92325316\n",
      "        nan        nan 0.92326323        nan 0.92325819 0.92325316\n",
      " 0.92325316 0.92325819 0.92325819 0.92325316        nan        nan\n",
      " 0.92326323        nan 0.92326323 0.92325316 0.92325316 0.92325819\n",
      " 0.92325819 0.92326323        nan        nan 0.92326323        nan\n",
      " 0.92325819 0.92325316 0.92325316 0.92325819 0.92325819 0.92325819\n",
      "        nan        nan 0.92326323        nan 0.92325819 0.92325316\n",
      " 0.92325316 0.92325819 0.92325819 0.92324812        nan        nan\n",
      " 0.92325819        nan 0.92326323 0.92325316 0.92325316 0.92325819\n",
      " 0.92324812 0.92325819        nan        nan 0.92326323        nan\n",
      " 0.92324812 0.92325316 0.92325316 0.92325819 0.92326323 0.92325819]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [100, 500, 1000, 5000, 10000, 50000],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took 25 mins to get the best parameters.\n",
    "```\n",
    "param_space = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [100, 500, 1000, 5000, 10000, 50000],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "```\n",
    "The best params are: \n",
    "```\n",
    "{'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "```\n",
    "\n",
    "We revise our param_grid to improve the best parameters further so that we can avoid overfitting and underfitting. We choose 'l2' for 'penalty' and 'lbfgs' for 'solver'. For 'C', we generate 10 numbers between 0.01 and 1. For 'max_iter', we generate random integers between 50 and 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9265674407363397\n",
      "Precision: 0.9186826145148944\n",
      "Recall: 0.9265674407363397\n",
      "F1 Score: 0.9187615596666782\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with the best parameters to the training data\n",
    "model_best = LogisticRegression(**best_params)\n",
    "model_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_best.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)  # Handling division by zero for classes with no predictions\n",
    "recall = recall_score(y_val, y_pred, average='weighted')\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the evaluation metrics.\n",
    "```\n",
    "    Accuracy: 0.9265674407363397\n",
    "    Precision: 0.9186826145148944\n",
    "    Recall: 0.9265674407363397\n",
    "    F1 Score: 0.9187615596666782\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1291549665014884, 'max_iter': 88, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': np.logspace(-2, 0, 10),\n",
    "    'penalty': ['l2'],\n",
    "    'max_iter': np.random.randint(50, 251, size=10),\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9265875813175968\n",
      "Precision: 0.9187091269813016\n",
      "Recall: 0.9265875813175968\n",
      "F1 Score: 0.9187888187481864\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with the best parameters to the training data\n",
    "model_best = LogisticRegression(**best_params)\n",
    "model_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_best.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)  # Handling division by zero for classes with no predictions\n",
    "recall = recall_score(y_val, y_pred, average='weighted')\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the evaluation metrics.\n",
    "```\n",
    "    Accuracy: 0.9265875813175968\n",
    "    Precision: 0.9187091269813016\n",
    "    Recall: 0.9265875813175968\n",
    "    F1 Score: 0.9187888187481864\n",
    "```\n",
    "The performance is good. The second results has minor improvement compared to the first one and the running time for getting the best parameters is much less than the first one. In this case, after tuning hyperparameters, we found that \n",
    "```\n",
    "    Best parameters: {'C': 0.1291549665014884, 'max_iter': 88, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2394  2810]\n",
      " [  835 43612]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_test, pred, title):\n",
    "\tconf_matrix = confusion_matrix(y_test, pred)  # Use predictions_binary or the equivalent for your case\n",
    "\tplt.figure(figsize=(10, 7))\n",
    "\tsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Class1', 'Class2'], yticklabels=['Class1', 'Class2'])\n",
    "\tplt.xlabel('Predicted Labels')\n",
    "\tplt.ylabel('True Labels')\n",
    "\tplt.title(title)\n",
    "\tplt.show()\n",
    "\tprint(conf_matrix)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_val, y_pred, \"Logistic Regression Confusion Matrix\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
