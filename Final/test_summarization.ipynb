{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (4.32.1)\n",
      "Requirement already satisfied: datasets in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (2.18.0)\n",
      "Requirement already satisfied: torch in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: accelerate in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (0.29.1)\n",
      "Requirement already satisfied: filelock in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: psutil in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch scikit-learn accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('LightTai/personalized-email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['product', 'gender', 'profession', 'hobby', 'email'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "})\n",
      "{'product': ['piano lessons', 'guitar lessons', 'vacation plans', 'vacation plans', 'vacation plans'], 'gender': ['male', 'male', 'male', 'female', 'female'], 'profession': ['college students', 'college students', 'college students', 'college students', 'company employees'], 'hobby': ['like to play piano', 'like to play piano', 'like swimming', 'like to look at the scenery', 'like to look at the scenery'], 'email': [\"Subject: Elevate Your Piano Skills - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nLooking to unlock your piano potential? As a fellow male college student and a passionate piano player, I understand your love for music. That's why I'm thrilled to offer you exclusive piano lessons designed to fit your busy student schedule.\\n\\nMaster your favorite melodies, refine techniques, and gain a deeper understanding of music theory-all while enjoying a flexible lesson plan tailored to your goals. Let's embark on this musical journey together!\\n\\nLimited slots available, so act fast! Reply now for more details and an irresistible offer.\\n\\nKeep playing and shining!\\n\\n[Your Name]\", \"Subject: Expand Your Musical Horizons - Exclusive Guitar Lessons Offer!\\n\\nHey [Name],\\n\\nLooking to broaden your musical repertoire? As a male college student who enjoys playing the piano, I have an exciting opportunity for you. Discover the magic of guitar with our exclusive lessons tailored specifically for college students like you.\\n\\nUnleash your creativity, learn popular chords, and strum your favorite tunes in no time. Our flexible schedule ensures your studies won't miss a beat, and our experienced instructors will guide you every step of the way.\\n\\nLimited spaces available, so secure your spot now! Reply for more details and an exclusive offer that'll have you rocking the guitar in record time.\\n\\nStrumming success awaits!\\n\\n[Your Name]\", \"Subject: Dive into the Perfect Getaway - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nReady to make a splash on your next vacation? As a fellow male college student who loves swimming, I've got the perfect getaway plan for you. Imagine crystal-clear waters, vibrant marine life, and sandy beaches-the ultimate aquatic paradise!\\n\\nOur specially curated vacation package combines your passion for swimming with unforgettable adventures. Dive into pristine oceans, explore hidden coves, and unwind under the warm sun. It's the ultimate escape from college stress!\\n\\nLimited spots available, so don't miss out on this aquatic adventure. Reply now for more details and an exclusive offer that'll make your swimmer's heart leap!\\n\\nMake a splash!\\n\\n[Your Name]\", \"Subject: Escape to Enchanting Landscapes - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nCraving an awe-inspiring adventure? As a fellow female college student who appreciates breathtaking scenery, I have the perfect vacation plan for you. Immerse yourself in captivating landscapes, from lush valleys to majestic mountains, and create unforgettable memories.\\n\\nOur carefully curated vacation package combines stunning natural beauty with thrilling activities that cater to your adventurous spirit. Capture picture-perfect moments, hike scenic trails, and soak in the serenity of nature.\\n\\nLimited spots available, so don't miss out on this enchanting journey. Reply now for more details and an exclusive offer that will transport you to the most picturesque destinations.\\n\\nUnleash your wanderlust!\\n\\n[Your Name]\", \"Subject: Escape the Daily Grind - Exclusive Vacation Offer Inside!\\n\\nHey [Name],\\n\\nReady for a rejuvenating getaway? As a female professional who appreciates scenic beauty, I have an irresistible vacation plan tailored just for you and your colleagues. Step away from the office and immerse yourselves in breathtaking landscapes that will recharge your spirits.\\n\\nIndulge in mesmerizing vistas, explore hidden gems, and unwind amidst nature's wonders. Our curated package offers a perfect blend of relaxation and adventure, creating unforgettable memories for your team.\\n\\nLimited availability, so don't miss out on this exclusive offer. Reply now for more details and start planning a revitalizing escape from the corporate hustle.\\n\\nRediscover serenity!\\n\\n[Your Name]\"]}\n",
      "          product  gender         profession                        hobby  \\\n",
      "0   piano lessons    male   college students           like to play piano   \n",
      "1  guitar lessons    male   college students           like to play piano   \n",
      "2  vacation plans    male   college students                like swimming   \n",
      "3  vacation plans  female   college students  like to look at the scenery   \n",
      "4  vacation plans  female  company employees  like to look at the scenery   \n",
      "\n",
      "                                               email  \n",
      "0  Subject: Elevate Your Piano Skills - Exclusive...  \n",
      "1  Subject: Expand Your Musical Horizons - Exclus...  \n",
      "2  Subject: Dive into the Perfect Getaway - Exclu...  \n",
      "3  Subject: Escape to Enchanting Landscapes - Exc...  \n",
      "4  Subject: Escape the Daily Grind - Exclusive Va...  \n",
      "(30, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['train'][:5])\n",
    "df = dataset['train'].to_pandas()\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 24/24 [00:00<00:00, 44.24 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 6/6 [00:00<00:00, 16.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "model_checkpoint = \"postbot/distilgpt2-emailgen\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    features = [f\"{prod} {gen} {prof} {hob}\" \n",
    "                for prod, gen, prof, hob in zip(examples[\"product\"], \n",
    "                                                examples[\"gender\"], \n",
    "                                                examples[\"profession\"], \n",
    "                                                examples[\"hobby\"])]\n",
    "    tokenized_inputs = tokenizer(features, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Tokenize the email column which is our target\n",
    "    tokenized_targets = tokenizer(examples[\"email\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    tokenized_inputs['labels'] = tokenized_targets['input_ids']  # Assign target token ids as labels for training\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's already loaded\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Convert the DataFrames back to Hugging Face dataset format if necessary\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n",
    "# Assuming 'dataset' is a Hugging Face 'datasets' object\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 24\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__index_level_0__': 14,\n",
       " 'input_ids': [25119,\n",
       "  12437,\n",
       "  4048,\n",
       "  3710,\n",
       "  588,\n",
       "  284,\n",
       "  467,\n",
       "  9735,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [19776,\n",
       "  25,\n",
       "  5012,\n",
       "  656,\n",
       "  257,\n",
       "  16509,\n",
       "  2848,\n",
       "  338,\n",
       "  20494,\n",
       "  532,\n",
       "  35651,\n",
       "  7467,\n",
       "  33085,\n",
       "  14384,\n",
       "  0,\n",
       "  198,\n",
       "  198,\n",
       "  10814,\n",
       "  685,\n",
       "  5376,\n",
       "  4357,\n",
       "  198,\n",
       "  198,\n",
       "  1722,\n",
       "  257,\n",
       "  4048,\n",
       "  3710,\n",
       "  508,\n",
       "  10408,\n",
       "  262,\n",
       "  28576,\n",
       "  286,\n",
       "  9735,\n",
       "  11,\n",
       "  314,\n",
       "  1101,\n",
       "  6568,\n",
       "  284,\n",
       "  10400,\n",
       "  345,\n",
       "  284,\n",
       "  257,\n",
       "  995,\n",
       "  286,\n",
       "  2691,\n",
       "  6308,\n",
       "  9102,\n",
       "  13,\n",
       "  41401,\n",
       "  674,\n",
       "  5909,\n",
       "  4947,\n",
       "  286,\n",
       "  44493,\n",
       "  6977,\n",
       "  11,\n",
       "  18199,\n",
       "  11,\n",
       "  290,\n",
       "  517,\n",
       "  11,\n",
       "  477,\n",
       "  379,\n",
       "  534,\n",
       "  40142,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  5497,\n",
       "  377,\n",
       "  469,\n",
       "  287,\n",
       "  262,\n",
       "  3452,\n",
       "  12186,\n",
       "  11,\n",
       "  23658,\n",
       "  3748,\n",
       "  7228,\n",
       "  11,\n",
       "  290,\n",
       "  6128,\n",
       "  284,\n",
       "  534,\n",
       "  2612,\n",
       "  338,\n",
       "  2695,\n",
       "  11,\n",
       "  1231,\n",
       "  4305,\n",
       "  534,\n",
       "  18586,\n",
       "  2119,\n",
       "  13,\n",
       "  18179,\n",
       "  262,\n",
       "  15607,\n",
       "  286,\n",
       "  3049,\n",
       "  290,\n",
       "  9314,\n",
       "  8440,\n",
       "  11,\n",
       "  13359,\n",
       "  534,\n",
       "  649,\n",
       "  18852,\n",
       "  9240,\n",
       "  826,\n",
       "  319,\n",
       "  640,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  37214,\n",
       "  11500,\n",
       "  11,\n",
       "  523,\n",
       "  836,\n",
       "  470,\n",
       "  2051,\n",
       "  503,\n",
       "  319,\n",
       "  428,\n",
       "  8568,\n",
       "  2897,\n",
       "  13,\n",
       "  14883,\n",
       "  783,\n",
       "  329,\n",
       "  517,\n",
       "  3307,\n",
       "  290,\n",
       "  15647,\n",
       "  656,\n",
       "  257,\n",
       "  7354,\n",
       "  2848,\n",
       "  338,\n",
       "  31354,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  5756,\n",
       "  338,\n",
       "  6128,\n",
       "  705,\n",
       "  47163,\n",
       "  356,\n",
       "  4268,\n",
       "  0,\n",
       "  198,\n",
       "  198,\n",
       "  58,\n",
       "  7120,\n",
       "  6530,\n",
       "  60,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                              \n",
      " 33%|███▎      | 6/18 [02:34<04:30, 22.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.531621932983398, 'eval_runtime': 6.6834, 'eval_samples_per_second': 0.898, 'eval_steps_per_second': 0.299, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 12/18 [04:36<02:09, 21.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.367166042327881, 'eval_runtime': 5.3518, 'eval_samples_per_second': 1.121, 'eval_steps_per_second': 0.374, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 18/18 [06:02<00:00, 20.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.947717189788818, 'eval_runtime': 5.9565, 'eval_samples_per_second': 1.007, 'eval_steps_per_second': 0.336, 'epoch': 3.0}\n",
      "{'train_runtime': 362.7972, 'train_samples_per_second': 0.198, 'train_steps_per_second': 0.05, 'train_loss': 6.530542585584852, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=6.530542585584852, metrics={'train_runtime': 362.7972, 'train_samples_per_second': 0.198, 'train_steps_per_second': 0.05, 'train_loss': 6.530542585584852, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 140.85\n",
      "{'eval_loss': 4.947717189788818, 'eval_runtime': 7.3058, 'eval_samples_per_second': 0.821, 'eval_steps_per_second': 0.274, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
