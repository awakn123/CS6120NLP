{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (4.32.1)\n",
      "Requirement already satisfied: datasets in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (2.18.0)\n",
      "Requirement already satisfied: torch in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: accelerate in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (0.29.1)\n",
      "Requirement already satisfied: spacy in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (3.7.4)\n",
      "Requirement already satisfied: evaluate in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (0.4.1)\n",
      "Requirement already satisfied: nltk in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge_score in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: filelock in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: psutil in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (2.7.0)\n",
      "Requirement already satisfied: setuptools in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from evaluate) (0.13.3)\n",
      "Requirement already satisfied: click in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch scikit-learn accelerate spacy evaluate nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('LightTai/personalized-email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['product', 'gender', 'profession', 'hobby', 'email'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "})\n",
      "{'product': ['piano lessons', 'guitar lessons', 'vacation plans', 'vacation plans', 'vacation plans'], 'gender': ['male', 'male', 'male', 'female', 'female'], 'profession': ['college students', 'college students', 'college students', 'college students', 'company employees'], 'hobby': ['like to play piano', 'like to play piano', 'like swimming', 'like to look at the scenery', 'like to look at the scenery'], 'email': [\"Subject: Elevate Your Piano Skills - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nLooking to unlock your piano potential? As a fellow male college student and a passionate piano player, I understand your love for music. That's why I'm thrilled to offer you exclusive piano lessons designed to fit your busy student schedule.\\n\\nMaster your favorite melodies, refine techniques, and gain a deeper understanding of music theory-all while enjoying a flexible lesson plan tailored to your goals. Let's embark on this musical journey together!\\n\\nLimited slots available, so act fast! Reply now for more details and an irresistible offer.\\n\\nKeep playing and shining!\\n\\n[Your Name]\", \"Subject: Expand Your Musical Horizons - Exclusive Guitar Lessons Offer!\\n\\nHey [Name],\\n\\nLooking to broaden your musical repertoire? As a male college student who enjoys playing the piano, I have an exciting opportunity for you. Discover the magic of guitar with our exclusive lessons tailored specifically for college students like you.\\n\\nUnleash your creativity, learn popular chords, and strum your favorite tunes in no time. Our flexible schedule ensures your studies won't miss a beat, and our experienced instructors will guide you every step of the way.\\n\\nLimited spaces available, so secure your spot now! Reply for more details and an exclusive offer that'll have you rocking the guitar in record time.\\n\\nStrumming success awaits!\\n\\n[Your Name]\", \"Subject: Dive into the Perfect Getaway - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nReady to make a splash on your next vacation? As a fellow male college student who loves swimming, I've got the perfect getaway plan for you. Imagine crystal-clear waters, vibrant marine life, and sandy beaches-the ultimate aquatic paradise!\\n\\nOur specially curated vacation package combines your passion for swimming with unforgettable adventures. Dive into pristine oceans, explore hidden coves, and unwind under the warm sun. It's the ultimate escape from college stress!\\n\\nLimited spots available, so don't miss out on this aquatic adventure. Reply now for more details and an exclusive offer that'll make your swimmer's heart leap!\\n\\nMake a splash!\\n\\n[Your Name]\", \"Subject: Escape to Enchanting Landscapes - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nCraving an awe-inspiring adventure? As a fellow female college student who appreciates breathtaking scenery, I have the perfect vacation plan for you. Immerse yourself in captivating landscapes, from lush valleys to majestic mountains, and create unforgettable memories.\\n\\nOur carefully curated vacation package combines stunning natural beauty with thrilling activities that cater to your adventurous spirit. Capture picture-perfect moments, hike scenic trails, and soak in the serenity of nature.\\n\\nLimited spots available, so don't miss out on this enchanting journey. Reply now for more details and an exclusive offer that will transport you to the most picturesque destinations.\\n\\nUnleash your wanderlust!\\n\\n[Your Name]\", \"Subject: Escape the Daily Grind - Exclusive Vacation Offer Inside!\\n\\nHey [Name],\\n\\nReady for a rejuvenating getaway? As a female professional who appreciates scenic beauty, I have an irresistible vacation plan tailored just for you and your colleagues. Step away from the office and immerse yourselves in breathtaking landscapes that will recharge your spirits.\\n\\nIndulge in mesmerizing vistas, explore hidden gems, and unwind amidst nature's wonders. Our curated package offers a perfect blend of relaxation and adventure, creating unforgettable memories for your team.\\n\\nLimited availability, so don't miss out on this exclusive offer. Reply now for more details and start planning a revitalizing escape from the corporate hustle.\\n\\nRediscover serenity!\\n\\n[Your Name]\"]}\n",
      "          product  gender         profession                        hobby  \\\n",
      "0   piano lessons    male   college students           like to play piano   \n",
      "1  guitar lessons    male   college students           like to play piano   \n",
      "2  vacation plans    male   college students                like swimming   \n",
      "3  vacation plans  female   college students  like to look at the scenery   \n",
      "4  vacation plans  female  company employees  like to look at the scenery   \n",
      "\n",
      "                                               email  \n",
      "0  Subject: Elevate Your Piano Skills - Exclusive...  \n",
      "1  Subject: Expand Your Musical Horizons - Exclus...  \n",
      "2  Subject: Dive into the Perfect Getaway - Exclu...  \n",
      "3  Subject: Escape to Enchanting Landscapes - Exc...  \n",
      "4  Subject: Escape the Daily Grind - Exclusive Va...  \n",
      "(30, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['train'][:5])\n",
    "df = dataset['train'].to_pandas()\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'random_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Assuming 'df' is your DataFrame and it's already loaded\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Convert the DataFrames back to Hugging Face dataset format if necessary\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(train_df)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_sig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/inspect.py:3037\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/inspect.py:3026\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3024\u001b[0m         arguments[kwargs_param\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   3025\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3026\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3027\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   3028\u001b[0m                 arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[0;31mTypeError\u001b[0m: got an unexpected keyword argument 'random_seed'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "model_checkpoint = \"postbot/distilgpt2-emailgen-V2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    features = [f\"{prod} {gen} {prof} {hob}\" \n",
    "                for prod, gen, prof, hob in zip(examples[\"product\"], \n",
    "                                                examples[\"gender\"], \n",
    "                                                examples[\"profession\"], \n",
    "                                                examples[\"hobby\"])]\n",
    "    tokenized_inputs = tokenizer(features, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Tokenize the email column which is our target\n",
    "    tokenized_targets = tokenizer(examples[\"email\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    tokenized_inputs['labels'] = tokenized_targets['input_ids']  # Assign target token ids as labels for training\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's already loaded\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the DataFrames back to Hugging Face dataset format if necessary\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n",
    "# Assuming 'dataset' is a Hugging Face 'datasets' object\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 24\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__index_level_0__': 24,\n",
       " 'input_ids': [6966,\n",
       "  27757,\n",
       "  4257,\n",
       "  4409,\n",
       "  3504,\n",
       "  12,\n",
       "  1886,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [19776,\n",
       "  25,\n",
       "  2295,\n",
       "  46565,\n",
       "  3406,\n",
       "  7326,\n",
       "  1704,\n",
       "  532,\n",
       "  35651,\n",
       "  10437,\n",
       "  27757,\n",
       "  33085,\n",
       "  14384,\n",
       "  0,\n",
       "  198,\n",
       "  198,\n",
       "  10814,\n",
       "  685,\n",
       "  5376,\n",
       "  4357,\n",
       "  198,\n",
       "  198,\n",
       "  1722,\n",
       "  257,\n",
       "  6563,\n",
       "  290,\n",
       "  30511,\n",
       "  3504,\n",
       "  12,\n",
       "  1886,\n",
       "  4257,\n",
       "  4708,\n",
       "  11,\n",
       "  314,\n",
       "  423,\n",
       "  281,\n",
       "  8568,\n",
       "  2897,\n",
       "  326,\n",
       "  24538,\n",
       "  534,\n",
       "  3748,\n",
       "  45581,\n",
       "  13,\n",
       "  3954,\n",
       "  15313,\n",
       "  1627,\n",
       "  286,\n",
       "  36432,\n",
       "  318,\n",
       "  3562,\n",
       "  284,\n",
       "  9494,\n",
       "  534,\n",
       "  3288,\n",
       "  3033,\n",
       "  290,\n",
       "  5750,\n",
       "  534,\n",
       "  2116,\n",
       "  12,\n",
       "  39745,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  44596,\n",
       "  257,\n",
       "  3094,\n",
       "  2837,\n",
       "  286,\n",
       "  8683,\n",
       "  41836,\n",
       "  11,\n",
       "  1341,\n",
       "  1939,\n",
       "  533,\n",
       "  11,\n",
       "  290,\n",
       "  8737,\n",
       "  3186,\n",
       "  27571,\n",
       "  284,\n",
       "  262,\n",
       "  2476,\n",
       "  286,\n",
       "  15345,\n",
       "  1450,\n",
       "  13,\n",
       "  2295,\n",
       "  46565,\n",
       "  35985,\n",
       "  32126,\n",
       "  326,\n",
       "  31219,\n",
       "  680,\n",
       "  11,\n",
       "  46834,\n",
       "  378,\n",
       "  11,\n",
       "  290,\n",
       "  9494,\n",
       "  534,\n",
       "  5585,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  37214,\n",
       "  11500,\n",
       "  11,\n",
       "  523,\n",
       "  836,\n",
       "  470,\n",
       "  2051,\n",
       "  503,\n",
       "  319,\n",
       "  428,\n",
       "  8568,\n",
       "  2897,\n",
       "  13,\n",
       "  14883,\n",
       "  783,\n",
       "  329,\n",
       "  517,\n",
       "  3307,\n",
       "  290,\n",
       "  36830,\n",
       "  534,\n",
       "  41836,\n",
       "  8027,\n",
       "  351,\n",
       "  674,\n",
       "  40123,\n",
       "  4947,\n",
       "  286,\n",
       "  36432,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  10161,\n",
       "  46565,\n",
       "  534,\n",
       "  6628,\n",
       "  11,\n",
       "  42241,\n",
       "  0,\n",
       "  198,\n",
       "  198,\n",
       "  58,\n",
       "  7120,\n",
       "  6530,\n",
       "  60,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "rouge = evaluate.load('rouge')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits from numpy arrays to PyTorch tensors\n",
    "    logits_tensor = torch.tensor(logits)\n",
    "    \n",
    "    # Convert logits to predicted token IDs using argmax\n",
    "    predictions_ids = torch.argmax(logits_tensor, dim=-1)\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    predictions = tokenizer.batch_decode(predictions_ids, skip_special_tokens=True)\n",
    "    # Assuming labels are already decoded; if not, decode them similarly\n",
    "    references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Compute ROUGE scores\n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_predictions = tokenized_train_dataset[1][\"input_ids\"]\n",
    "# test_references = tokenized_train_dataset[1][\"labels\"]\n",
    "# test_eval_pred = (test_predictions, test_references)\n",
    "# rouge_results = compute_metrics(test_eval_pred)\n",
    "# print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caoyun/miniconda3/envs/ml-env/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 6/18 [1:09:46<2:19:32, 697.74s/it]\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 6/18 [01:38<04:23, 22.00s/it]\n",
      " 33%|███▎      | 6/18 [01:56<04:23, 22.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.867982387542725, 'eval_rouge1': 0.042624800159872106, 'eval_rouge2': 0.0, 'eval_rougeL': 0.03945778171084146, 'eval_rougeLsum': 0.039713047620065306, 'eval_runtime': 17.5636, 'eval_samples_per_second': 0.342, 'eval_steps_per_second': 0.114, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12/18 [04:17<02:33, 25.58s/it]\n",
      " 67%|██████▋   | 12/18 [04:28<02:33, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.897278070449829, 'eval_rouge1': 0.04845423551009703, 'eval_rouge2': 0.0, 'eval_rougeL': 0.045584099972756906, 'eval_rougeLsum': 0.04585939214764048, 'eval_runtime': 11.1477, 'eval_samples_per_second': 0.538, 'eval_steps_per_second': 0.179, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [06:22<00:00, 20.17s/it]\n",
      "100%|██████████| 18/18 [06:30<00:00, 21.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.998006582260132, 'eval_rouge1': 0.04845423551009703, 'eval_rouge2': 0.0, 'eval_rougeL': 0.045584099972756906, 'eval_rougeLsum': 0.04585939214764048, 'eval_runtime': 7.0582, 'eval_samples_per_second': 0.85, 'eval_steps_per_second': 0.283, 'epoch': 3.0}\n",
      "{'train_runtime': 389.9726, 'train_samples_per_second': 0.185, 'train_steps_per_second': 0.046, 'train_loss': 5.576114230685764, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.json',\n",
       " './saved_model/merges.txt',\n",
       " './saved_model/added_tokens.json',\n",
       " './saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "\t\tcompute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer after training is complete\n",
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 54.49\n",
      "{'eval_loss': 3.998006582260132, 'eval_rouge1': 0.04845423551009703, 'eval_rouge2': 0.0, 'eval_rougeL': 0.045584099972756906, 'eval_rougeLsum': 0.04585939214764048, 'eval_runtime': 4.004, 'eval_samples_per_second': 1.499, 'eval_steps_per_second': 0.5, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['truncation'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m profession \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftware Engineer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m hobby \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGaming\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 46\u001b[0m email_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_email\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhobby\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Email:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(email_text)\n",
      "Cell \u001b[0;32mIn[60], line 26\u001b[0m, in \u001b[0;36mgenerate_email\u001b[0;34m(product, gender, profession, hobby)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_email\u001b[39m(product, gender, profession, hobby):\n\u001b[1;32m     25\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgender\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprofession\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhobby\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 更低温度以减少随机性\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 生成多个序列，选取最佳结果\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# 从生成的序列中选择最佳输出\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     best_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([clean_generated_text(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:204\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/pipelines/base.py:1129\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m         )\n\u001b[1;32m   1127\u001b[0m     )\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/pipelines/base.py:1136\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1135\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1136\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/pipelines/base.py:1035\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1034\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1035\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:265\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/generation/utils.py:1423\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m-> 1423\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m logits_processor \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/generation/utils.py:1243\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['truncation'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 加载保存的模型和分词器\n",
    "model_checkpoint = \"./saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 设置生成管道\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def clean_generated_text(text):\n",
    "    # 基本清理\n",
    "    # text = re.sub(r'^(Re:|Fwd:)', '', text)  # 删除回复和转发标记\n",
    "    # text = re.sub(r'Best regards,.*$', '', text, flags=re.DOTALL)  # 删除签名以后的所有内容\n",
    "    # text = re.sub(r'PHONE.*$', '', text, flags=re.DOTALL)  # 删除电话号码以后的所有内容\n",
    "    # text = re.sub(r'Email:.*$', '', text, flags=re.DOTALL)  # 删除邮箱以后的所有内容\n",
    "    # text = re.sub(r'Cc:.*$', '', text, flags=re.DOTALL)  # 删除抄送列表\n",
    "    # text = re.sub(r'\\* Attachments:.*', '', text, flags=re.S)  # 删除'Attachments:'及其后的所有内容\n",
    "    # text = re.sub(r'©️ .*$', '', text, flags=re.DOTALL)  # 删除版权和所有权声明\n",
    "    # text = re.sub(r'URL If this message is not displaying properly, click here.*$', '', text, flags=re.DOTALL)  # 删除错误显示消息和链接\n",
    "    # text = re.sub(r'\\d{5,}', 'NUMBER', text)  # 替换过长的数字序列，可能是电话号码或邮编\n",
    "    return text.strip()\n",
    "\n",
    "def generate_email(product, gender, profession, hobby):\n",
    "    input_text = f\"{product} {gender} {profession} {hobby}\"\n",
    "    result = generator(\n",
    "        input_text,\n",
    "        max_length=256,\n",
    "        do_sample=True,\n",
    "        top_k=20,\n",
    "        top_p=0.6,\n",
    "        temperature=0.4,  # 更低温度以减少随机性\n",
    "        repetition_penalty=1.5,\n",
    "        truncation=True,\n",
    "        num_return_sequences=3)  # 生成多个序列，选取最佳结果\n",
    "    # 从生成的序列中选择最佳输出\n",
    "    best_text = sorted([clean_generated_text(r['generated_text']) for r in result], key=len)[-1]\n",
    "    return best_text\n",
    "\n",
    "# 示例参数和生成\n",
    "product = \"Laptop\"\n",
    "gender = \"Male\"\n",
    "profession = \"Software Engineer\"\n",
    "hobby = \"Gaming\"\n",
    "\n",
    "email_text = generate_email(product, gender, profession, hobby)\n",
    "print(\"Generated Email:\")\n",
    "print(email_text)\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Email:\n",
    "Laptop Male Software Engineer Gaming Technology - COMPANY Corporation of America (NYSE: SNE)\n",
    "Experience the power and excitement that drives gaming, social games & entertainment. Experience your personalized experiences with a variety of mobile devices from anywhere in our network including tablets to smartphones, game consoles, plasma TVs, Blu-ray players etc.. The experience is immersive so you can experience all aspects at one time while exploring other technologies. You will also be able to access exclusive content such as Pandora's World which allows users to listen directly on their phones or watch videos instantly on any device. We are confident this will help us achieve more than NUMBER million people worldwide who have already purchased product via apps like Facebook Messenger, Twitter API for Android and Windows Phone/iPad Air Connectivity. For more information please visit URL\n",
    "URL logo\n",
    "Questions? Call Igor Gruppin PHONE Fax +PHONE EMAIL\n",
    "To contact Member Services | View Our Privacy Statement | Unsubscribe\n",
    "©️NUMBER Microsoft Corporation. All rights reserved. | Acceptable Use Policy | Contact Customer Service | NUMBER-HOUR CONTROL CENTER| USA\n",
    "This email was sent by: Microsoft Corporation\n",
    "NUMBER Marta St., Suite NUMBERSan Diego del Norte, CA NUM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load Spacy's English tokenizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set a random seed for reproducible results\n",
    "set_seed(42)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_checkpoint = \"./saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, padding_side='left')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Set up the pipeline using the freshly trained model and tokenizer\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def advanced_text_cleaning(text):\n",
    "    # Use Spacy to parse sentences and filter out overly short fragments and placeholders\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = []\n",
    "    for sent in doc.sents:\n",
    "        # Filtering out too short sentences and placeholders\n",
    "        if len(sent.text) > 20 and not re.search(r'NUMB[RL]|NUMBE[RSZ]', sent.text):\n",
    "            cleaned_text.append(sent.text.replace('\\n', ' ').strip())\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "def clean_generated_text(text):\n",
    "    return advanced_text_cleaning(text.strip())\n",
    "\n",
    "def generate_email(product, gender, profession, hobby, use_pipeline=False):\n",
    "    # 构建输入文本\n",
    "    input_text = f\"As a {gender.lower()} {profession} interested in {hobby}, I am looking for a {product} that suits my needs.\"\n",
    "\n",
    "    if use_pipeline:\n",
    "        result = generator(input_text, max_length=1024, do_sample=True, top_k=50, temperature=0.9, repetition_penalty=1.1, truncation=True)\n",
    "        generated_text = result[0]['generated_text']\n",
    "    else:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=1024,  # Increase maximum length\n",
    "            temperature=0.9,  # Higher diversity\n",
    "            top_k=50,         # Broader vocabulary choice\n",
    "            top_p=0.95,       # Nucleus sampling\n",
    "            no_repeat_ngram_size=2,  # Allow minimal repetition\n",
    "            repetition_penalty=1.1   # Slightly less repetition penalty\n",
    "        )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return clean_generated_text(generated_text)\n",
    "\n",
    "\n",
    "# 示例参数\n",
    "product = \"Laptop\"\n",
    "gender = \"Male\"\n",
    "profession = \"Software Engineer\"\n",
    "hobby = \"Gaming\"\n",
    "\n",
    "# 生成电子邮件\n",
    "email_text = generate_email(product, gender, profession, hobby, use_pipeline=True)\n",
    "print(\"Generated Email:\")\n",
    "print(email_text)\n",
    "print(\"--------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic cleanup: remove redundant phrases and unwanted tokens\n",
    "    # text = re.sub(r'\\b(URL)\\b\\s*', '', text)  # Remove 'URL' placeholders\n",
    "    # text = re.sub(r'\\b(\\d{1,2}[:]\\d{2}\\s*(AM|PM))\\b', '', text)  # Remove standalone time\n",
    "    # text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citation-like numbers\n",
    "    # text = re.sub(r\"NUMBER|EMAIL|PHONE|FAX\", \"\", text)\n",
    "    # text = re.sub(r\"\\[.*?\\]|-\\s*-\\s*|<.*?>\", \"\", text)  # Clean up brackets and dashed lines\n",
    "    # text = re.sub(r'\\s{2,}', ' ', text)  # Replace multiple spaces with a single space\n",
    "    # text = re.sub(r'\\* Attachments:.*', '', text, flags=re.S)  # 删除'Attachments:'及其后的所有内容\n",
    "    # text = re.sub(r'\\bAttachments:\\s*(\\( Bytes\\)\\s*)+(x\\s*\\( Bytes\\)\\s*)*', '', text, flags=re.S)\n",
    "    # text = re.sub(r'\\|\\s.*', '', text)  # Remove suffixes after names # This regex removes everything after a '|' character"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用pipeline生成文本\n",
    "custom_text1_with_pipeline = generate_email(\"Brief project update email for a software development team\", use_pipeline=True)\n",
    "custom_text2_with_pipeline = generate_email(\"Detailed financial report request by a senior analyst\", use_pipeline=True)\n",
    "custom_text3_with_pipeline = generate_email(\"Creative brief for a new marketing campaign targeting young adults\", use_pipeline=True)\n",
    "\n",
    "# 不使用pipeline生成文本\n",
    "custom_text1_without_pipeline = generate_email(\"Brief project update email for a software development team\", use_pipeline=False)\n",
    "custom_text2_without_pipeline = generate_email(\"Detailed financial report request by a senior analyst\", use_pipeline=False)\n",
    "custom_text3_without_pipeline = generate_email(\"Creative brief for a new marketing campaign targeting young adults\", use_pipeline=False)\n",
    "\n",
    "# 打印使用pipeline生成的文本\n",
    "print(\"Software Development Team Email (with pipeline):\")\n",
    "print(custom_text1_with_pipeline)\n",
    "print(\"--------------------\")\n",
    "print(\"Financial Report Request Email (with pipeline):\")\n",
    "print(custom_text2_with_pipeline)\n",
    "print(\"--------------------\")\n",
    "print(\"Marketing Campaign Creative Brief (with pipeline):\")\n",
    "print(custom_text3_with_pipeline)\n",
    "print(\"--------------------\")\n",
    "\n",
    "# 打印不使用pipeline生成的文本\n",
    "print(\"Software Development Team Email (without pipeline):\")\n",
    "print(custom_text1_without_pipeline)\n",
    "print(\"--------------------\")\n",
    "print(\"Financial Report Request Email (without pipeline):\")\n",
    "print(custom_text2_without_pipeline)\n",
    "print(\"--------------------\")\n",
    "print(\"Marketing Campaign Creative Brief (without pipeline):\")\n",
    "print(custom_text3_without_pipeline)\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Output V3\n",
    "Software Development Team Email (with pipeline):\n",
    "Brief project update email for a software development team meeting this weekend was provided below in advance of Tuesday's meeting. We understand that the schedule for this meeting has changed. I wanted to get everyone an update on our progress. Please provide your feedback by this Thursday, May th after we have been completed and finalized. I wish you all the best in the best possible future. Best regards, Maxene A. Verma\n",
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Output V2\n",
    "\n",
    "Software Development Team Email (with pipeline):\n",
    "Brief project update email for a software development team meeting this weekend was provided below in advance of Tuesday's meeting. We understand that the schedule for this meeting has changed. I wanted to get everyone an update on our progress. Please provide your feedback by this Thursday, May th after we have been completed and finalized. I wish you all the best in the best possible future. Best regards, Maxene A. Verma | Marketing & Research Paralegal | COMPANY Pictures Releasing International, West Washington Blvd., Thalberg Bldg. | Culver City, CA ( | |\n",
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Output V1\n",
    "\n",
    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "Software Development Team Email (with pipeline):\n",
    "Brief project update email for a software development team meeting this weekend was provided below in advance of Tuesday's meeting. We understand that the schedule for this meeting has changed. I wanted to get everyone an update on our progress. Please provide your feedback by this Thursday, May th after we have been completed and finalized. I wish you all the best in the best possible future. Best regards, Maxene A. Verma | Marketing & Research Paralegal | COMPANY Pictures Releasing International, West Washington Blvd., Thalberg Bldg. | Culver City, CA ( | | * Attachments: ( Bytes) x ( Bytes) x ( Bytes) x ( Bytes) ( Bytes) x ( Bytes) ( Bytes) x ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) x ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) m ( Bytes) x ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) Each individual must participate in the \"Join Together\" component of the Round Up Process. The Rules will be reviewed with the Rules by the Committee in partnership with the Chamber. Please contact me if you have any questions about these materials. Regards, Maxene A. Verma | Marketing & Research Paralegal | COMPANY Pictures Releasing International, West Washington Blvd., Thalberg Bldg. | Culver City, CA ( | | * Attachments: x ( Bytes) x ( Bytes) x ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) m ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) (\n",
    "--------------------\n",
    "Financial Report Request Email (with pipeline):\n",
    "Detailed financial report request by a senior analyst for a Northwest bank, the company is currently conducting an online survey of its employees and business operations. Our goal is to identify, among other things, companies that have \"met Wall Street's expectations\" or \"the market needs that are needed,\" the information being gathered from the survey should be deemed complete. An objective of the survey was that we would send an electronic resume, either verbal or by email in the next few days. To this end, the deadline for the survey has been extended, starting in March, to December February and ending in April th February. This will take place as follows: EES will conduct the survey using the online questionnaire to date. It will focus on identifying the most market-sceptical \"attributable\" employees. The results will be disclosed on Monday, with the second survey being conducted early next week, and our input will be considered during the second survey in the coming weeks. Regards, COMPANY Williams Associate Director, Public Relations If you are contacted regarding the above survey, please e-mail If you would like to be removed from future survey distribution, please e-mail If you would like to be removed from future surveys, please Please be advised that your direct reports will not be given again until completed. Thank you, Ingrid O'Donnell, Senior Manager, Commercial Support EES Business Development Contact Information You can also send an electronic resume, if you would like to have them taken a look on their qualifications for positions. Sincerely, The Survey Company DataSite Team - Customer Relationship Studies Team - POLITICAL PARTYic National Committee Montgomery Street, Suite LOCATION, Texas USA Telephone: (U) - Fax: (M) - Site Visit Map:<(toll-free):<(filing location: Hyatt Regency, Orange County, CA) Thanks for your participation and participation on the survey! Sincerely, The Survey Company DataSite Team - Customer Relationship Studies Team - POLITICAL PARTYic National Committee Montgomery Street, Suite LOCATION, Texas USA Telephone: (U) - Fax: (M) - Site Visit Map:<(toll-free):<(filing location: Hyatt Regency, Orange County, CA) All e-mails come directly from The Survey Company. In addition to being members of the Company's Executive Team who are not on the Company's Executive Team please consider the environment before printing this e-mail. This message was sent to: If you no longer wish to receive these emails, click on the following link: Unsubscribe. <a style=\"text-align:center!important; width=\"px!important; }.ExternalClass p:link, span.intent, div.us_SignInRow {margin: px ;} .ExternalClass font { font-family:\"Calibri\", \"Verdana\", \"Helvetica\", line-height:px; padding: px!important;} @media only screen and (max-width: px) { *.layout-, *.footer-layout { width:px!important; } *.layout-, *.center-align, *.show-icons,*.one-column, *.mediumgrid-header, *.outer-layout, *.media-layout, *.grid-layout, *.header-layout, *.featureagentads-layout, *.ads-layout, *.housead-layout, *.app, *.subscribe, *.unlimited-access, *.footer, *.access, *.app-class, *.app-span, *.copyright-block, *.view-layout { width:px!important; } *.media-layout { padding: px!important; margin:!important; } *.view-layout { margin: px !important; } *.grid-layout { margin:px px px!important; } *.header-layout\n",
    "--------------------\n",
    "Marketing Campaign Creative Brief (with pipeline):\n",
    "Creative brief for a new marketing campaign targeting young adults and their fans: It's not clear what the plan is for this animated movie from the beginning. ) How to Give an EXTRA EXPENSION of the trailer with the promise of a new brand, it's going to work very well for us - especially as in the new trailer for \"Grown Ups ,\" but we need to have a feeling about how much fun and powerful \"Grown Ups \" has been. We will be discussing this with everyone this weekend. ) The new trailer for Interview # (which is still in production) promises a lot more emotion and action for the film. Cc: Vollack, Lia; Marshall, Bob; Guerin, Jean This wasn't made in-the-works by Mike. They'd also like to give the current joke - an old joke, without any real conflict, to the movie, without the promise that it's a sequel or something. The script (and script) incorporates language I understand from the conversations with James and Phil last week: Regarding Interview # we just wanted you to know that we are committed to working on all the points on the horizontal and vertical artwork to show off COMPANY's upcoming campaign trailer. We want to address a number of concerns namely, if the'story' is used incorrectly, it's no good to use references from the previous trailers. If this isn't clear, then we may want to adjust and clarify what we mean by using some additional language to the story/concept. As we talked on Friday, I'm sending you the following email (from JLL) regarding several instances where we were able to bypass out our own campaign-include one that's more generic. But again, we will need to communicate with JLL about these issues/ concerns so that they don't pop into places where we are literally communicating. If this isn't clear then we may want to go ahead with the other campaign to get us next steps. Hereto be clear: when we talk about the possibility of splitting up the various businesses (Digital, Media Networks, Home Entertainment etc.), we understand the scope of the partnership, but are concerned about different businesses that can serve multiple brands in one fashion or another. We don't want to appear too stupid or wrong, however when we talk about a partnership that could deliver big gains in the long run. Again, this is confidential, and we will need to ask JLL to help provide the answer asap. On May , , at : PM, \"Belgrad, Doug\" < wrote: Hey guys, Just got off the phone with JLL about the below-referenced positioning for the Playstation in the wake of today's announcement. We're excited to be collaborating with the Alliance and Disney on Uncharted and Bloodshot, both coming out before The Avengers. JLL and the Alliance have made a tremendous impact in terms of funding for the three games (the \"Smurfs\" series, Venom and Sinister Six). There are some exciting new features that these are potential tentpoles in the future: Battle of the Five - A look at our two game plans by Shane Black's team as well as looking at how Activision can benefit from this in developing four separate game based games and whether he is leveraging the technology here. Booming In Productions - We want to do an ambitious and successful pitch to support the vision of our motion picture assets. We feel that this approach will deliver big returns for us. The biggest difference from our pre-COMPANY Pictures strategy was with the release of Rise of Electro, which cost more than CURRENCY million at D (a jump from Budget). We are currently working on many more franchises, including Bond franchise, Dr. Seuss, Batman and more. We're also expanding the Spider-Man universe and expanding the world of DC with a darker tone and feel for Spider-Man. Additionally, we would like to get a sense of what COMPANY can and can't hide and what we're working on to achieve in those three games. Hoping you all have a good weekend. Doug On May , , at : PM, \"Gumpert, Andrew\" < wrote: Believe me, I am looking forward to speaking with you soon. Look forward to talking. mike....... Andrew Gumpert President, Worldwide Business Affairs & Operations COMPANY Telephone: Facsimile: E-Mail: PRIVACY NOTICE: ument by mistake, please e-mail the sender securely dispose of it.\n",
    "--------------------\n",
    "Software Development Team Email (without pipeline):\n",
    "Brief project update email for a software development team, please contact me. I look forward to hearing from you. If you have any questions, you can reach me at . I will be in meetings this afternoon. Thanks so much! Best regards, Shelly SHALINIE BERMAN | CONTACT | SOLUTIONS INC | P: | F: () Email: Please see attached draft correspondence from the SEC on file sharing. We have also discussed the possibility of sending around an email that shows marked \"Full Schedule\". We are scheduled to meet as soon as possible, but would love to have you guys review. My team has also gotten some comments from Kaz Hirai and want to make sure this is covered as well. Thank you all very much for your cooperation. Best, - Sharon > > > _____ This e-mail and any files transmitted with it are intended solely for the use of the individual or entity to whom they are addressed. This e mai has been sent to the attention of only the named addressee(s). If the reader of this e mail is not the intended recipient or the employee or agent responsible for delivering the message to thi you are hereby notified that any use dissemination, forwarding, printing or copying of th e com e of any sort is strictly prohibited. Any views or opinions expressed are solely those of sakethe only author and do not necessarily represent those or should necessarily be taken as an excuse. e ___________________________________________________________ SUBSCRIBE TO OUR AWARD WINNING NEWSLETTERS TRAVEL TIPS, THE WINE LIST AND THE DAILY FOOD & Wine THE DISH NUM\n",
    "--------------------\n",
    "Financial Report Request Email (without pipeline):\n",
    "Detailed financial report request by a senior analyst that includes related companies. The analyst estimates that the analyst's annual risk forecast will be CURRENCY billion by July S. The analyst believes this year will reach C- million. As such, it was profitable to report that number from NUMABS and other non-financial companies while also operating as margin and basis. He estimated that COMPANY may see a net loss of CMM, which would hit COMPAE, or C/b. If all alternatives are not feasible, then he expects his team to continue through NUMP and report to each of them this November. After that, management thinks the stock should close to its October start date. And COMPENSENSON is preparing additional capital expenditure in the coming months, according to Tomn Bersch. Berszch was also informed last year that at the end of FYNUMBIRING THE SECURITY RANGE, NUMRBIO would require more capital, because of this, had COMPensenRON's business practices reported on their website last September. That was more accurate than the sum sheet in October that came out next week. His team made clear they did not want a \"crashed\" release as a result. At the same time, Berszlch also admitted that some people have assumed the public can view COMPenron's assets in a certain light. Below is his thoughts. According to an internal memo sent to me by the SEC this afternoon, no one from COMP's group has responded via email to us with the numbers that were not included in our analysis. I think what I will assume, however, is that we will put on solid media tomorrow. Hopefully that nothing leaks before Friday. To my credit, you probably already know that there are several possible investors this fiscal year - including several senior COMPanyer named as potential investment bankers. All of the above is considered a necessary investment. However, in order to gain a timely and accurate portrayal of our overall portfolio, we are going to rely heavily on media coverage throughout the process. When we do look at what media is covering, I believe that this proceeds will likely flow as we work closely with both sides to understand what the opportunities are and what types of questions we're looking for;\n",
    "--------------------\n",
    "Marketing Campaign Creative Brief (without pipeline):\n",
    "Creative brief for a new marketing campaign targeting young adults + -Minute TV Spot for First Time ABC-Disney's 'The Walking Dead' (Video) o It looks like ABC has the right to promote the show on a big screen. Here are a few highlights: The Young Adult Swim, Disney Channel and ABC Family were among the top rated networks in the last year of its second season Digital TV Spots for st Century Fox, NBCU Cable, FOX Sports Network and Yahoo! Television announced last week that they will be joining the network along with their networks' digital ad network, Nickelodeon and VHNUMB. \"The idea of creating video content in order to drive revenue through the process,\" said Doug Belgrad, president of worldwide commercial for COMPANY. \"This is the first time in our history that a network could have an effect on ad viewing. \" In addition, AOL launched video streaming service VuduStream in February. The service allows subscribers to watch live sports videos on their mobile devices. Vubu is a major player in that industry, bringing in more than NUMB of pay television subscribers from around the world to view and comment on content across their homes. Ad-supported services also allow viewers to stream live sporting events across multiple platforms. About VUDuSource presents a powerful advertising strategy. For more information please click here. Contact Lisa Woo at to schedule a phone interview or email. This e-mail and any files transmitted with it are intended solely for the use of the individual or entity to whom they are addressed. If the reader of this e E-Mail is not the intended recipient or the employee or agent responsible for delivering the message to the original recipient, you are hereby notified that any use dissemination, forwarding, printing or copying of thi Attachments: [imageNUMb. A Letter from the Publisher and CEO of Vodu Digital Distribution, Inc. vid. This electronic file (including any attachments) contains information concerning VOD and/or electronic sell-through (EST), content protection and other technology related to VAN. IF YOU HAVE RECEIVED THIS COMMUNICATION IN ERROR, PLEASE NOTIFY US IMMEDIATELY BY TELEPHONING THE ORIGINAL COMMUNITY AND DESTROY ALL COPIES, BOTH ELECTRONIC AND OTHER, OF\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate text using the pipeline\n",
    "# email_text1 = generate_email(\"Hello, Following up on the bubblegum shipment.\", use_pipeline=True)\n",
    "# email_text2 = generate_email(\"Please confirm the delivery date for our next order.\", use_pipeline=True)\n",
    "# email_text3 = generate_email(\"Can you update me on the status of the invoice?\", use_pipeline=True)\n",
    "\n",
    "# # Print the generated emails\n",
    "# print(email_text1)\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(email_text2)\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(email_text3)\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Pipeline Output\n",
    "\n",
    "Hello, Following up on the bubblegum shipment.\n",
    "I'll send you the info.\n",
    "Hope you are well.\n",
    "Best,\n",
    "Janeh.\n",
    "Janeh.\n",
    "On Tue, Feb ,  at : AM, Cavanaugh, Kristin < wrote:Hi Janeh,I hope you are well.I spoke to Tom Rothman and he said he'd give me a call regarding your order, I told him I wanted to talk to you about your order and the order and I said I would send you a separate email that was so that I could communicate the order.I don't know if you had a chance to look at it and he suggested I send it to him directly. I've enclosed the order and my contact info for your reference.Thank you.Best,KristinErnest,Thank you for purchasing your itemPlease note that it has arrived in your carrier.\n",
    "The items listed in this shipment are:x x x inch-wide x inch-wide x inch-wide x inch-wide x inch-wide x inch-wide\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n",
    "Please confirm the delivery date for our next order.\n",
    "If you have any questions, please contact the Global Service Desk or your Local IT Representative.\n",
    "Global Service Desk Contact Information:\n",
    "North America: \n",
    "US Toll Free: -SPE-SONY\n",
    "Europe: ()-International Toll Fee Numbers\n",
    "OnNet: - or -\n",
    "\n",
    "GSD Live Chat\n",
    "Regards,\n",
    "SPE Identity Management\n",
    "MP/WPF\n",
    "ASAP\n",
    "CRB(Competitive Releases)\n",
    "DICER\n",
    "FCM\n",
    "GPMS - CopyRight\n",
    "GPMS - MAGIC\n",
    "GPMS - SCRY\n",
    "GPMS - Titles and Registration\n",
    "IntSales\n",
    "Motion Pictures Portal\n",
    "Script Tracker\n",
    "SpiritWorld\n",
    "Superbad\n",
    "Worldwide Print Tracking System (WPTS)\n",
    "Worldwide Publicity Website\n",
    "Productions\n",
    "C\n",
    "Calypso\n",
    "Dropzone\n",
    "GPAS\n",
    "Motion Pictures Production Database(MPPDB)\n",
    "Tview\n",
    "TV\n",
    "BB\n",
    "CC\n",
    "Carmen\n",
    "DealTracker\n",
    "Dr. Oz\n",
    "DTSM\n",
    "ITSM/SARA\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n",
    "Can you update me on the status of the invoice?\n",
    "If not, please let me know.\n",
    "Thanks,\n",
    "Larry\n",
    "Larry Marino\n",
    "\n",
    "On Apr , , at : AM, Larry Marino < wrote:\n",
    "Hi Larry,\n",
    "Jane was hoping to see if we could get an invoice from ECS for this month's sales tax. We are working on a deal to make this work; the invoice is supposed to be CURRENCY,. I'm sorry but we won't be able to get a hard copy.\n",
    "Thanks again!\n",
    "Larry Marino\n",
    "\n",
    "On Apr , , at : AM, Larry Marino < wrote:\n",
    "Thanks Larry,\n",
    "I hope that you are well.\n",
    "I wanted to follow up with you briefly regarding the payment for the June invoice. I have received an invoice from ECS to make certain that we were billed by ECS for this year.\n",
    "The invoice is supposed to be CURRENCY,.\n",
    "The invoice is for May  through  and the invoice will be CURRENCY,.\n",
    "Thanks again,\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Detailed Output\n",
    "\n",
    "Laptop Male Software Engineer Cycling Club\n",
    "URL URL URL URL\n",
    "_____\n",
    "This e-mail message is intended only for the individual or entity to which it is addressed and may contain information that is privileged, confidential, or exempt from disclosure under applicable Federal or State law. If the reader of this e-mai-mail is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipients, you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n",
    "Mobile Female Data Scientist Hiking in the US\n",
    "URL\n",
    "--URL This message was sent by: BBC Worldwide Americas\n",
    " Avenue of the Americas New York, NY, \n",
    "This email is to provide you with a personal digital file of all emails from BBC Worldwide into your inbox. If you prefer not to continue receiving email communications, please unsubscribe here instead of replying to this email.\n",
    "To update your profile and customize what email alerts and newsletters you receive, please click here.\n",
    "Having\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n",
    "Desktop Male Graphic Designer Painting -.\n",
    "URL URL URL URL #\n",
    ">\n",
    "> -Houzz Logo\n",
    "> //RESERVATION STAMPED HANDS AND LINE BLUES CURRENCY.\n",
    "Image Credit: Kiki Bentonka\n",
    "This is a photo message from my Houzz ideabook.\n",
    "If you are having trouble viewing this, click here.\n",
    "Share This: URL URL\n",
    "URL\n",
    "Thank You,\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1 Mode Output\n",
    "\n",
    "<!-- # Laptop Male Software Engineer Cycling Team\n",
    "# <NUMBER>-<PHONE>\n",
    "# Email: <EMAIL> | Twitter: @Cycling_Team\n",
    "# Cyclists are invited to participate in the Cycling Cycling World Cup in Brazil.\n",
    "# The Cycling Federation of Brazil is a global organization dedicated to promoting cycling \n",
    "# and the development of sustainable living. Cycling is an international organization that promotes the health, \n",
    "# safety and well-being of all people. The Cycling Foundation is dedicated solely to the pursuit of the highest quality \n",
    "# and quality of life -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
