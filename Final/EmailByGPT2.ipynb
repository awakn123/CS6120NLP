{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.7.4)\n",
      "Requirement already satisfied: evaluate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge_score in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (2.6.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: responses<0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch scikit-learn accelerate spacy evaluate nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('LightTai/personalized-email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['product', 'gender', 'profession', 'hobby', 'email'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "})\n",
      "{'product': ['piano lessons', 'guitar lessons', 'vacation plans', 'vacation plans', 'vacation plans'], 'gender': ['male', 'male', 'male', 'female', 'female'], 'profession': ['college students', 'college students', 'college students', 'college students', 'company employees'], 'hobby': ['like to play piano', 'like to play piano', 'like swimming', 'like to look at the scenery', 'like to look at the scenery'], 'email': [\"Subject: Elevate Your Piano Skills - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nLooking to unlock your piano potential? As a fellow male college student and a passionate piano player, I understand your love for music. That's why I'm thrilled to offer you exclusive piano lessons designed to fit your busy student schedule.\\n\\nMaster your favorite melodies, refine techniques, and gain a deeper understanding of music theory-all while enjoying a flexible lesson plan tailored to your goals. Let's embark on this musical journey together!\\n\\nLimited slots available, so act fast! Reply now for more details and an irresistible offer.\\n\\nKeep playing and shining!\\n\\n[Your Name]\", \"Subject: Expand Your Musical Horizons - Exclusive Guitar Lessons Offer!\\n\\nHey [Name],\\n\\nLooking to broaden your musical repertoire? As a male college student who enjoys playing the piano, I have an exciting opportunity for you. Discover the magic of guitar with our exclusive lessons tailored specifically for college students like you.\\n\\nUnleash your creativity, learn popular chords, and strum your favorite tunes in no time. Our flexible schedule ensures your studies won't miss a beat, and our experienced instructors will guide you every step of the way.\\n\\nLimited spaces available, so secure your spot now! Reply for more details and an exclusive offer that'll have you rocking the guitar in record time.\\n\\nStrumming success awaits!\\n\\n[Your Name]\", \"Subject: Dive into the Perfect Getaway - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nReady to make a splash on your next vacation? As a fellow male college student who loves swimming, I've got the perfect getaway plan for you. Imagine crystal-clear waters, vibrant marine life, and sandy beaches-the ultimate aquatic paradise!\\n\\nOur specially curated vacation package combines your passion for swimming with unforgettable adventures. Dive into pristine oceans, explore hidden coves, and unwind under the warm sun. It's the ultimate escape from college stress!\\n\\nLimited spots available, so don't miss out on this aquatic adventure. Reply now for more details and an exclusive offer that'll make your swimmer's heart leap!\\n\\nMake a splash!\\n\\n[Your Name]\", \"Subject: Escape to Enchanting Landscapes - Exclusive Offer Inside!\\n\\nHey [Name],\\n\\nCraving an awe-inspiring adventure? As a fellow female college student who appreciates breathtaking scenery, I have the perfect vacation plan for you. Immerse yourself in captivating landscapes, from lush valleys to majestic mountains, and create unforgettable memories.\\n\\nOur carefully curated vacation package combines stunning natural beauty with thrilling activities that cater to your adventurous spirit. Capture picture-perfect moments, hike scenic trails, and soak in the serenity of nature.\\n\\nLimited spots available, so don't miss out on this enchanting journey. Reply now for more details and an exclusive offer that will transport you to the most picturesque destinations.\\n\\nUnleash your wanderlust!\\n\\n[Your Name]\", \"Subject: Escape the Daily Grind - Exclusive Vacation Offer Inside!\\n\\nHey [Name],\\n\\nReady for a rejuvenating getaway? As a female professional who appreciates scenic beauty, I have an irresistible vacation plan tailored just for you and your colleagues. Step away from the office and immerse yourselves in breathtaking landscapes that will recharge your spirits.\\n\\nIndulge in mesmerizing vistas, explore hidden gems, and unwind amidst nature's wonders. Our curated package offers a perfect blend of relaxation and adventure, creating unforgettable memories for your team.\\n\\nLimited availability, so don't miss out on this exclusive offer. Reply now for more details and start planning a revitalizing escape from the corporate hustle.\\n\\nRediscover serenity!\\n\\n[Your Name]\"]}\n",
      "          product  gender         profession                        hobby  \\\n",
      "0   piano lessons    male   college students           like to play piano   \n",
      "1  guitar lessons    male   college students           like to play piano   \n",
      "2  vacation plans    male   college students                like swimming   \n",
      "3  vacation plans  female   college students  like to look at the scenery   \n",
      "4  vacation plans  female  company employees  like to look at the scenery   \n",
      "\n",
      "                                               email  \n",
      "0  Subject: Elevate Your Piano Skills - Exclusive...  \n",
      "1  Subject: Expand Your Musical Horizons - Exclus...  \n",
      "2  Subject: Dive into the Perfect Getaway - Exclu...  \n",
      "3  Subject: Escape to Enchanting Landscapes - Exc...  \n",
      "4  Subject: Escape the Daily Grind - Exclusive Va...  \n",
      "(30, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['train'][:5])\n",
    "df = dataset['train'].to_pandas()\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f35f13677f9413cb0f1ccf0e920055c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c68fbd2a9cc4064830d205484df704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "model_checkpoint = \"postbot/distilgpt2-emailgen-V2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    features = [f\"{prod} {gen} {prof} {hob}\" \n",
    "                for prod, gen, prof, hob in zip(examples[\"product\"], \n",
    "                                                examples[\"gender\"], \n",
    "                                                examples[\"profession\"], \n",
    "                                                examples[\"hobby\"])]\n",
    "    tokenized_inputs = tokenizer(features, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Tokenize the email column which is our target\n",
    "    tokenized_targets = tokenizer(examples[\"email\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    tokenized_inputs['labels'] = tokenized_targets['input_ids']  # Assign target token ids as labels for training\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's already loaded\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the DataFrames back to Hugging Face dataset format if necessary\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n",
    "# Assuming 'dataset' is a Hugging Face 'datasets' object\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"product\", \"gender\", \"profession\", \"hobby\", \"email\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 24\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__index_level_0__': 24,\n",
       " 'input_ids': [6966,\n",
       "  27757,\n",
       "  4257,\n",
       "  4409,\n",
       "  3504,\n",
       "  12,\n",
       "  1886,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [19776,\n",
       "  25,\n",
       "  2295,\n",
       "  46565,\n",
       "  3406,\n",
       "  7326,\n",
       "  1704,\n",
       "  532,\n",
       "  35651,\n",
       "  10437,\n",
       "  27757,\n",
       "  33085,\n",
       "  14384,\n",
       "  0,\n",
       "  198,\n",
       "  198,\n",
       "  10814,\n",
       "  685,\n",
       "  5376,\n",
       "  4357,\n",
       "  198,\n",
       "  198,\n",
       "  1722,\n",
       "  257,\n",
       "  6563,\n",
       "  290,\n",
       "  30511,\n",
       "  3504,\n",
       "  12,\n",
       "  1886,\n",
       "  4257,\n",
       "  4708,\n",
       "  11,\n",
       "  314,\n",
       "  423,\n",
       "  281,\n",
       "  8568,\n",
       "  2897,\n",
       "  326,\n",
       "  24538,\n",
       "  534,\n",
       "  3748,\n",
       "  45581,\n",
       "  13,\n",
       "  3954,\n",
       "  15313,\n",
       "  1627,\n",
       "  286,\n",
       "  36432,\n",
       "  318,\n",
       "  3562,\n",
       "  284,\n",
       "  9494,\n",
       "  534,\n",
       "  3288,\n",
       "  3033,\n",
       "  290,\n",
       "  5750,\n",
       "  534,\n",
       "  2116,\n",
       "  12,\n",
       "  39745,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  44596,\n",
       "  257,\n",
       "  3094,\n",
       "  2837,\n",
       "  286,\n",
       "  8683,\n",
       "  41836,\n",
       "  11,\n",
       "  1341,\n",
       "  1939,\n",
       "  533,\n",
       "  11,\n",
       "  290,\n",
       "  8737,\n",
       "  3186,\n",
       "  27571,\n",
       "  284,\n",
       "  262,\n",
       "  2476,\n",
       "  286,\n",
       "  15345,\n",
       "  1450,\n",
       "  13,\n",
       "  2295,\n",
       "  46565,\n",
       "  35985,\n",
       "  32126,\n",
       "  326,\n",
       "  31219,\n",
       "  680,\n",
       "  11,\n",
       "  46834,\n",
       "  378,\n",
       "  11,\n",
       "  290,\n",
       "  9494,\n",
       "  534,\n",
       "  5585,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  37214,\n",
       "  11500,\n",
       "  11,\n",
       "  523,\n",
       "  836,\n",
       "  470,\n",
       "  2051,\n",
       "  503,\n",
       "  319,\n",
       "  428,\n",
       "  8568,\n",
       "  2897,\n",
       "  13,\n",
       "  14883,\n",
       "  783,\n",
       "  329,\n",
       "  517,\n",
       "  3307,\n",
       "  290,\n",
       "  36830,\n",
       "  534,\n",
       "  41836,\n",
       "  8027,\n",
       "  351,\n",
       "  674,\n",
       "  40123,\n",
       "  4947,\n",
       "  286,\n",
       "  36432,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  10161,\n",
       "  46565,\n",
       "  534,\n",
       "  6628,\n",
       "  11,\n",
       "  42241,\n",
       "  0,\n",
       "  198,\n",
       "  198,\n",
       "  58,\n",
       "  7120,\n",
       "  6530,\n",
       "  60,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "rouge = evaluate.load('rouge')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits from numpy arrays to PyTorch tensors\n",
    "    logits_tensor = torch.tensor(logits)\n",
    "    \n",
    "    # Convert logits to predicted token IDs using argmax\n",
    "    predictions_ids = torch.argmax(logits_tensor, dim=-1)\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    predictions = tokenizer.batch_decode(predictions_ids, skip_special_tokens=True)\n",
    "    # Assuming labels are already decoded; if not, decode them similarly\n",
    "    references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Compute ROUGE scores\n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_predictions = tokenized_train_dataset[1][\"input_ids\"]\n",
    "# test_references = tokenized_train_dataset[1][\"labels\"]\n",
    "# test_eval_pred = (test_predictions, test_references)\n",
    "# rouge_results = compute_metrics(test_eval_pred)\n",
    "# print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe774c8f3c243649c5729866bab4689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abac63a773b40db9b8ca4b26d985c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.660834789276123, 'eval_rouge1': 0.021925645041699898, 'eval_rouge2': 0.0, 'eval_rougeL': 0.021110100532562275, 'eval_rougeLsum': 0.02119088841883852, 'eval_runtime': 57.5775, 'eval_samples_per_second': 0.104, 'eval_steps_per_second': 0.035, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9993076ea2b84ded98ac137d131b2814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.186565399169922, 'eval_rouge1': 0.028891390452366064, 'eval_rouge2': 0.0027548209366391185, 'eval_rougeL': 0.028953425191820155, 'eval_rougeLsum': 0.02588916459884202, 'eval_runtime': 48.7059, 'eval_samples_per_second': 0.123, 'eval_steps_per_second': 0.041, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc525e89d2c41a1a5c9aae297aedcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.7408928871154785, 'eval_rouge1': 0.01988552732612043, 'eval_rouge2': 0.0, 'eval_rougeL': 0.01814900511746635, 'eval_rougeLsum': 0.019059697109619012, 'eval_runtime': 74.3375, 'eval_samples_per_second': 0.081, 'eval_steps_per_second': 0.027, 'epoch': 3.0}\n",
      "{'train_runtime': 2164.9761, 'train_samples_per_second': 0.033, 'train_steps_per_second': 0.008, 'train_loss': 5.988736046685113, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.json',\n",
       " './saved_model/merges.txt',\n",
       " './saved_model/added_tokens.json',\n",
       " './saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "\t\tcompute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer after training is complete\n",
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48e0a0f4bc44696ba79b695e35b7df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 85.27\n",
      "{'eval_loss': 4.4457783699035645, 'eval_rouge1': 0.0393718765311122, 'eval_rouge2': 0.0005324813631522896, 'eval_rougeL': 0.03623211660950514, 'eval_rougeLsum': 0.03342282173771027, 'eval_runtime': 51.5, 'eval_samples_per_second': 0.117, 'eval_steps_per_second': 0.039, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"24NLPGroupO/EmailGeneration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'use_fast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m24NLPGroupO/EmailGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Set up the generation pipeline\u001b[39;00m\n\u001b[1;32m     10\u001b[0m generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-env/lib/python3.8/site-packages/transformers/modeling_utils.py:2876\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2876\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'use_fast'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load saved model and tokenizer\n",
    "model_checkpoint = \"24NLPGroupO/EmailGeneration\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, truncation=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Set up the generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def clean_generated_text(text):\n",
    "    # Basic cleaning\n",
    "    text = re.sub(r'^(Re:|Fwd:)', '', text)  # Remove reply and forward marks\n",
    "    text = re.sub(r'Best regards,.*$', '', text, flags=re.DOTALL)  # Remove everything after signature\n",
    "    text = re.sub(r'PHONE.*$', '', text, flags=re.DOTALL)  # Remove everything after phone numbers\n",
    "    text = re.sub(r'Email:.*$', '', text, flags=re.DOTALL)  # Remove everything after email addresses\n",
    "    text = re.sub(r'Cc:.*$', '', text, flags=re.DOTALL)  # Remove CC list\n",
    "    text = re.sub(r'\\* Attachments:.*', '', text, flags=re.S)  # Remove 'Attachments:' and everything following it\n",
    "    text = re.sub(r'©️ .*$', '', text, flags=re.DOTALL)  # Remove copyright and ownership statements\n",
    "    text = re.sub(r'URL If this message is not displaying properly, click here.*$', '', text, flags=re.DOTALL)  # Remove error display message and links\n",
    "    text = re.sub(r'\\d{5,}', 'NUMBER', text)  # Replace long sequences of numbers, likely phone numbers or ZIP codes\n",
    "    return text.strip()\n",
    "\n",
    "def generate_email(product, gender, profession, hobby):\n",
    "    input_text = f\"{product} {gender} {profession} {hobby}\"\n",
    "    result = generator(\n",
    "        input_text,                 # Initial text to prompt the model. Sets the context or topic for text generation.\n",
    "        max_length=256,             # Maximum length of the generated text in tokens, limiting the output size.\n",
    "        do_sample=True,             # Enables stochastic sampling; the model can generate diverse outputs at each step.\n",
    "        top_k=20,                   # Limits the vocabulary considered at each step to the top-k most likely next words.\n",
    "        top_p=0.6,                  # Uses nucleus sampling: Narrows down to the smallest set of words totaling 60% of the likelihood.\n",
    "        temperature=0.4,            # Scales logits before sampling to reduce randomness and produce more deterministic output.\n",
    "        repetition_penalty=1.5,     # Penalizes words that were already mentioned, reducing repetition in the text.\n",
    "        # truncation=True,            # Truncates the output to the maximum length if it exceeds it.\n",
    "        num_return_sequences=3      # Generates three different sequences to choose from, enhancing output variety.\n",
    "        ) \n",
    "    # Select the best output from the generated sequences\n",
    "    best_text = sorted([clean_generated_text(r['generated_text']) for r in result], key=len)[-1]\n",
    "    return best_text\n",
    "\n",
    "# Example parameters and generation\n",
    "product = \"Laptop\"\n",
    "gender = \"Male\"\n",
    "profession = \"Software Engineer\"\n",
    "hobby = \"Gaming\"\n",
    "\n",
    "email_text = generate_email(product, gender, profession, hobby)\n",
    "print(\"Generated Email:\")\n",
    "print(email_text)\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Email:\n",
    "Laptop Male Software Engineer Gaming Technology - COMPANY\n",
    "Experience the power of mobile devices and make sure you are comfortable with your smartphone.\n",
    "Mobile device Management is a key component to achieving success in our industry, helping us achieve breakthroughs for business leaders by leveraging the best-in-class experience available. We have been working closely together on numerous projects that will benefit from this evolution into an integrated marketing tool used throughout all aspects including media creation & distribution; digital content development/retail fulfillment (FOD), social networking forums such as Twitter, Facebook etc.; strategic communications solutions like Magento CMS or even SAPPRFT products.\n",
    "We would be honored if you could join us at one another's pace! Please let me know what time works best for everyone else who needs it most especially during their busy schedules. Thank you so much for considering this request. I look forward to hearing from you soon. Best regards, Karen Kondo | Senior Vice President Corporate Social Responsibility | COMPANY Corporation of America NUMBER Madison Avenue, Room NUMBERSNUMBERCulver City, CA NUMBER( PHONE| EMAIL)\n",
    "Attachments:\n",
    "x (PHONE Bytes)\n",
    "ATTNUMBER.htm (\"\");<EMAIL\n",
    "X (PHONE By\n",
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Email:\n",
    "Laptop Male Software Engineer Gaming Technology - COMPANY Corporation of America (NYSE: SNE)\n",
    "Experience the power and excitement that drives gaming, social games & entertainment. Experience your personalized experiences with a variety of mobile devices from anywhere in our network including tablets to smartphones, game consoles, plasma TVs, Blu-ray players etc.. The experience is immersive so you can experience all aspects at one time while exploring other technologies. You will also be able to access exclusive content such as Pandora's World which allows users to listen directly on their phones or watch videos instantly on any device. We are confident this will help us achieve more than NUMBER million people worldwide who have already purchased product via apps like Facebook Messenger, Twitter API for Android and Windows Phone/iPad Air Connectivity. For more information please visit URL\n",
    "URL logo\n",
    "Questions? Call Igor Gruppin PHONE Fax +PHONE EMAIL\n",
    "To contact Member Services | View Our Privacy Statement | Unsubscribe\n",
    "©️NUMBER Microsoft Corporation. All rights reserved. | Acceptable Use Policy | Contact Customer Service | NUMBER-HOUR CONTROL CENTER| USA\n",
    "This email was sent by: Microsoft Corporation\n",
    "NUMBER Marta St., Suite NUMBERSan Diego del Norte, CA NUM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load Spacy's English tokenizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set a random seed for reproducible results\n",
    "set_seed(42)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_checkpoint = \"./saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, padding_side='left')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Set up the pipeline using the freshly trained model and tokenizer\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def advanced_text_cleaning(text):\n",
    "    # Use Spacy to parse sentences and filter out overly short fragments and placeholders\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = []\n",
    "    for sent in doc.sents:\n",
    "        # Filtering out too short sentences and placeholders\n",
    "        if len(sent.text) > 20 and not re.search(r'NUMB[RL]|NUMBE[RSZ]', sent.text):\n",
    "            cleaned_text.append(sent.text.replace('\\n', ' ').strip())\n",
    "    return ' '.join(cleaned_text)\n",
    "\n",
    "def clean_generated_text(text):\n",
    "    return advanced_text_cleaning(text.strip())\n",
    "\n",
    "def generate_email(product, gender, profession, hobby, use_pipeline=False):\n",
    "    # 构建输入文本\n",
    "    input_text = f\"As a {gender.lower()} {profession} interested in {hobby}, I am looking for a {product} that suits my needs.\"\n",
    "\n",
    "    if use_pipeline:\n",
    "        result = generator(input_text, max_length=1024, do_sample=True, top_k=50, temperature=0.9, repetition_penalty=1.1, truncation=True)\n",
    "        generated_text = result[0]['generated_text']\n",
    "    else:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=1024,  # Increase maximum length\n",
    "            temperature=0.9,  # Higher diversity\n",
    "            top_k=50,         # Broader vocabulary choice\n",
    "            top_p=0.95,       # Nucleus sampling\n",
    "            no_repeat_ngram_size=2,  # Allow minimal repetition\n",
    "            repetition_penalty=1.1   # Slightly less repetition penalty\n",
    "        )\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return clean_generated_text(generated_text)\n",
    "\n",
    "\n",
    "# 示例参数\n",
    "product = \"Laptop\"\n",
    "gender = \"Male\"\n",
    "profession = \"Software Engineer\"\n",
    "hobby = \"Gaming\"\n",
    "\n",
    "# 生成电子邮件\n",
    "email_text = generate_email(product, gender, profession, hobby, use_pipeline=True)\n",
    "print(\"Generated Email:\")\n",
    "print(email_text)\n",
    "print(\"--------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic cleanup: remove redundant phrases and unwanted tokens\n",
    "    # text = re.sub(r'\\b(URL)\\b\\s*', '', text)  # Remove 'URL' placeholders\n",
    "    # text = re.sub(r'\\b(\\d{1,2}[:]\\d{2}\\s*(AM|PM))\\b', '', text)  # Remove standalone time\n",
    "    # text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citation-like numbers\n",
    "    # text = re.sub(r\"NUMBER|EMAIL|PHONE|FAX\", \"\", text)\n",
    "    # text = re.sub(r\"\\[.*?\\]|-\\s*-\\s*|<.*?>\", \"\", text)  # Clean up brackets and dashed lines\n",
    "    # text = re.sub(r'\\s{2,}', ' ', text)  # Replace multiple spaces with a single space\n",
    "    # text = re.sub(r'\\* Attachments:.*', '', text, flags=re.S)  # 删除'Attachments:'及其后的所有内容\n",
    "    # text = re.sub(r'\\bAttachments:\\s*(\\( Bytes\\)\\s*)+(x\\s*\\( Bytes\\)\\s*)*', '', text, flags=re.S)\n",
    "    # text = re.sub(r'\\|\\s.*', '', text)  # Remove suffixes after names # This regex removes everything after a '|' character"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用pipeline生成文本\n",
    "custom_text1_with_pipeline = generate_email(\"Brief project update email for a software development team\", use_pipeline=True)\n",
    "custom_text2_with_pipeline = generate_email(\"Detailed financial report request by a senior analyst\", use_pipeline=True)\n",
    "custom_text3_with_pipeline = generate_email(\"Creative brief for a new marketing campaign targeting young adults\", use_pipeline=True)\n",
    "\n",
    "# 不使用pipeline生成文本\n",
    "custom_text1_without_pipeline = generate_email(\"Brief project update email for a software development team\", use_pipeline=False)\n",
    "custom_text2_without_pipeline = generate_email(\"Detailed financial report request by a senior analyst\", use_pipeline=False)\n",
    "custom_text3_without_pipeline = generate_email(\"Creative brief for a new marketing campaign targeting young adults\", use_pipeline=False)\n",
    "\n",
    "# 打印使用pipeline生成的文本\n",
    "print(\"Software Development Team Email (with pipeline):\")\n",
    "print(custom_text1_with_pipeline)\n",
    "print(\"--------------------\")\n",
    "print(\"Financial Report Request Email (with pipeline):\")\n",
    "print(custom_text2_with_pipeline)\n",
    "print(\"--------------------\")\n",
    "print(\"Marketing Campaign Creative Brief (with pipeline):\")\n",
    "print(custom_text3_with_pipeline)\n",
    "print(\"--------------------\")\n",
    "\n",
    "# 打印不使用pipeline生成的文本\n",
    "print(\"Software Development Team Email (without pipeline):\")\n",
    "print(custom_text1_without_pipeline)\n",
    "print(\"--------------------\")\n",
    "print(\"Financial Report Request Email (without pipeline):\")\n",
    "print(custom_text2_without_pipeline)\n",
    "print(\"--------------------\")\n",
    "print(\"Marketing Campaign Creative Brief (without pipeline):\")\n",
    "print(custom_text3_without_pipeline)\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Verison 3\n",
    "\n",
    "Software Development Team Email \n",
    "Brief project update email for a software development team meeting this weekend was provided below in advance of Tuesday's meeting. We understand that the schedule for this meeting has changed. I wanted to get everyone an update on our progress. Please provide your feedback by this Thursday, May th after we have been completed and finalized. I wish you all the best in the best possible future. Best regards, Maxene A. Verma\n",
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Verison 2\n",
    "\n",
    "Software Development Team Email \n",
    "Brief project update email for a software development team meeting this weekend was provided below in advance of Tuesday's meeting. We understand that the schedule for this meeting has changed. I wanted to get everyone an update on our progress. Please provide your feedback by this Thursday, May th after we have been completed and finalized. I wish you all the best in the best possible future. Best regards, Maxene A. Verma | Marketing & Research Paralegal | COMPANY Pictures Releasing International, West Washington Blvd., Thalberg Bldg. | Culver City, CA ( | |\n",
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Verison 1\n",
    "\n",
    "Software Development Team Email \n",
    "Brief project update email for a software development team meeting this weekend was provided below in advance of Tuesday's meeting. We understand that the schedule for this meeting has changed. I wanted to get everyone an update on our progress. Please provide your feedback by this Thursday, May th after we have been completed and finalized. I wish you all the best in the best possible future. Best regards, Maxene A. Verma | Marketing & Research Paralegal | COMPANY Pictures Releasing International, West Washington Blvd., Thalberg Bldg. | Culver City, CA ( | | * Attachments: ( Bytes) x ( Bytes) x ( Bytes) x ( Bytes) ( Bytes) x ( Bytes) ( Bytes) x ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) x ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) m ( Bytes) x ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) Each individual must participate in the \"Join Together\" component of the Round Up Process. The Rules will be reviewed with the Rules by the Committee in partnership with the Chamber. Please contact me if you have any questions about these materials. Regards, Maxene A. Verma | Marketing & Research Paralegal | COMPANY Pictures Releasing International, West Washington Blvd., Thalberg Bldg. | Culver City, CA ( | | * Attachments: x ( Bytes) x ( Bytes) x ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) m ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) ( Bytes) (\n",
    "--------------------\n",
    "Financial Report Request Email (with pipeline):\n",
    "Detailed financial report request by a senior analyst for a Northwest bank, the company is currently conducting an online survey of its employees and business operations. Our goal is to identify, among other things, companies that have \"met Wall Street's expectations\" or \"the market needs that are needed,\" the information being gathered from the survey should be deemed complete. An objective of the survey was that we would send an electronic resume, either verbal or by email in the next few days. To this end, the deadline for the survey has been extended, starting in March, to December February and ending in April th February. This will take place as follows: EES will conduct the survey using the online questionnaire to date. It will focus on identifying the most market-sceptical \"attributable\" employees. The results will be disclosed on Monday, with the second survey being conducted early next week, and our input will be considered during the second survey in the coming weeks. Regards, COMPANY Williams Associate Director, Public Relations If you are contacted regarding the above survey, please e-mail If you would like to be removed from future survey distribution, please e-mail If you would like to be removed from future surveys, please Please be advised that your direct reports will not be given again until completed. Thank you, Ingrid O'Donnell, Senior Manager, Commercial Support EES Business Development Contact Information You can also send an electronic resume, if you would like to have them taken a look on their qualifications for positions. Sincerely, The Survey Company DataSite Team - Customer Relationship Studies Team - POLITICAL PARTYic National Committee Montgomery Street, Suite LOCATION, Texas USA Telephone: (U) - Fax: (M) - Site Visit Map:<(toll-free):<(filing location: Hyatt Regency, Orange County, CA) Thanks for your participation and participation on the survey! Sincerely, The Survey Company DataSite Team - Customer Relationship Studies Team - POLITICAL PARTYic National Committee Montgomery Street, Suite LOCATION, Texas USA Telephone: (U) - Fax: (M) - Site Visit Map:<(toll-free):<(filing location: Hyatt Regency, Orange County, CA) All e-mails come directly from The Survey Company. In addition to being members of the Company's Executive Team who are not on the Company's Executive Team please consider the environment before printing this e-mail. This message was sent to: If you no longer wish to receive these emails, click on the following link: Unsubscribe. <a style=\"text-align:center!important; width=\"px!important; }.ExternalClass p:link, span.intent, div.us_SignInRow {margin: px ;} .ExternalClass font { font-family:\"Calibri\", \"Verdana\", \"Helvetica\", line-height:px; padding: px!important;} @media only screen and (max-width: px) { *.layout-, *.footer-layout { width:px!important; } *.layout-, *.center-align, *.show-icons,*.one-column, *.mediumgrid-header, *.outer-layout, *.media-layout, *.grid-layout, *.header-layout, *.featureagentads-layout, *.ads-layout, *.housead-layout, *.app, *.subscribe, *.unlimited-access, *.footer, *.access, *.app-class, *.app-span, *.copyright-block, *.view-layout { width:px!important; } *.media-layout { padding: px!important; margin:!important; } *.view-layout { margin: px !important; } *.grid-layout { margin:px px px!important; } *.header-layout\n",
    "--------------------\n",
    "Marketing Campaign Creative Brief (with pipeline):\n",
    "Creative brief for a new marketing campaign targeting young adults and their fans: It's not clear what the plan is for this animated movie from the beginning. ) How to Give an EXTRA EXPENSION of the trailer with the promise of a new brand, it's going to work very well for us - especially as in the new trailer for \"Grown Ups ,\" but we need to have a feeling about how much fun and powerful \"Grown Ups \" has been. We will be discussing this with everyone this weekend. ) The new trailer for Interview # (which is still in production) promises a lot more emotion and action for the film. Cc: Vollack, Lia; Marshall, Bob; Guerin, Jean This wasn't made in-the-works by Mike. They'd also like to give the current joke - an old joke, without any real conflict, to the movie, without the promise that it's a sequel or something. The script (and script) incorporates language I understand from the conversations with James and Phil last week: Regarding Interview # we just wanted you to know that we are committed to working on all the points on the horizontal and vertical artwork to show off COMPANY's upcoming campaign trailer. We want to address a number of concerns namely, if the'story' is used incorrectly, it's no good to use references from the previous trailers. If this isn't clear, then we may want to adjust and clarify what we mean by using some additional language to the story/concept. As we talked on Friday, I'm sending you the following email (from JLL) regarding several instances where we were able to bypass out our own campaign-include one that's more generic. But again, we will need to communicate with JLL about these issues/ concerns so that they don't pop into places where we are literally communicating. If this isn't clear then we may want to go ahead with the other campaign to get us next steps. Hereto be clear: when we talk about the possibility of splitting up the various businesses (Digital, Media Networks, Home Entertainment etc.), we understand the scope of the partnership, but are concerned about different businesses that can serve multiple brands in one fashion or another. We don't want to appear too stupid or wrong, however when we talk about a partnership that could deliver big gains in the long run. Again, this is confidential, and we will need to ask JLL to help provide the answer asap. On May , , at : PM, \"Belgrad, Doug\" < wrote: Hey guys, Just got off the phone with JLL about the below-referenced positioning for the Playstation in the wake of today's announcement. We're excited to be collaborating with the Alliance and Disney on Uncharted and Bloodshot, both coming out before The Avengers. JLL and the Alliance have made a tremendous impact in terms of funding for the three games (the \"Smurfs\" series, Venom and Sinister Six). There are some exciting new features that these are potential tentpoles in the future: Battle of the Five - A look at our two game plans by Shane Black's team as well as looking at how Activision can benefit from this in developing four separate game based games and whether he is leveraging the technology here. Booming In Productions - We want to do an ambitious and successful pitch to support the vision of our motion picture assets. We feel that this approach will deliver big returns for us. The biggest difference from our pre-COMPANY Pictures strategy was with the release of Rise of Electro, which cost more than CURRENCY million at D (a jump from Budget). We are currently working on many more franchises, including Bond franchise, Dr. Seuss, Batman and more. We're also expanding the Spider-Man universe and expanding the world of DC with a darker tone and feel for Spider-Man. Additionally, we would like to get a sense of what COMPANY can and can't hide and what we're working on to achieve in those three games. Hoping you all have a good weekend. Doug On May , , at : PM, \"Gumpert, Andrew\" < wrote: Believe me, I am looking forward to speaking with you soon. Look forward to talking. mike....... Andrew Gumpert President, Worldwide Business Affairs & Operations COMPANY Telephone: Facsimile: E-Mail: PRIVACY NOTICE: ument by mistake, please e-mail the sender securely dispose of it.\n",
    "--------------------\n",
    "Software Development Team Email (without pipeline):\n",
    "Brief project update email for a software development team, please contact me. I look forward to hearing from you. If you have any questions, you can reach me at . I will be in meetings this afternoon. Thanks so much! Best regards, Shelly SHALINIE BERMAN | CONTACT | SOLUTIONS INC | P: | F: () Email: Please see attached draft correspondence from the SEC on file sharing. We have also discussed the possibility of sending around an email that shows marked \"Full Schedule\". We are scheduled to meet as soon as possible, but would love to have you guys review. My team has also gotten some comments from Kaz Hirai and want to make sure this is covered as well. Thank you all very much for your cooperation. Best, - Sharon > > > _____ This e-mail and any files transmitted with it are intended solely for the use of the individual or entity to whom they are addressed. This e mai has been sent to the attention of only the named addressee(s). If the reader of this e mail is not the intended recipient or the employee or agent responsible for delivering the message to thi you are hereby notified that any use dissemination, forwarding, printing or copying of th e com e of any sort is strictly prohibited. Any views or opinions expressed are solely those of sakethe only author and do not necessarily represent those or should necessarily be taken as an excuse. e ___________________________________________________________ SUBSCRIBE TO OUR AWARD WINNING NEWSLETTERS TRAVEL TIPS, THE WINE LIST AND THE DAILY FOOD & Wine THE DISH NUM\n",
    "--------------------\n",
    "Financial Report Request Email (without pipeline):\n",
    "Detailed financial report request by a senior analyst that includes related companies. The analyst estimates that the analyst's annual risk forecast will be CURRENCY billion by July S. The analyst believes this year will reach C- million. As such, it was profitable to report that number from NUMABS and other non-financial companies while also operating as margin and basis. He estimated that COMPANY may see a net loss of CMM, which would hit COMPAE, or C/b. If all alternatives are not feasible, then he expects his team to continue through NUMP and report to each of them this November. After that, management thinks the stock should close to its October start date. And COMPENSENSON is preparing additional capital expenditure in the coming months, according to Tomn Bersch. Berszch was also informed last year that at the end of FYNUMBIRING THE SECURITY RANGE, NUMRBIO would require more capital, because of this, had COMPensenRON's business practices reported on their website last September. That was more accurate than the sum sheet in October that came out next week. His team made clear they did not want a \"crashed\" release as a result. At the same time, Berszlch also admitted that some people have assumed the public can view COMPenron's assets in a certain light. Below is his thoughts. According to an internal memo sent to me by the SEC this afternoon, no one from COMP's group has responded via email to us with the numbers that were not included in our analysis. I think what I will assume, however, is that we will put on solid media tomorrow. Hopefully that nothing leaks before Friday. To my credit, you probably already know that there are several possible investors this fiscal year - including several senior COMPanyer named as potential investment bankers. All of the above is considered a necessary investment. However, in order to gain a timely and accurate portrayal of our overall portfolio, we are going to rely heavily on media coverage throughout the process. When we do look at what media is covering, I believe that this proceeds will likely flow as we work closely with both sides to understand what the opportunities are and what types of questions we're looking for;\n",
    "--------------------\n",
    "Marketing Campaign Creative Brief (without pipeline):\n",
    "Creative brief for a new marketing campaign targeting young adults + -Minute TV Spot for First Time ABC-Disney's 'The Walking Dead' (Video) o It looks like ABC has the right to promote the show on a big screen. Here are a few highlights: The Young Adult Swim, Disney Channel and ABC Family were among the top rated networks in the last year of its second season Digital TV Spots for st Century Fox, NBCU Cable, FOX Sports Network and Yahoo! Television announced last week that they will be joining the network along with their networks' digital ad network, Nickelodeon and VHNUMB. \"The idea of creating video content in order to drive revenue through the process,\" said Doug Belgrad, president of worldwide commercial for COMPANY. \"This is the first time in our history that a network could have an effect on ad viewing. \" In addition, AOL launched video streaming service VuduStream in February. The service allows subscribers to watch live sports videos on their mobile devices. Vubu is a major player in that industry, bringing in more than NUMB of pay television subscribers from around the world to view and comment on content across their homes. Ad-supported services also allow viewers to stream live sporting events across multiple platforms. About VUDuSource presents a powerful advertising strategy. For more information please click here. Contact Lisa Woo at to schedule a phone interview or email. This e-mail and any files transmitted with it are intended solely for the use of the individual or entity to whom they are addressed. If the reader of this e E-Mail is not the intended recipient or the employee or agent responsible for delivering the message to the original recipient, you are hereby notified that any use dissemination, forwarding, printing or copying of thi Attachments: [imageNUMb. A Letter from the Publisher and CEO of Vodu Digital Distribution, Inc. vid. This electronic file (including any attachments) contains information concerning VOD and/or electronic sell-through (EST), content protection and other technology related to VAN. IF YOU HAVE RECEIVED THIS COMMUNICATION IN ERROR, PLEASE NOTIFY US IMMEDIATELY BY TELEPHONING THE ORIGINAL COMMUNITY AND DESTROY ALL COPIES, BOTH ELECTRONIC AND OTHER, OF\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate text using the pipeline\n",
    "# email_text1 = generate_email(\"Hello, Following up on the bubblegum shipment.\", use_pipeline=True)\n",
    "# email_text2 = generate_email(\"Please confirm the delivery date for our next order.\", use_pipeline=True)\n",
    "# email_text3 = generate_email(\"Can you update me on the status of the invoice?\", use_pipeline=True)\n",
    "\n",
    "# # Print the generated emails\n",
    "# print(email_text1)\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(email_text2)\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(email_text3)\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")\n",
    "# print(\"--------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Pipeline Output\n",
    "\n",
    "Hello, Following up on the bubblegum shipment.\n",
    "I'll send you the info.\n",
    "Hope you are well.\n",
    "Best,\n",
    "Janeh.\n",
    "Janeh.\n",
    "On Tue, Feb ,  at : AM, Cavanaugh, Kristin < wrote:Hi Janeh,I hope you are well.I spoke to Tom Rothman and he said he'd give me a call regarding your order, I told him I wanted to talk to you about your order and the order and I said I would send you a separate email that was so that I could communicate the order.I don't know if you had a chance to look at it and he suggested I send it to him directly. I've enclosed the order and my contact info for your reference.Thank you.Best,KristinErnest,Thank you for purchasing your itemPlease note that it has arrived in your carrier.\n",
    "The items listed in this shipment are:x x x inch-wide x inch-wide x inch-wide x inch-wide x inch-wide x inch-wide\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n",
    "Please confirm the delivery date for our next order.\n",
    "If you have any questions, please contact the Global Service Desk or your Local IT Representative.\n",
    "Global Service Desk Contact Information:\n",
    "North America: \n",
    "US Toll Free: -SPE-SONY\n",
    "Europe: ()-International Toll Fee Numbers\n",
    "OnNet: - or -\n",
    "\n",
    "GSD Live Chat\n",
    "Regards,\n",
    "SPE Identity Management\n",
    "MP/WPF\n",
    "ASAP\n",
    "CRB(Competitive Releases)\n",
    "DICER\n",
    "FCM\n",
    "GPMS - CopyRight\n",
    "GPMS - MAGIC\n",
    "GPMS - SCRY\n",
    "GPMS - Titles and Registration\n",
    "IntSales\n",
    "Motion Pictures Portal\n",
    "Script Tracker\n",
    "SpiritWorld\n",
    "Superbad\n",
    "Worldwide Print Tracking System (WPTS)\n",
    "Worldwide Publicity Website\n",
    "Productions\n",
    "C\n",
    "Calypso\n",
    "Dropzone\n",
    "GPAS\n",
    "Motion Pictures Production Database(MPPDB)\n",
    "Tview\n",
    "TV\n",
    "BB\n",
    "CC\n",
    "Carmen\n",
    "DealTracker\n",
    "Dr. Oz\n",
    "DTSM\n",
    "ITSM/SARA\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n",
    "Can you update me on the status of the invoice?\n",
    "If not, please let me know.\n",
    "Thanks,\n",
    "Larry\n",
    "Larry Marino\n",
    "\n",
    "On Apr , , at : AM, Larry Marino < wrote:\n",
    "Hi Larry,\n",
    "Jane was hoping to see if we could get an invoice from ECS for this month's sales tax. We are working on a deal to make this work; the invoice is supposed to be CURRENCY,. I'm sorry but we won't be able to get a hard copy.\n",
    "Thanks again!\n",
    "Larry Marino\n",
    "\n",
    "On Apr , , at : AM, Larry Marino < wrote:\n",
    "Thanks Larry,\n",
    "I hope that you are well.\n",
    "I wanted to follow up with you briefly regarding the payment for the June invoice. I have received an invoice from ECS to make certain that we were billed by ECS for this year.\n",
    "The invoice is supposed to be CURRENCY,.\n",
    "The invoice is for May  through  and the invoice will be CURRENCY,.\n",
    "Thanks again,\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Detailed Output\n",
    "\n",
    "Laptop Male Software Engineer Cycling Club\n",
    "URL URL URL URL\n",
    "_____\n",
    "This e-mail message is intended only for the individual or entity to which it is addressed and may contain information that is privileged, confidential, or exempt from disclosure under applicable Federal or State law. If the reader of this e-mai-mail is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipients, you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n",
    "Mobile Female Data Scientist Hiking in the US\n",
    "URL\n",
    "--URL This message was sent by: BBC Worldwide Americas\n",
    " Avenue of the Americas New York, NY, \n",
    "This email is to provide you with a personal digital file of all emails from BBC Worldwide into your inbox. If you prefer not to continue receiving email communications, please unsubscribe here instead of replying to this email.\n",
    "To update your profile and customize what email alerts and newsletters you receive, please click here.\n",
    "Having\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n",
    "Desktop Male Graphic Designer Painting -.\n",
    "URL URL URL URL #\n",
    ">\n",
    "> -Houzz Logo\n",
    "> //RESERVATION STAMPED HANDS AND LINE BLUES CURRENCY.\n",
    "Image Credit: Kiki Bentonka\n",
    "This is a photo message from my Houzz ideabook.\n",
    "If you are having trouble viewing this, click here.\n",
    "Share This: URL URL\n",
    "URL\n",
    "Thank You,\n",
    "--------------------\n",
    "--------------------\n",
    "--------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1 Mode Output\n",
    "\n",
    "<!-- # Laptop Male Software Engineer Cycling Team\n",
    "# <NUMBER>-<PHONE>\n",
    "# Email: <EMAIL> | Twitter: @Cycling_Team\n",
    "# Cyclists are invited to participate in the Cycling Cycling World Cup in Brazil.\n",
    "# The Cycling Federation of Brazil is a global organization dedicated to promoting cycling \n",
    "# and the development of sustainable living. Cycling is an international organization that promotes the health, \n",
    "# safety and well-being of all people. The Cycling Foundation is dedicated solely to the pursuit of the highest quality \n",
    "# and quality of life -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
